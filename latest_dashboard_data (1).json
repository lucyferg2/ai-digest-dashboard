{
  "metadata": {
    "generatedAt": "2025-04-02 15:06:09",
    "year": 2025,
    "month": 4,
    "monthName": "April",
    "articleCount": 101
  },
  "articles": [
    {
      "id": 0,
      "title": "Introducing AWS MCP Servers for code assistants (Part 1)",
      "link": "https://aws.amazon.com/blogs/machine-learning/introducing-aws-mcp-servers-for-code-assistants-part-1/",
      "description": "We’re excited to announce the open source release of AWS MCP Servers for code assistants — a suite of specialized Model Context Protocol (MCP) servers that bring Amazon Web Services (AWS) best practices directly to your development workflow. Our specialized AWS MCP servers combine deep AWS knowledge with agentic AI capabilities to accelerate development across key areas. Each AWS MCP Server focuses on a specific domain of AWS best practices, working together to provide comprehensive guidance throughout your development journey.\nThis post is the first in a series covering AWS MCP Servers. In this post, we walk through how these specialized MCP servers can dramatically reduce your development time while incorporating security controls, cost optimizations, and AWS Well-Architected best practices into your code. Whether you’re an experienced AWS developer or just getting started with cloud development, you’ll discover how to use AI-powered coding assistants to tackle common challenges such as complex service configurations, infrastructure as code (IaC) implementation, and knowledge base integration. By the end of this post, you’ll understand how to start using AWS MCP Servers to transform your development workflow and deliver better solutions, faster.\nIf you want to get started right away, skip ahead to the section “From Concept to working code in minutes.”\nAI is transforming how we build software, creating opportunities to dramatically accelerate development while improving code quality and consistency. Today’s AI assistants can understand complex requirements, generate production-ready code, and help developers navigate technical challenges in real time. This AI-driven approach is particularly valuable in cloud development, where developers need to orchestrate multiple services while maintaining security, scalability, and cost-efficiency.\nDevelopers need code assistants that understand the nuances of AWS services and best practices. Specialized AI agents can address these needs by:\n\nProviding contextual guidance on AWS service selection and configuration\nOptimizing compliance with security best practices and regulatory requirements\nPromoting the most efficient utilization and cost-effective solutions\nAutomating repetitive implementation tasks with AWS specific patterns\n\nThis approach means developers can focus on innovation while AI assistants handle the undifferentiated heavy lifting of coding. Whether you’re using Amazon Q, Amazon Bedrock, or other AI tools in your workflow, AWS MCP Servers complement and enhance these capabilities with deep AWS specific knowledge to help you build better solutions faster.\nModel Context Protocol (MCP) is a standardized open protocol that enables seamless interaction between large language models (LLMs), data sources, and tools. This protocol allows AI assistants to use specialized tooling and to access domain-specific knowledge by extending the model’s capabilities beyond its built-in knowledge—all while keeping sensitive data local. Through MCP, general-purpose LLMs can now seamlessly access relevant knowledge beyond initial training data and be effectively steered towards desired outputs by incorporating specific context and best practices.\nAccelerate building on AWS\nWhat if your AI assistant could instantly access deep AWS knowledge, understanding every AWS service, best practice, and architectural pattern? With MCP, we can transform general-purpose LLMs into AWS specialists by connecting them to specialized knowledge servers. This opens up exciting new possibilities for accelerating cloud development while maintaining security and following best practices.\nBuild on AWS in a fraction of the time, with best practices automatically applied from the first line of code. Skip hours of documentation research and immediately access ready-to-use patterns for complex services such as Amazon Bedrock Knowledge Bases. Our MCP Servers will help you write well-architected code from the start, implement AWS services correctly the first time, and deploy solutions that are secure, observable, and cost-optimized by design. Transform how you build on AWS today.\n\nEnforce AWS best practices automatically – Write well-architected code from the start with built-in security controls, proper observability, and optimized resource configurations\nCut research time dramatically – Stop spending hours reading documentation. Our MCP Servers provide contextually relevant guidance for implementing AWS services correctly, addressing common pitfalls automatically\nAccess ready-to-use patterns instantly – Use pre-built AWS CDK constructs, Amazon Bedrock Agents schema generators, and Amazon Bedrock Knowledge Bases integration templates that follow AWS best practices from the start\nOptimize cost proactively – Prevent over-provisioning as you design your solution by getting cost-optimization recommendations and generating a comprehensive cost report to analyze your AWS spending before deployment\n\nTo turn this vision into reality and make AWS development faster, more secure, and more efficient, we’ve created AWS MCP Servers—a suite of specialized AWS MCP Servers that bring AWS best practices directly to your development workflow. Our specialized AWS MCP Servers combine deep AWS knowledge with AI capabilities to accelerate development across key areas. Each AWS MCP Server focuses on a specific domain of AWS best practices, working together to provide comprehensive guidance throughout your development journey.\nOverview of domain-specific MCP Servers for AWS development\nOur specialized MCP Servers are designed to cover distinct aspects of AWS development, each bringing deep knowledge to specific domains while working in concert to deliver comprehensive solutions:\n\nCore – The foundation server that provides AI processing pipeline capabilities and serves as a central coordinator. It helps provide clear plans for building AWS solutions and can federate to other MCP servers as needed.\nAWS Cloud Development Kit (AWS CDK) – Delivers AWS CDK knowledge with tools for implementing best practices, security configurations with cdk-nag, Powertools for AWS Lambda integration, and specialized constructs for generative AI services. It makes sure infrastructure as code (IaC) follows AWS Well-Architected principles from the start.\nAmazon Bedrock Knowledge Bases – Enables seamless access to Amazon Bedrock Knowledge Bases so developers can query enterprise knowledge with natural language, filter results by data source, and use reranking for improved relevance.\nAmazon Nova Canvas – Provides image generation capabilities using Amazon Nova Canvas through Amazon Bedrock, enabling the creation of visuals from text prompts and color palettes—perfect for mockups, diagrams, and UI design concepts.\nCost – Analyzes AWS service costs and generates comprehensive cost reports, helping developers understand the financial implications of their architectural decisions and optimize for cost-efficiency.\n\nPrerequisites\nTo complete the solution, you need to have the following prerequisites in place:\n\nuv package manager\nInstall Python using uv python install 3.13\nAWS credentials with appropriate permissions\nAn MCP-compatible LLM client (such as Anthropic’s Claude for Desktop, Cline, Amazon Q CLI, or Cursor)\n\nFrom concept to working code in minutes\nYou can download the AWS MCP Servers on GitHub or through the PyPI package manager. Here’s how to get started using your favorite code assistant with MCP support.\nTo install MCP Servers, enter the following code:\n\n\n# Install and setup the MCP servers\n{\n  \"mcpServers\": {\n    \"awslabs.core-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"awslabs.core-mcp-server@latest\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n        \"MCP_SETTINGS_PATH\": \"path to your mcp server settings\"\n      },\n      \"autoApprove\": [],\n      \"disabled\": false\n    },\n    \"awslabs.bedrock-kb-retrieval-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.bedrock-kb-retrieval-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    },\n    \"awslabs.cdk-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.cdk-mcp-server@latest\"],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    },\n    \"awslabs.cost-analysis-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.cost-analysis-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    },\n    \"awslabs.nova-canvas-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.nova-canvas-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    }\n  }\n}\n\n\nAWS MCP Servers in action\nHere’s how AWS MCP servers transform the development experience:\nDeveloper: “I need to build an AI-powered chatbot using Amazon Bedrock that can answer questions from our company’s knowledge base. I also want to add a tool for the chatbot to call our internal API.”\nCore: “I’ll help you build an Amazon Bedrock Knowledge Bases chatbot with API integration. Let’s create an architecture that uses Amazon Bedrock Agents with a custom action group to call your internal API.”\nCore generates a comprehensive architecture diagram showing the knowledge base integration, Amazon Bedrock Agents configuration with action groups, API connectivity, and data flow between components.\nAWS CDK: “Here’s the infrastructure code for your chatbot with the Amazon Bedrock Agents action group. I’ve included proper IAM roles, security controls, and Lambda Powertools for observability.”\n\n// CDK code with Well-Architected best practices\n const knowledgeBase = new BedrockKnowledgeBase(this, 'CompanyKB', {\n   embeddingModel: BedrockFoundationModel.TITAN_EMBED_TEXT_V1,\n   vectorStore: new OpenSearchServerlessVectorStore(this, 'VectorStore', {\n     encryption: OpenSearchEncryption.KMS,\n     ebs: OpenSearchEbsOptions.provisioned(100, OpenSearchVolumeType.GP3)\n   })\n });\n\n // Action Group for API integration\n const actionGroupSchema = new BedrockAgentSchema(this, 'ApiActionSchema', {\n   schemaPath: 'schemas/api-action-schema.json',\n   // Schema will be auto-generated from your Lambda code\n   generateFromLambda: {\n     lambdaPath: 'lambda/api-actions.py',\n     outputPath: 'schemas/api-action-schema.json'\n   }\n });\n\n const apiActionGroup = new BedrockAgentActionGroup(this, 'ApiActionGroup', {\n   actionGroupName: 'InternalApiActions',\n   description: 'Actions to interact with internal API',\n   actionGroupExecutor: new BedrockAgentLambdaExecutor(apiActionsLambda),\n   actionGroupSchema: actionGroupSchema\n });\n\nThe CDK MCP Server generates complete AWS CDK code to deploy the entire solution. It automatically runs cdk-nag to identify potential security issues and provides remediation steps for each finding, making sure that the infrastructure follows AWS Well-Architected best practices.\nAmazon Bedrock Knowledge Bases retrieval: “I’ve configured the optimal settings for your knowledge base queries, including proper reranking for improved relevance.”\nAmazon Bedrock Knowledge Bases MCP Server demonstrates how to structure queries to the knowledge base for maximum relevance, provides sample code for filtering by data source, and shows how to integrate the knowledge base responses with the chatbot interface.\nAmazon Nova Canvas: “To enhance your chatbot’s capabilities, I’ve created visualizations that can be generated on demand when users request data explanations.”\nAmazon Nova Canvas MCP server generates sample images showing how Amazon Nova Canvas can create charts, diagrams, and visual explanations based on knowledge base content, making complex information more accessible to users.\nCost Analysis: “Based on your expected usage patterns, here’s the estimated monthly cost breakdown and optimization recommendations.”\nThe Cost Analysis MCP Server generates a detailed cost analysis report showing projected expenses for each AWS service, identifies cost optimization opportunities such as reserved capacity for Amazon Bedrock, and provides specific recommendations to reduce costs without impacting performance.\nWith AWS MCP Servers, what would typically take days of research and implementation is completed in minutes, with better quality, security, and cost-efficiency than manual development in that same time.\nBest practices for MCP-assisted development\nTo maximize the benefits of MCP assisted development while maintaining security and code quality, developers should follow these essential guidelines:\n\nAlways review generated code for security implications before deployment\nUse MCP Servers as accelerators, not replacements for developer judgment and expertise\nKeep MCP Servers updated with the latest AWS security best practices\nFollow the principle of least privilege when configuring AWS credentials\nRun security scanning tools on generated infrastructure code\n\nComing up in the series\nThis post introduced the foundations of AWS MCP Servers and how they accelerate AWS development through specialized, AWS specific MCP Servers. In upcoming posts, we’ll dive deeper into:\n\nDetailed walkthroughs of each MCP server’s capabilities\nAdvanced patterns for integrating AWS MCP Servers into your development workflow\nReal-world case studies showing AWS MCP Servers’ impact on development velocity\nHow to extend AWS MCP Servers with your own custom MCP servers\n\nStay tuned to learn how AWS MCP Servers can transform your specific AWS development scenarios and help you build better solutions faster. Visit our GitHub repository or Pypi package manager to explore example implementations and get started today.\n\nAbout the Authors\nJimin Kim is a Prototyping Architect on the AWS Prototyping and Cloud Engineering (PACE) team, based in Los Angeles. With specialties in Generative AI and SaaS, she loves helping her customers succeed in their business. Outside of work, she cherishes moments with her wife and three adorable calico cats.\nPranjali Bhandari is part of the Prototyping and Cloud Engineering (PACE) team at AWS, based in the San Francisco Bay Area. She specializes in Generative AI, distributed systems, and cloud computing. Outside of work, she loves exploring diverse hiking trails, biking, and enjoying quality family time with her husband and son.\nLaith Al-Saadoon is a Principal Prototyping Architect on the Prototyping and Cloud Engineering (PACE) team. He builds prototypes and solutions using generative AI, machine learning, data analytics, IoT & edge computing, and full-stack development to solve real-world customer challenges. In his personal time, Laith enjoys the outdoors–fishing, photography, drone flights, and hiking.\nPaul Vincent is a Principal Prototyping Architect on the AWS Prototyping and Cloud Engineering (PACE) team. He works with AWS customers to bring their innovative ideas to life. Outside of work, he loves playing drums and piano, talking with others through Ham radio, all things home automation, and movie nights with the family.\nJustin Lewis leads the Emerging Technology Accelerator at AWS. Justin and his team help customers build with emerging technologies like generative AI by providing open source software examples to inspire their own innovation. He lives in the San Francisco Bay Area with his wife and son.\nAnita Lewis is a Technical Program Manager on the AWS Emerging Technology Accelerator team, based in Denver, CO. She specializes in helping customers accelerate their innovation journey with generative AI and emerging technologies. Outside of work, she enjoys competitive pickleball matches, perfecting her golf game, and discovering new travel destinations.",
      "date": "2025-04-01",
      "authors": "Jimin Kim",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article introduces the open-source release of AWS MCP Servers, a suite of specialized Model Context Protocol servers designed to enhance AWS development by providing AI-driven assistance, best practices, and context-specific guidance throughout the software development lifecycle.",
      "takeaways": [
        "- AWS MCP Servers leverage AI capabilities to streamline the development process, enabling faster coding and improved application quality by automatically integrating AWS best practices.",
        "- The Model Context Protocol allows AI assistants to access domain-specific knowledge, enhancing their ability to provide relevant guidance for AWS service configurations and cost optimizations.",
        "- Specialized MCP Servers focus on various aspects of AWS development (e.g., security, cost analysis, infrastructure as code), significantly reducing development time compared to traditional methods."
      ]
    },
    {
      "id": 1,
      "title": "Harness the power of MCP servers with Amazon Bedrock Agents",
      "link": "https://aws.amazon.com/blogs/machine-learning/harness-the-power-of-mcp-servers-with-amazon-bedrock-agents/",
      "description": "AI agents extend large language models (LLMs) by interacting with external systems, executing complex workflows, and maintaining contextual awareness across operations. Amazon Bedrock Agents enables this functionality by orchestrating foundation models (FMs) with data sources, applications, and user inputs to complete goal-oriented tasks through API integration and knowledge base augmentation. However, in the past, connecting these agents to diverse enterprise systems has created development bottlenecks, with each integration requiring custom code and ongoing maintenance—a standardization challenge that slows the delivery of contextual AI assistance across an organization’s digital ecosystem. This is a problem that you can solve by using Model Context Protocol (MCP), which provides a standardized way for LLMs to connect to data sources and tools.\nToday, MCP is providing agents standard access to an expanding list of accessible tools that you can use to accomplish a variety of tasks. In time, MCP can promote better discoverability of agents and tools through marketplaces, enabling agents to share context and have common workspaces for better interaction, and scale agent interoperability across the industry.\nIn this post, we show you how to build an Amazon Bedrock agent that uses MCP to access data sources to quickly build generative AI applications. Using Amazon Bedrock Agents, your agent can be assembled on the fly with MCP-based tools as in this example:\n\nInlineAgent(\n    foundation_model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    instruction=\"You are a friendly assistant for resolving user queries\",\n    agent_name=\"SampleAgent\",\n    action_groups=[\n        ActionGroup(\n            name=\"SampleActionGroup\",\n            mcp_clients=[mcp_client_1, mcp_client_2],\n        )\n    ],\n).invoke(input_text=”Convert 11am from NYC time to London time”)\n\nWe showcase an example of building an agent to understand your Amazon Web Service (AWS) spend by connecting to AWS Cost Explorer, Amazon CloudWatch, and Perplexity AI through MCP. You can use the code referenced in this post to connect your agents to other MCP servers to address challenges for your business. We envision a world where agents have access to an ever-growing list of MCP servers that they can use for accomplishing a wide variety of tasks.\nModel Context Protocol\nDeveloped by Anthropic as an open protocol, MCP provides a standardized way to connect AI models to virtually any data source or tool. Using a client-server architecture, MCP enables developers to expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. Through this architecture, MCP enables users to build more powerful, context-aware AI agents that can seamlessly access the information and tools they need. Whether you’re connecting to external systems or internal data stores or tools, you can now use MCP to interface with all of them in the same way. The client-server architecture of MCP enables your agent to access new capabilities as the MCP server updates without requiring any changes to the application code.\nMCP architecture\nMCP uses a client-server architecture that contains the following components and is shown in the following figure:\n\nHost: An MCP host is a program or AI tool that requires access to data through the MCP protocol, such as Claude Desktop, an integrated development environment (IDE), or any other AI application.\nClient: Protocol clients that maintain one-to-one connections with servers.\nServer: Lightweight programs that expose capabilities through standardized MCP.\nLocal data sources: Your databases, local data sources, and services that MCP servers can securely access.\nRemote services: External systems available over the internet through APIs that MCP servers can connect to.\n\n\nLet’s walk through how to set up Amazon Bedrock agents that take advantage of MCP servers.\nUsing MCP with Amazon Bedrock agents\nIn this post, we provide a step-by-step guide for how to connect your favorite MCP servers with Amazon Bedrock agents as Action Groups that an agent can use to accomplish tasks provided by the user. The AgentInlineSDK provides a straightforward way to create inline agents, containing a built-in MCP client implementation that provides you with direct access to tools delivered by an MCP server.\nAs part of creating an agent, the developer creates an MCP client specific to each MCP server that requires agent communication. When invoked, the agent determines which tools are needed for the user’s task; if MCP server tools are required, it uses the corresponding MCP client to request tool execution from that server. The user code doesn’t need to be aware of the MCP protocol because that’s handled by the MCP client provided the InlineAgent code repository.\nTo orchestrate this workflow, you take advantage of the return control capability of Amazon Bedrock Agents. The following diagram illustrates the end-to-end flow of an agent handling a request that uses two tools. In the first flow, a Lambda-based action is taken, and in the second, the agent uses an MCP server.\n\nUse case: transform how you manage your AWS spend across different AWS services including Amazon Bedrock\nTo show how an Amazon Bedrock agent can use MCP servers, let’s walk through a sample use case. Imagine asking questions like “Help me understand my Bedrock spend over the last few weeks” or “What were my EC2 costs last month across regions and instance types?” and getting a human-readable analysis of the data instead of raw numbers on a dashboard. The system interprets your intent and delivers precisely what you need—whether that’s detailed breakdowns, trend analyses, visualizations, or cost-saving recommendations. This is useful because what you’re interested in is insights rather than data. You can accomplish this using two MCP servers: a custom-built MCP server for retrieving the AWS spend data and an open source MCP server from Perplexity AI to interpret the data. You add these two MCP servers as action groups in an inline Amazon Bedrock agent. This gives you an AI agent that can transform the way you manage your AWS spend. All the code for this post is available in the GitHub repository.\nLet’s walk through how this agent is created using inline agents. You can use inline agents to define and configure Amazon Bedrock agents dynamically at runtime. They provide greater flexibility and control over agent capabilities, enabling users to specify FMs, instructions, action groups, guardrails, and knowledge bases as needed without relying on pre-configured control plane settings. It’s worth noting that you can also orchestrate this behavior without inline agents by using RETURN_CONTROL with the InvokeAgent API.\nMCP components in Amazon Bedrock Agents\n\nHost: This is the Amazon Bedrock inline agent. This agent adds MCP clients as action groups that can be invoked through RETURN_CONTROL when the user asks an AWS spend-related question.\nClient: You create two clients that establish one-to-one connections with their respective servers: a cost explorer client with specific cost server parameters and a Perplexity AI client with Perplexity server parameters.\nServers: You create two MCP servers that each run locally on your machine and communicate to your application over standard input/output (alternatively, you could also configure the client to talk to remote MCP servers). \n  \nCost Explorer and Amazon CloudWatch Logs (for Amazon Bedrock model invocation log data) and an MCP server to retrieve the AWS spend data.\nPerplexity AI MCP server to interpret the AWS spend data.\n \nData sources: The MCP servers talk to remote data sources such as Cost Explorer API, CloudWatch Logs and the Perplexity AI search API.\n\nPrerequisites\nYou need the following prerequisites to get started implementing the solution in this post:\n\nAn AWS account\nFamiliarity with FMs and Amazon Bedrock\nInstall AWS Command Line Interface (AWS CLI) and set up credentials\nPython 3.11 or later\nAWS Cloud Development Kit (AWS CDK) CLI\nEnable model access for Anthropic’s Claude 3.5 Sonnet v2\nYou need to have your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY so that you can set them using environment variables for the server\nThe two MCP servers are run as Docker daemons, so you need to have Docker installed and running on your computer\n\nThe MCP servers run locally on your computer and need to access AWS services and the Perplexity API. You can read more about AWS credentials in Manage access keys for IAM users. Make sure that your credentials include AWS Identity and Access Manager (IAM) read access to Cost Explorer and CloudWatch. You can do this by using AWSBillingReadOnlyAccess and CloudWatchReadOnlyAccess managed IAM permissions. You can get the Perplexity API key from the Perplexity Sonar API page.\nSteps to run\nWith the prerequisites in place, you’re ready to implement the solution.\n\nNavigate to the InlineAgent GitHub repository.\nFollow the setup steps.\nNavigate to the cost_explorer_agent This folder contains the code for this post. \n  \ncd examples/mcp/cost_explorer_agent\n \nCreate a .env file in cost_explorer_agent directory using example. \n  \nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nAWS_REGION=\nBEDROCK_LOG_GROUP_NAME=\nPERPLEXITY_API_KEY=\n \nBuild aws-cost-explorer-mcp server \n  \ngit clone https://github.com/aarora79/aws-cost-explorer-mcp-server.git\ncd aws-cost-explorer-mcp-server/\ndocker build -t aws-cost-explorer-mcp\n \nYou’re now ready to create an agent that can invoke these MCP servers to provide insights into your AWS spend. You can do this by running the python main.py command. The output will look like the following example. The agent connects with the two MCP servers and accesses their respective tools. Next, the agent lays out a plan to use specific tools sequentially, uses code interpreter to generate a chart showing cost distribution, and uses Cost Explorer and Perplexity API to provide information about Amazon Bedrock and the AWS account’s cost expenditure. \n  \nConnected to server with tools: ['get_bedrock_daily_usage_stats', 'get_bedrock_hourly_usage_stats', 'get_ec2_spend_last_day', 'get_detailed_breakdown_by_day']\nConnected to server with tools: ['perplexity_ask']\n\nThought: Let me break this down into parts:\n1. First, I'll ask about Amazon Bedrock using Perplexity\n2. Then I'll get the cost breakdown for the last 7 days\n3. Finally, I'll create a visualization of the spending data\n\nTool use: perplexity_ask with these inputs: messages[[{role=user, content=What is Amazon Bedrock? Provide a concise explanation.}] (array)]\n\nTool use: get_detailed_breakdown_by_day with these inputs: days[7 (integer)]\n\nThought: Let me analyze the data and create a visualization of the top services by cost. I'll use Python to process the data and create a bar chart.\n\nCode interpreter:\nGenerated code\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n...\n```\n\nCode interpreter output:\naws_costs.png (image/png)\nDirectory output created successfully\n\nThought: Now I can provide a comprehensive answer about Amazon Bedrock and the cost analysis. Amazon Bedrock is a fully managed service that provides access to foundation models (FMs) from leading AI companies through a single API. Based on the cost analysis of the last 7 days, your top spending services were:\n\n1. Amazon Neptune: $24\n2. Amazon SageMaker: $567\n3. EC2 (Compute + Other): $454\n4. Amazon OpenSearch Service: $34\n5. Amazon Bedrock: $1235\n6. Amazon Q: $178\n \n\n\n\n\n\n\nImplementation details\nNow that you understand the output produced by an agent, let’s lift the curtain and review some of the important pieces of code that produce the output.\n\nCreating MCP clients: config.py, defines the two MCP clients that talk to your two MCP servers. \n  \nServer parameters are defined for the cost explorer and Perplexity clients. The solution uses StdioServerParameters, which configures how the client should communicate over standard input/output (stdio) streams. This contains the parameters required by the server to access the required data through APIs. \n    \n# Cost server parameters\ncost_server_params = StdioServerParameters(\n    command=\"/usr/local/bin/docker\",\n    args=[\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"AWS_ACCESS_KEY_ID\",\n        \"-e\",\n        \"AWS_SECRET_ACCESS_KEY\",\n        \"-e\",\n        \"AWS_REGION\",\n        \"-e\",\n        \"BEDROCK_LOG_GROUP_NAME\",\n        \"-e\",\n        \"stdio\",\n        \"aws-cost-explorer-mcp:latest\",\n    ],\n    env={\n        \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n        \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n        \"AWS_REGION\": AWS_REGION,\n        \"BEDROCK_LOG_GROUP_NAME\": BEDROCK_LOG_GROUP_NAME,\n    },\n)\n\n# Perplexity server parameters\nperplexity_server_params = StdioServerParameters(\n    command=\"/usr/local/bin/docker\",\n    args=[\"run\", \"-i\", \"--rm\", \"-e\", \"PERPLEXITY_API_KEY\", \"mcp/perplexity-ask\"],\n    env={\"PERPLEXITY_API_KEY\": PERPLEXITY_API_KEY},\n)\n \nIn main.py, the MCP server parameters are imported and used to create your two MCP clients. \n    \ncost_explorer_mcp_client = await MCPClient.create(server_params=cost_server_params)\nperplexity_mcp_client = await MCPClient.create(server_params=perplexity_server_params)\n \n \n\n\nConfigure agent action group: main.py creates the action group that combines the MCP clients into a single interface that the agent can access. This enables the agent to ask your application to invoke either of these MCP servers as needed through return of control. \n  \n# Create action group with both MCP clients\ncost_action_group = ActionGroup(\n    name=\"CostActionGroup\",\n    mcp_clients=[cost_explorer_mcp_client, perplexity_mcp_client]\n)\n \nInline agent creation: The inline agent can be created with the following specifications: \n  \nFoundation model: Configure your choice of FM to power your agent. This can be any model provided on Amazon Bedrock. This example uses Anthropic’s Claude 3.5 Sonnet model.\nAgent instruction: Provide instructions to your agent that contain the guidance and steps for orchestrating responses to user queries. These instructions anchor the agent’s approach to handling various types of queries\nAgent name: Name of your agent.\nAction groups: Define the action groups that your agent can access. These can include single or multiple action groups, with each group having access to multiple MCP clients or AWS Lambda As an option, you can configure your agent to use Code Interpreter to generate, run, and test code for your application.\n \n\n\n# Create and invoke the inline agent\nawait InlineAgent(\n    foundation_model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    instruction=\"\"\"You are a friendly assistant that is responsible for resolving user queries.\n    \n    You have access to search, cost tool and code interpreter. \n    \n    \"\"\",\n    agent_name=\"cost_agent\",\n    action_groups=[\n        cost_action_group,\n        {\n            \"name\": \"CodeInterpreter\",\n            \"builtin_tools\": {\n                \"parentActionGroupSignature\": \"AMAZON.CodeInterpreter\"\n            },\n        },\n    ],\n).invoke(\n    input_text=\"<user-query-here>\"\n)\n\nYou can use this example to build an inline agent on Amazon Bedrock that establishes connections with different MCP servers and groups their clients into a single action group for the agent to access.\nConclusion\nThe Anthropic MCP protocol offers a standardized way of connecting FMs to data sources, and now you can use this capability with Amazon Bedrock Agents. In this post, you saw an example of combining the power of Amazon Bedrock and MCP to build an application that offers a new perspective on understanding and managing your AWS spend.\nOrganizations can now offer their teams natural, conversational access to complex financial data while enhancing responses with contextual intelligence from sources like Perplexity. As AI continues to evolve, the ability to securely connect models to your organization’s critical systems will become increasingly valuable. Whether you’re looking to transform customer service, streamline operations, or gain deeper business insights, the Amazon Bedrock and MCP integration provides a flexible foundation for your next AI innovation. You can dive deeper on this MCP integration by exploring our code samples.\nHere are some examples of what you can build by connecting your Amazon Bedrock Agents to MCP servers:\n\nA multi-data source agent that retrieves data from different data sources such as Amazon Bedrock Knowledge Bases, Sqlite, or even your local filesystem.\nA developer productivity assistant agent that integrates with Slack and GitHub MCP servers.\nA machine learning experiment tracking agent that integrates with the Opik MCP server from Comet ML for managing, visualizing, and tracking machine learning experiments directly within development environments.\n\nWhat business challenges will you tackle with these powerful new capabilities?\n\nAbout the authors\nMark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark’s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification.\nEashan Kaushik is a Specialist Solutions Architect AI/ML at Amazon Web Services. He is driven by creating cutting-edge generative AI solutions while prioritizing a customer-centric approach to his work. Before this role, he obtained an MS in Computer Science from NYU Tandon School of Engineering. Outside of work, he enjoys sports, lifting, and running marathons.\nMadhur Prashant  is an AI and ML Solutions Architect at Amazon Web Services. He is passionate about the intersection of human thinking and generative AI. His interests lie in generative AI, specifically building solutions that are helpful and harmless, and most of all optimal for customers. Outside of work, he loves doing yoga, hiking, spending time with his twin, and playing the guitar.\nAmit Arora is an AI and ML Specialist Architect at Amazon Web Services, helping enterprise customers use cloud-based machine learning services to rapidly scale their innovations. He is also an adjunct lecturer in the MS data science and analytics program at Georgetown University in Washington, D.C.\nAndy Palmer is a Director of Technology for AWS Strategic Accounts. His teams provide Specialist Solutions Architecture skills across a number of speciality domain areas, including AIML, generative AI, data and analytics, security, network, and open source software. Andy and his team have been at the forefront of guiding our most advanced customers through their generative AI journeys and helping to find ways to apply these new tools to both existing problem spaces and net new innovations and product experiences.",
      "date": "2025-04-01",
      "authors": "Mark Roy",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the integration of Model Context Protocol (MCP) with Amazon Bedrock Agents to enhance the functionality of AI agents by allowing them to interact with external systems and data sources through standardized connections. It highlights the advantages of MCP in streamlining the development process and enabling complex workflows while providing a step-by-step guide on creating an Amazon Bedrock agent that utilizes MCP servers for various tasks.",
      "takeaways": [
        "- MCP provides a standardized architecture that allows AI agents to connect to diverse data sources and tools, promoting interoperability and reducing development bottlenecks.",
        "- Amazon Bedrock Agents can dynamically build and utilize inline agents that access multiple MCP clients, enabling organizations to efficiently manage complex data interactions and workflows.",
        "- The integration facilitates natural, conversational access to financial data, paving the way for advanced analytical capabilities and insights in areas like AWS cost management."
      ]
    },
    {
      "id": 2,
      "title": "Generate compliant content with Amazon Bedrock and ConstitutionalChain",
      "link": "https://aws.amazon.com/blogs/machine-learning/generate-compliant-content-with-amazon-bedrock-and-constitutionalchain/",
      "description": "Generative AI has emerged as a powerful tool for content creation, offering several key benefits that can significantly enhance the efficiency and effectiveness of content production processes such as creating marketing materials, image generation, content moderation etc. Constitutional AI and LangGraph‘s reflection mechanisms represent two complementary approaches to ensuring AI systems behave ethically – with Anthropic embedding principles during training while LangGraph implements them during inference/runtime through reflection and self-correction mechanisms. By using LanGraph’s Constitutional AI, content creators can streamline their workflow while maintaining high standards of user-defined compliance and ethical integrity. This method not only reduces the need for extensive human oversight but also enhances the transparency and accountability of content generation process by AI.\nIn this post, we explore practical strategies for using Constitutional AI to produce compliant content efficiently and effectively using Amazon Bedrock and LangGraph to build ConstitutionalChain for rapid content creation in highly regulated industries like finance and healthcare. Although AI offers significant productivity benefits, maintaining compliance with strict regulations are crucial. Manual validation of AI-generated content for regulatory adherence can be time-consuming and challenging. We also provide an overview of how Insagic, a Publicis Groupe company, integrated this concept into their existing healthcare marketing workflow using Amazon Bedrock. Insagic is a next-generation insights and advisory business that combines data, design, and dialogues to deliver actionable insights and transformational intelligence for healthcare marketers. It uses expertise from data scientists, behavior scientists, and strategists to drive better outcomes in the healthcare industry.\nUnderstanding Constitutional AI\nConstitutional AI is designed to align large language models (LLMs) with human values and ethical considerations. It works by integrating a set of predefined rules, principles, and constraints into the LLM’s core architecture and training process. This approach makes sure that the LLM operates within specified ethical and legal parameters, much like how a constitution governs a nation’s laws and actions.\nThe key benefits of Constitutional AI for content creation include:\n\nEthical alignment – Content generated using Constitutional AI is inherently aligned with predefined ethical standards\nLegal compliance – The LLM is designed to operate within legal frameworks, reducing the risk of producing non-compliant content\nTransparency – The principles guiding the LLM’s decision-making process are clearly defined and can be inspected\nReduced human oversight – By embedding ethical guidelines into the LLM, the need for extensive human review is significantly reduced\n\nLet’s explore how you can harness the power of Constitutional AI to generate compliant content for your organization.\nSolution overview\nFor this solution, we use Amazon Bedrock Knowledge Bases to store a repository of healthcare documents. We employ a Retrieval Augmented Generation (RAG) approach, first retrieving relevant context and synthesizing an answer based on the retrieved context, to generate articles based on the repository. We then use the open source orchestration framework LangGraph and ConstitutionalChain to generate, critique, and review prompts in an Amazon SageMaker notebook and develop an agentic workflow to generate compliance content. The following diagram illustrates this architecture.\n\nThis implementation demonstrates a sophisticated agentic workflow that not only generates responses based on a knowledge base but also employs a reflection technique to examine its outputs through ethical principles, allowing it to refine and improve its outputs. We upload a sample set of mental health documents to Amazon Bedrock Knowledge Bases and use those documents to write an article on mental health using a RAG-based approach. Later, we define a constitutional principle with a custom Diversity, Equity, and Inclusion (DEI) principle, specifying how to critique and revise responses for inclusivity.\nPrerequisites\nTo deploy the solution, you need the following prerequisites:\n\nAn AWS account\nAppropriate AWS Identity and Access Management (IAM) permissions to access an Amazon Simple Storage Service (Amazon S3) bucket, create Amazon Bedrock knowledge bases, and create a SageMaker notebook instance\n\nCreate an Amazon Bedrock knowledge base\nTo demonstrate this capability, we download a mental health article from the following GitHub repo and store it in Amazon S3. We then use Amazon Bedrock Knowledge Bases to index the articles. By default, Amazon Bedrock uses Amazon OpenSearch Serverless as a vector database. For full instructions to create an Amazon Bedrock knowledge base with Amazon S3 as the data source, see Create a knowledge base in Amazon Bedrock Knowledge Bases.\n\n\n\nOn the Amazon Bedrock console, create a new knowledge base.\nProvide a name for your knowledge base and create a new IAM service role.\nChoose Amazon S3 as the data source and provide the S3 bucket storing the mental health article.\nChoose Amazon Titan Text Embeddings v2 as the embeddings model and OpenSearch Serverless as the vector store.\nChoose Create Knowledge Base.\n \n\nImport statements and set up an Amazon Bedrock client\nFollow the instructions provided in the README file in the GitHub repo. Clone the GitHub repo to make a local copy. We recommend running this code in a SageMaker JupyterLab environment. The following code imports the necessary libraries, including Boto3 for AWS services, LangChain components, and Streamlit. It sets up an Amazon Bedrock client and configures Anthropic’s Claude 3 Haiku model with specific parameters.\n\nimport boto3\nfrom langchain_aws import ChatBedrock\n…\n\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\nllm = ChatBedrock(client=bedrock_runtime, model_id=\"anthropic.claude-3-haiku-20240307-v1:0\")\n…..\n\nDefine Constitutional AI components\nNext, we define a Critique class to structure the output of the critique process. Then we create prompt templates for critique and revision. Lastly, we set up chains using LangChain for generating responses, critiques, and revisions.\n\n# LangChain Constitutional chain migration to LangGraph\n\nclass Critique(TypedDict):\n    \"\"\"Generate a critique, if needed.\"\"\"\n\n    critique_needed: Annotated[bool, ..., \"Whether or not a critique is needed.\"]\n    critique: Annotated[str, ..., \"If needed, the critique.\"]\n\ncritique_prompt = ChatPromptTemplate.from_template(\n    \"Critique this response according to the critique request. \"\n…\n)\n\nrevision_prompt = ChatPromptTemplate.from_template(\n    \"Revise this response according to the critique and reivsion request.\\n\\n\"\n    ….\n)\nchain = llm | StrOutputParser()\ncritique_chain = critique_prompt | llm.with_structured_output(Critique)\nrevision_chain = revision_prompt | llm | StrOutputParser()\n\n\nDefine a State class and refer to the Amazon Bedrock Knowledge Bases retriever\nWe define a LangGraph State class to manage the conversation state, including the query, principles, responses, and critiques:\n\n# LangGraph State\n\nclass State(TypedDict):\n    query: str\n    constitutional_principles: List[ConstitutionalPrinciple]\n\n\nNext, we set up an Amazon Bedrock Knowledge Bases retriever to extract the relevant information. We refer to the Amazon Bedrock knowledge base we created earlier to create an article based on mental health documents. Make sure to update the knowledge base ID in the following code with the knowledge base you created in previous steps:\n\n#-----------------------------------------------------------------\n# Amazon Bedrock KnowledgeBase\n\nfrom langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n\nretriever = AmazonKnowledgeBasesRetriever(\nknowledge_base_id=\"W3NMIJXLUE\", # Change it to your Knowledge base ID\n…\n)\n\nCreate LangGraph nodes and a LangGraph graph along with constitutional principles\nThe next section of code integrates graph-based workflow orchestration, ethical principles, and a user-friendly interface to create a sophisticated Constitutional AI model. The following diagram illustrates the workflow.\n\nIt uses a StateGraph to manage the flow between RAG and critique/revision nodes, incorporating a custom DEI principle to guide the LLM’s responses. The system is presented through a Streamlit application, which provides an interactive chat interface where users can input queries and view the LLM’s initial responses, critiques, and revised answers. The application also features a sidebar displaying a graph visualization of the workflow and a description of the applied ethical principle. This comprehensive approach makes sure that the LLM’s outputs are not only knowledge-based but also ethically aligned by using customizable constitutional principles that guide a reflection flow (critique and revise), all while maintaining a user-friendly experience with features like chat history management and a clear chat option.\nStreamlit application\nThe Streamlit application component of this code creates an interactive and user-friendly interface for the Constitutional AI model. It sets up a side pane that displays a visualization of the LLM’s workflow graph and provides a description of the DEI principle being applied. The main interface features a chat section where users can input their queries and view the LLM’s responses.\n\n# ------------------------------------------------------------------------\n# Streamlit App\n\n# Clear Chat History fuction\ndef clear_screen():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\nwith st.sidebar:\n    st.subheader('Constitutional AI Demo')\n…..\n    ConstitutionalPrinciple(\n        name=\"DEI Principle\",\n        critique_request=\"Analyze the content for any lack of diversity, equity, or inclusion. Identify specific instances where the text could be more inclusive or representative of diverse perspectives.\",\n        revision_request=\"Rewrite the content by incorporating critiques to be more diverse, equitable, and inclusive. Ensure representation of various perspectives and use inclusive language throughout.\"\n    )\n    \"\"\")\n    st.button('Clear Screen', on_click=clear_screen)\n\n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\n# Chat Input - User Prompt \nif prompt := st.chat_input():\n….\n\n    with st.spinner(f\"Generating...\"):\n        ….\n    with st.chat_message(\"assistant\"):\n        st.markdown(\"**[initial response]**\")\n….\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"[revised response] \" + generation['response']})\n\n\nThe application maintains a chat history, displaying both user inputs and LLM responses, including the initial response, any critiques generated, and the final revised response. Each step of the LLM’s process is clearly labeled and presented to the user. The interface also includes a Clear Screen button to reset the chat history. When processing a query, the application shows a loading spinner and displays the runtime, providing transparency into the LLM’s operation. This comprehensive UI design allows users to interact with the LLM while observing how constitutional principles are applied to refine the LLM’s outputs.\nTest the solution using the Streamlit UI\nIn the Streamlit application, when a user inputs a query, the application initiates the process by creating and compiling the graph defined earlier. It then streams the execution of this graph, which includes the RAG and critique/revise steps. During this process, the application displays real-time updates for each node’s execution, showing the user what’s happening behind the scenes. The system measures the total runtime, providing transparency about the processing duration. When it’s complete, the application presents the results in a structured manner within the chat interface. It displays the initial LLM-generated response, followed by any critiques made based on the constitutional principles, and finally shows the revised response that incorporates these ethical considerations. This step-by-step presentation allows users to see how the LLM’s response evolves through the constitutional AI process, from initial generation to ethical refinement. As mentioned, in the GitHub README file, in order to run the Streamlit application, use the following code:\n\npip install -r requirements.txt\nstreamlit run main.py\n\n\nFor details on using a Jupyter proxy to access the Streamlit application, refer to Build Streamlit apps in Amazon SageMaker Studio.\nModify the Studio URL, replacing lab? with proxy/8501/.\n\nHow Insagic uses Constitutional AI to generate compliant content\nInsagic uses real-world medical data to help brands understand people as patients and patients as people, enabling them to deliver actionable insights in the healthcare marketing space. Although generating deep insights in the health space can yield profound dividends, it must be done with consideration for compliance and the personal nature of health data. By defining federal guidelines as constitutional principles, Insagic makes sure that the content delivered by generative AI complies with federal guidelines for healthcare marketing.\nClean up\nWhen you have finished experimenting with this solution, clean up your resources to prevent AWS charges from being incurred:\n\nEmpty the S3 buckets.\nDelete the SageMaker notebook instance.\nDelete the Amazon Bedrock knowledge base.\n\nConclusion\nThis post demonstrated how to implement a sophisticated generative AI solution using Amazon Bedrock and LangGraph to generate compliant content. You can also integrate this workflow to generate responses based on a knowledge base and apply ethical principles to critique and revise its outputs, all within an interactive web interface. Insagic is looking at more ways to incorporate this into existing workflows by defining custom principles to achieve compliance goals.\nYou can expand this concept further by incorporating Amazon Bedrock Guardrails. Amazon Bedrock Guardrails and LangGraph Constitutional AI can create a comprehensive safety system by operating at different levels. Amazon Bedrock provides API-level content filtering and safety boundaries, and LangGraph implements constitutional principles in reasoning workflows. Together, they enable multi-layered protection through I/O filtering, topic restrictions, ethical constraints, and logical validation steps in AI applications.\nTry out the solution for your own use case, and leave your feedback in the comments.\n\nAbout the authors\n Sriharsh Adari is a Senior Solutions Architect at Amazon Web Services (AWS), where he helps customers work backwards from business outcomes to develop innovative solutions on AWS. Over the years, he has helped multiple customers on data platform transformations across industry verticals. His core area of expertise include Technology Strategy, Data Analytics, and Data Science. In his spare time, he enjoys playing sports, binge-watching TV shows, and playing Tabla.\nDavid Min is a Senior Partner Sales Solutions Architect at Amazon Web Services (AWS) specializing in Generative AI, where he helps customers transform their businesses through innovative AI solutions. Throughout his career, David has helped numerous organizations across industries bridge the gap between cutting-edge AI technology and practical business applications, focusing on executive engagement and successful solution adoption.\nStephen Garth is a Data Scientist at Insagic, where he develops advanced machine learning solutions, including LLM-powered automation tools and deep clustering models for actionable, consumer insights. With a strong background spanning software engineering, healthcare data science, and computational research, he is passionate to bring his expertise in AI-driven analytics and large-scale data processing to drive solutions.\nChris Cocking specializes in scalable enterprise application design using multiple programming languages. With a nearly 20 years of experience, he excels in LAMP and IIS environments, SEO strategies, and most recently designing agentic systems. Outside of work, Chris is an avid bassist and music lover, which helps fuel his creativity and problem-solving skills.",
      "date": "2025-04-01",
      "authors": "Sriharsh Adari",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "This article discusses the integration of Amazon Bedrock and Constitutional AI to streamline the process of generating compliant content in regulated industries like healthcare and finance. It highlights the use of ethical principles and reflection techniques to ensure the content aligns with legal and ethical standards, as well as detailing how Insagic employs this technology for their marketing workflows.",
      "takeaways": [
        "- Constitutional AI ensures that generative models produce content that adheres to predefined ethical and legal parameters, reducing the need for human oversight.",
        "- The integration of Retrieval Augmented Generation (RAG) and LangGraph facilitates a sophisticated workflow for generating compliant content using large language models.",
        "- The application described includes a user-friendly Streamlit interface that allows users to interactively generate and review AI-generated content, incorporating critiques based on diversity, equity, and inclusion principles."
      ]
    },
    {
      "id": 3,
      "title": "Minimize generative AI hallucinations with Amazon Bedrock Automated Reasoning checks",
      "link": "https://aws.amazon.com/blogs/machine-learning/minimize-generative-ai-hallucinations-with-amazon-bedrock-automated-reasoning-checks/",
      "description": "Foundation models (FMs) and generative AI are transforming enterprise operations across industries. McKinsey & Company’s recent research estimates generative AI could contribute up to $4.4 trillion annually to the global economy through enhanced operational efficiency, productivity growth of 0.1% to 0.6% annually, improved customer experience through personalized interactions, and accelerated digital transformation.\nToday, organizations struggle with AI hallucination when moving generative AI applications from experimental to production environments. Model hallucination, where AI systems generate plausible but incorrect information, remains a primary concern. The 2024 Gartner CIO Generative AI Survey highlights three major risks: reasoning errors from hallucinations (59% of respondents), misinformation from bad actors (48%), and privacy concerns (44%).\nTo improve factual accuracy of large language model (LLM) responses, AWS announced Amazon Bedrock Automated Reasoning checks (in gated preview) at AWS re:Invent 2024. Through logic-based algorithms and mathematical validation, Automated Reasoning checks validate LLM outputs against domain knowledge encoded in the Automated Reasoning policy to help prevent factual inaccuracies. Automated reasoning checks is part of Amazon Bedrock Guardrails, a comprehensive framework that also provides content filtering, personally identifiable information (PII) redaction, and enhanced security measures. Together, these capabilities enable organizations to implement reliable generative AI safeguards—with Automated Reasoning checks addressing factual accuracy while other Amazon Bedrock Guardrails features help protect against harmful content and safeguard sensitive information.\nIn this post, we discuss how to help prevent generative AI hallucinations using Amazon Bedrock Automated Reasoning checks.\nAutomated Reasoning overview\nAutomated Reasoning is a specialized branch of computer science that uses mathematical proof techniques and formal logical deduction to verify compliance with rules and requirements with absolute certainty under given assumptions. As organizations face increasing needs to verify complex rules and requirements with mathematical certainty, automated reasoning techniques offer powerful capabilities. For example, AWS customers have direct access to automated reasoning-based features such as IAM Access Analyzer, S3 Block Public Access, or VPC Reachability Analyzer.\nUnlike probabilistic approaches prevalent in machine learning, Automated Reasoning relies on formal mathematical logic to provide definitive guarantees about what can and can’t be proven. This approach mirrors the rigors of auditors verifying financial statements or compliance officers validating regulatory requirements, but with mathematical precision. By using rigorous logical frameworks and theorem-proving methodologies, Automated Reasoning can conclusively determine whether statements are true or false under given assumptions. This makes it exceptionally valuable for applications that demand high assurance and need to deliver unambiguous conclusions to their users.\nThe following workflow illustrates solver-based formal verification, showing both the process flow and algorithm for verifying formal system properties through logical analysis and SAT/SMT solvers.\n\nOne of the widely used Automated Reasoning techniques is SAT/SMT solving, which involves encoding a representation of rules and requirements into logical formulas. A logical formula is a mathematical expression that uses variables and logical operators to represent conditions and relationships. After the rules and requirements are encoded into these formulas, specialized tools known as solvers are applied to compute solutions that satisfy these constraints. These solvers determine whether the formulas can be satisfied—whether there exist values for variables that make the formulas true.\nThis process starts with two main inputs: a formal representation of the system (like code or a policy) expressed as logical formulas, and a property to analyze (such as whether certain conditions are possible or requirements can be met). The solver can return one of three possible outcomes:\n\nSatisfiable – The solver finds an assignment of values that makes the formulas true, proving that the system can satisfy the given requirements. The solver provides this assignment, which can serve as a concrete example of correct behavior.\nUnsatisfiable – The solver proves that no assignment exists that make all formulas true, proving that the requirements can’t be met. This often comes with information about which constraints are in conflict, helping identify the incorrect assumptions in the system.\nUnknown – In some cases, the solver might not be able to determine satisfiability within reasonable computational limits, or the encoding might not contain enough information to reach a conclusion.\n\nThis technique makes sure that you either get confirmation that the specific property holds (with a concrete example), proof that it can’t be satisfied (with information on conflicting constraints), or an indication that the problem needs to be reformulated or analyzed differently.\nKey features of Automated Reasoning checks\nAutomated Reasoning checks offer the following key features:\n\nMathematical validation framework – The feature verifies LLM outputs using mathematical logical deduction. Unlike probabilistic methods, it uses sound mathematical approaches to provide definitive guarantees about system behaviors within defined parameters.\nPolicy-based knowledge representation – Organizations can create Automated Reasoning policies that encode their rules, procedures, and guidelines into structured, mathematical formats. Organizations can upload documents like PDFs containing HR guidelines or operational workflows, which are then automatically converted into formal logic structures. Policy changes are automatically versioned with unique Amazon Resource Names (ARNs), allowing for change tracking, auditing, and rollback capabilities to maintain consistent policy enforcement.\nDomain expert enablement – The feature is designed to empower domain experts, such as HR personnel or operational managers, to directly encode their knowledge without technical intermediaries. This makes sure that business rules and policies are accurately captured and maintained by those who understand them best.\nNatural language to logic translation – The system uses two complementary approaches: LLMs handle natural language understanding, and a symbolic reasoning engine performs mathematical validation. This hybrid architecture allows users to input policies in plain language while maintaining mathematically rigorous verification.\nExplainable validation results – Each validation check produces detailed findings that indicate whether content is Valid, Invalid, or No Data. The feature provides clear explanations for its decisions, including extracted factual statements, and suggested corrections for invalid content.\nInteractive testing environment – Users can access a chat playground on the Amazon Bedrock console to test and refine policies in real time. The feature supports both interactive testing through the Amazon Bedrock console and automated testing through API integrations, with the ability to export test cases in JSON format for integration into continuous testing pipelines or documentation workflows.\nSeamless AWS integration – The feature integrates directly with Amazon Bedrock Guardrails and can be used alongside other configurable guardrails like Contextual Grounding checks. It can be accessed through both the Amazon Bedrock console and APIs, making it flexible for various implementation needs.\n\nThese features combine to create a powerful framework that helps organizations maintain factual accuracy in their AI applications while providing transparent and mathematically sound validation processes.\nSolution overview\nNow that we understand the key features of Automated Reasoning checks, let’s examine how this capability works within Amazon Bedrock Guardrails. The following section provides a comprehensive overview of the architecture and demonstrates how different components work together to promote factual accuracy and help prevent hallucinations in generative AI applications.\nAutomated Reasoning checks in Amazon Bedrock Guardrails provides an end-to-end solution for validating AI model outputs using mathematically sound principles. This automated process uses formal logic and mathematical proofs to verify responses against established policies, offering definitive validation results that can significantly improve the reliability of your AI applications.\nThe following solution architecture follows a systematic workflow that enables rigorous validation of model outputs.\n\nThe workflow consists of the following steps:\n\nSource documents (such as HR guidelines or operational procedures) are uploaded to the system. These documents, along with optional intent descriptions, are processed to create structured rules and variables that form the foundation of an Automated Reasoning policy.\nSubject matter experts review and inspect the created policy to verify accurate representation of business rules. Each validated policy is versioned and assigned a unique ARN for tracking and governance purposes.\nThe validated Automated Reasoning policy is associated with Amazon Bedrock Guardrails, where specific policy versions can be selected for implementation. This integration enables automated validation of generative AI outputs.\nWhen the generative AI application produces a response, Amazon Bedrock Guardrails triggers the Automated Reasoning checks. The system creates logical representations of both the input question and the application’s response, evaluating them against the established policy rules.\nThe Automated Reasoning check provides detailed validation results, including whether statements are Valid, Invalid, or No Data. For each finding, it explains which rules and variables were considered, and provides suggestions for making invalid statements valid.\n\nWith this solution architecture in place, organizations can confidently deploy generative AI applications knowing that responses will be automatically validated against your established policies using mathematically sound principles.\nPrerequisites\nTo use Automated Reasoning checks in Amazon Bedrock, make sure you have met the following prerequisites:\n\nAn active AWS account\nAccess permission through your AWS Account Manager, because Automated Reasoning checks is currently in gated preview\nConfirmation of AWS Regions where Automated Reasoning checks is available\n\nInput dataset\nFor this post, we examine a sample Paid Leave of Absence (LoAP) policy document as our example dataset. This policy document contains detailed guidelines covering employee eligibility criteria, duration limits, application procedures, and benefits coverage for paid leave. It’s an ideal example to demonstrate how Automated Reasoning checks can validate AI-generated responses against structured business policies, because it contains clear rules and conditions that can be converted into logical statements. The document’s mix of quantitative requirements (such as minimum tenure and leave duration) and qualitative conditions (like performance status and approval processes) makes it particularly suitable for showcasing the capabilities of automated reasoning validation.\nThe following screenshot shows an example of our policy document.\n\nStart an Automated Reasoning check using the Amazon Bedrock console\nThe first step is to encode your knowledge—in this case, the sample LoAP policy—into an Automated Reasoning policy. Complete the following steps to initiate an Automated Reasoning check using the Amazon Bedrock console:\n\nOn the Amazon Bedrock console, choose Automated Reasoning Preview under Safeguards in the navigation pane.\nChoose Create policy.\n\n\n\nProvide a policy name and policy description.\n\n\n\nUpload your source document. The source content can’t be modified after creation and must not exceed 6,000 characters with limitations on table sizes and image processing.\nInclude a description of the intent of the Automated Reasoning policy you’re creating. For the sample policy, you can use the following intent:\n\n\nCreate a logical model of the Leave of Absence, Paid (LoAP) policy in this document.\nEmployees will ask questions about what are the eligibility requirements for the program,\nwhether they are allowed to take LOAP and for how long, duration and benefits during the\ntime off, and return to work. \nBelow is an example question:\nQUESTION: I am a temporary contractor working in operations. Am I eligible for LOAP?\nANSWER: No, only full-time employees are eligible for LoAP.\n\nThe policy creation process takes a few minutes to complete. The rules and variables are created after creating the policy and they can be edited, removed, or have new rules or variables added to them.\n\nThe policy document version is outlined in the details section along with the intent description and build status.\n\nNext, you create a guardrail in Amazon Bedrock by configuring as many filters as you need.\n\nOn the Amazon Bedrock console, choose Guardrails under Safeguards in the navigation pane.\nChoose Create guardrail.\n\n\n\nProvide guardrail details such as a name and an optional description.\n\n\n\nAdd an Automated Reasoning check by choosing Enable Automated Reasoning policy, and choose the policy name and version.\nChoose Next and complete the creation of the guardrail.\n\n\n\nNavigate back to the Automated Reasoning section of the Amazon Bedrock console and open the newly created policy. You can use the test playground and input sample questions and answers that represent real user interactions with your LLM.\nChoose the guardrail you created, then choose Submit to evaluate how your policy handles these exchanges.\n\n\nAfter submitting, you’ll be presented with one or more findings. A finding contains a set of facts that were extracted from the input Q&A and are analyzed independently. Each finding includes four key components:\n\nValidation results – Shows the outcome of Automated Reasoning checks. The system determines these results by evaluating extracted variable assignments against your defined policy rules.\nApplied rules – Displays the specific rules from your policy that were used to reach the validation conclusion.\nExtracted variables – Lists the variables that were identified and used in the validation process.\nSuggestions – Shows variable assignments that would make invalid responses valid, or for valid responses, identifies necessary assumptions. These can be used to generate feedback for your LLM.\n\n\nFinally, you can use the feedback suggestions to improve your LLM’s responses.\n\nCollect rules from valid results with suggestions and invalid results.\nFeed these collected variables and rules back to your LLM to revise its original.\nRefine your policy: \n  \nEdit incorrect rules using natural language.\nImprove variable descriptions when Automated Reasoning checks fail to assign values.\nFor effective variable descriptions, include both technical definitions and common user expressions. For example, for a variable named is_full_time, \"works more than 20 hours per week\" is technically correct because it’s a quote from the source policy, but won’t help Automated Reasoning checks understand what users mean when they say “part-time.” Instead, use \"works more than 20 hours per week; set to true if user says 'full-time' and false if user says 'part-time'\".\n \n\nStart an Automated Reasoning check using Python SDK and APIs\nFirst, you need to create an Automated Reasoning policy from your documents using the Amazon Bedrock console as outlined in the previous section. Next, you can use the policy created with the ApplyGuardrail API to validate your generative AI application.\nTo use the Python SDK for validation using Automated Reasoning checks, follow these steps:\n\nFirst, set up the required configurations:\n\n\nimport boto3\nimport botocore\nimport os\nimport json\n\n# Configuration parameters\nDEFAULT_GUARDRAIL_NAME = \"<YOUR_GUARDRAIL_NAME>\"  # e.g., \"my_policy_guardrail\"\nDEFAULT_AR_POLICY_VERSION = \"1\"\n\n# AWS configuration\nregion = \"us-west-2\"\nar_policy = \"<YOUR_AR_POLICY_ID>\"  # e.g., \"ABC123DEF456\"\nmodel_id = \"<YOUR_MODEL_ID>\"  # e.g., \"anthropic.claude-3-haiku-20240307-v1:0\"\n\n\nBefore using Amazon Bedrock with Automated Reasoning policies, you will need to load the required service models. After being allowlisted for Amazon Bedrock access, you will receive two model files along with their corresponding version information. The following is a Python script to help you load these service models:\n\n\n\ndef add_service_model(model_file, service_name, version):\n    \"\"\"\n    Adds a service model to the AWS configuration directory.\n    \n    Args:\n        model_file (str): Path to the model file\n        service_name (str): Name of the AWS service\n        version (str): Service model version\n    \"\"\"\n    # Configure paths\n    source = f\"models/{model_file}\"  # Your downloaded model files directory\n    dest_dir = os.path.expanduser(f\"~/.aws/models/{service_name}/{version}\")\n    dest_file = f\"{dest_dir}/service-2.json\"\n\n    try:\n        # Create directory and copy model file\n        os.makedirs(dest_dir, exist_ok=True)\n        with open(source) as f:\n            model = json.load(f)\n        with open(dest_file, 'w') as f:\n            json.dump(model, f, indent=2)\n        print(f\"Successfully added model for {service_name}\")\n        return True\n    except Exception as e:\n        print(f\"Error adding {service_name} model: {e}\")\n        return False\n\ndef main():\n    # Define your model files and versions\n    # Replace with your actual model information provided by AWS\n    models = {\n        '<bedrock-model-file>.json': ('bedrock', '<bedrock-version>'),\n        '<runtime-model-file>.json': ('bedrock-runtime', '<runtime-version>')\n    }\n    \n    # Load each model\n    for model_file, (service_name, version) in models.items():\n        add_service_model(model_file, service_name, version)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nAfter you set up the service models, initialize the AWS clients for both Amazon Bedrock and Amazon Bedrock Runtime services. These clients will be used to interact with the models and apply guardrails.\n\n\n# Initialize AWS clients\nboto_session = boto3.Session(region_name=region)\nruntime_client = boto_session.client(\"bedrock-runtime\")\nbedrock_client = boto_session.client(\"bedrock\")\n\n\nBefore applying Automated Reasoning policies, you need to either locate an existing guardrail or create a new one. The following code first attempts to find a guardrail by name, and if not found, creates a new guardrail with the specified Automated Reasoning policy configuration. This makes sure you have a valid guardrail to work with before proceeding with policy enforcement.\n\n\ndef find_guardrail_id(client, name) -> tuple[str, str]:\n    \"\"\"\n    Finds the ID and version of a guardrail by its name.\n    \n    Args:\n        client: The Bedrock client object\n        name (str): Name of the guardrail to find\n    \n    Returns:\n        tuple[str, str]: Guardrail ID and version if found, None otherwise\n    \"\"\"\n    next_token = None\n    while True:\n        # List existing guardrails\n        resp = client.list_guardrails(\n        ) if next_token is None else client.list_guardrail(nextToken=next_token)\n\n        # Search for matching guardrail\n        for g in resp[\"guardrails\"]:\n            if g[\"name\"] == name:\n                return g[\"id\"], g[\"version\"]\n\n        # Handle pagination\n        if \"nextToken\" in resp and resp[\"nextToken\"] != \"\":\n            next_token = resp[\"nextToken\"]\n        else:\n            break\n    return None, None\n\n# Find or create guardrail with AR policy\ntry:\n    # First, try to find existing guardrail\n    guardrail_id, guardrail_version = find_guardrail_id(\n        bedrock_client, DEFAULT_GUARDRAIL_NAME)\n    \n    # If not found, create new guardrail\n    if guardrail_id is None:\n        create_resp = bedrock_client.create_guardrail(\n            name=DEFAULT_GUARDRAIL_NAME,\n            description=\"Automated Reasoning checks demo guardrail\",\n            automatedReasoningPolicyConfig={\n                \"policyIdentifier\": ar_policy,\n                \"policyVersion\": DEFAULT_AR_POLICY_VERSION\n            },\n            blockedInputMessaging='Input is blocked',\n            blockedOutputsMessaging='Output is blocked',\n        )\n        guardrail_id = create_resp[\"guardrailId\"]\n        guardrail_version = create_resp[\"version\"]\n        print(f\"✓ Created new guardrail: {guardrail_id}\")\n    else:\n        print(f\"✓ Found existing guardrail: {guardrail_id}\")\n        \nexcept botocore.exceptions.ClientError as e:\n    print(f\"✗ Error managing guardrail: {str(e)}\")\n    raise\n\n\nWhen testing guardrails with Automated Reasoning policies, you need to properly format your input data. The following code shows how to structure a sample question and answer pair for validation:\n\n\ndef create_sample_input():\n    \"\"\"\n    Creates a formatted sample input for guardrail validation.\n    \n    The format requires both the query and response to be properly structured\n    with appropriate qualifiers.\n    \n    Returns:\n        list: Formatted input for guardrail validation\n    \"\"\"\n    sample_query = \"I am a part-time employee, am I eligible for LoAP?\"\n    sample_response = \"Yes, part time employees are allowed to use LoAP\"\n    \n    return [\n        {\n            \"text\": {\n                \"text\": sample_query,\n                \"qualifiers\": [\"query\"]\n            }\n        },\n        {\n            \"text\": {\n                \"text\": sample_response,\n                \"qualifiers\": [\"guard_content\"]\n            }\n        }\n    ]\n\n# Example usage\nguardrail_input = create_sample_input()\n\nprint(json.dumps(guardrail_input, indent=2))\n\n\nNow that you have your formatted input data, you can apply the guardrail with Automated Reasoning policies to validate the content. The following code sends the input to Amazon Bedrock Guardrails and returns the validation results:\n\n\nguardrails_output = runtime_client.apply_guardrail(\n            guardrailIdentifier= guardrail_id,\n            guardrailVersion= guardrail_version,\n            source=\"OUTPUT\",\n            content=guardrail_input,\n        )\n\n\nAfter applying guardrails, you need to extract and analyze the Automated Reasoning assessment results. The following code shows how to process the guardrail output:\n\n\n# Extract Automated Reasoning assessment\nar_assessment = None\nfor assessment in guardrails_output[\"assessments\"]:\n    if \"automatedReasoningPolicy\" in assessment:\n        ar_assessment = assessment[\"automatedReasoningPolicy\"][\"findings\"]\n        break\n\nif ar_assessment is None:\n    print(\"No Automated Reasoning assessment found\")\nelse:\n    print(\"Automated Reasoning Assessment Results:\")\n    print(json.dumps(ar_assessment, indent=2))\n\n    # Process any policy violations\n    for finding in ar_assessment:\n        if finding[\"result\"] == \"INVALID\":\n            print(\"\\nPolicy Violations Found:\")\n            # Print violated rules\n            for rule in finding.get(\"rules\", []):\n                print(f\"Rule: {rule['description']}\")\n            \n            # Print suggestions if any\n            if \"suggestions\" in finding:\n                print(\"\\nSuggested Corrections:\")\n                for suggestion in finding[\"suggestions\"]:\n                    print(f\"- {suggestion}\")\n\nThe output will look something like the following:\n\n{\n    \"result\": \"INVALID\",\n    \"assignments\": [...],\n    \"suggestions\": [...],\n    \"rules\": [\n        {\n            \"identifier\": \"<IDENTIFIER>\",\n            \"description\": \"An employee is eligible for LoAP if and only if...\"\n        }\n    ]\n}\n\nWhen a response violates AR policies, the system identifies which rules were violated and provides information about the conflicts. The feedback from the AR policy validation can be routed back to improve the model’s output, promoting compliance while maintaining response quality.\nPossible use cases\nAutomated Reasoning checks can be applied across various industries to promote accuracy, compliance, and reliability in AI-generated responses while maintaining industry-specific standards and regulations. Although we have tested these checks across multiple applications, we continue to explore additional potential use cases. The following table provides some applications across different sectors.\n\n\n\nIndustry\nUse Cases\n\n\nHealthcare\n\n\nValidate AI-generated treatment recommendations against clinical care protocols and guidelines\nVerify medication dosage calculations and check for potential drug interactions\nMake sure patient education materials align with medical best practices\nValidate clinical documentation for regulatory compliance\n \n\n\nFinancial Services\n\n\nVerify investment recommendations against regulatory requirements and risk policies\nValidate customer communications for compliance with financial regulations\nVerify that credit decision explanations meet fairness and transparency guidelines\nCheck transaction processing against anti-fraud and anti-money laundering policies\n \n\n\nTravel and Hospitality\n\n\nValidate booking and ticketing policies for accuracy\nVerify loyalty program benefit calculations follow established rules\nVerify travel documentation requirements and restrictions\nValidate pricing and refund calculations\n \n\n\nInsurance\n\n\nVerify claim processing decisions against policy terms\nValidate coverage explanations for accuracy and completeness\nMake sure that risk assessment recommendations follow underwriting guidelines\nCheck policy documentation for regulatory compliance\n \n\n\nEnergy and Utilities\n\n\nValidate maintenance scheduling against equipment specifications\nVerify emergency response protocols for different scenarios\nMake sure that field operation instructions follow safety guidelines\nCheck grid management decisions against operational parameters\n \n\n\nManufacturing\n\n\nValidate quality control procedures against industry standards\nVerify production scheduling against capacity and resource constraints\nMake sure that safety protocols are followed in operational instructions\nCheck inventory management decisions against supply chain policies\n \n\n\n\nBest practices for implementation\nSuccessfully implementing Automated Reasoning checks requires careful attention to detail and a systematic approach to achieve optimal validation accuracy and reliable results. The following are some key best practices:\n\nDocument preparation – Use structured text-based PDF documents. Content should be limited to 6,000 characters. Avoid complex formatting that could interfere with the logical model generation.\nIntent description engineering – Create precise policy intents using a clear format. The intent should comprehensively cover expected use cases and potential edge cases. For example: \n  \nCreate a logical model for [USE CASE] with policy rules. \nUsers will ask questions about [SPECIFIC TOPICS].\nExample Q&A: [INCLUDE SAMPLE].\n \nPolicy validation – Review the generated rules and variables to make sure they accurately capture your business logic and policy requirements. Regular audits of these rules help maintain alignment with current business policies.\nComprehensive testing –Develop a diverse set of sample Q&As in the test playground to evaluate different validation scenarios (valid, valid with suggestions, and invalid responses). Include edge cases and complex scenarios to provide robust validation coverage.\nIterative improvement –Regularly update rules and LLM applications based on validation feedback, paying special attention to suggested variables and invalid results to enhance response accuracy. Maintain a feedback loop for continuous refinement.\nVersion control management – Implement a systematic approach to policy versioning, maintaining detailed documentation of changes and conducting proper testing before deploying new versions. This helps track policy evolution and facilitates rollbacks if needed.\nError handling strategy – Develop a comprehensive plan for handling different validation results, including specific procedures for managing invalid responses and incorporating suggested improvements into the response generation process.\nRuntime optimization – Understand and monitor the two-step validation process (fact extraction and logic validation) to achieve optimal performance. Regularly review validation results to identify patterns that might indicate needed improvements in variable descriptions or rule definitions.\nFeedback integration – Establish a systematic process for collecting and analyzing validation feedback, particularly focusing on cases where NO_DATA is returned or when factual claims are incorrectly extracted. Use this information to continuously refine variable descriptions and policy rules.\n\nConclusion\nAmazon Bedrock Automated Reasoning checks represent a significant advancement in formally verifying the outputs of generative AI applications. By combining rigorous mathematical validation with a user-friendly interface, this feature addresses one of the most critical challenges in AI deployment: maintaining factual consistency and minimizing hallucinations. The solution’s ability to validate AI-generated responses against established policies using formal logic provides organizations with a powerful framework for building trustworthy AI applications that can be confidently deployed in production environments.\nThe versatility of Automated Reasoning checks, demonstrated through various industry use cases and implementation approaches, makes it a valuable tool for organizations across sectors. Whether implemented through the Amazon Bedrock console or programmatically using APIs, the feature’s comprehensive validation capabilities, detailed feedback mechanisms, and integration with existing AWS services enable organizations to establish quality control processes that scale with their needs. The best practices outlined in this post provide a foundation for organizations to maximize the benefits of this technology while maintaining high standards of accuracy.\nAs enterprises continue to expand their use of generative AI, the importance of automated validation mechanisms becomes increasingly critical. We encourage organizations to explore Amazon Bedrock Automated Reasoning checks and use its capabilities to build more reliable and accurate AI applications. To help you get started, we’ve provided detailed implementation guidance, practical examples, and a Jupyter notebook with code snippets in our GitHub repository that demonstrate how to effectively integrate this feature into your generative AI development workflow. Through systematic validation and continuous refinement, organizations can make sure that their AI applications deliver consistent, accurate, and trustworthy results.\n\nAbout the Authors\nAdewale Akinfaderin is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.\nNafi Diallo is a Sr. Applied Scientist in the Automated Reasoning Group and holds a PhD in Computer Science. She is passionate about using automated reasoning to ensure the security of computer systems, improve builder productivity, and enable the development of trustworthy and responsible AI workloads. She worked for more than 5 years in the AWS Application Security organization, helping build scalable API security testing solutions and shifting security assessment left.",
      "date": "2025-04-01",
      "authors": "Adewale Akinfaderin",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the launch of Amazon Bedrock's Automated Reasoning checks aimed at minimizing generative AI hallucinations by providing a mathematical framework to validate the outputs of large language models against encoded domain knowledge. This initiative addresses challenges in ensuring factual accuracy as generative AI applications transition from experimental to production environments.",
      "takeaways": [
        "- Amazon Bedrock Automated Reasoning checks utilize formal mathematical logic to verify AI-generated content, thus enhancing the reliability of generative AI applications.",
        "- The tool enables organizations to encode their policies into structured, mathematical formats and provides detailed validation results, improving transparency and accuracy in AI outputs.",
        "- Key features of the solution include natural language to logic translation, interactive testing environments, and seamless integration with existing AWS services, designed to empower domain experts in policy management."
      ]
    },
    {
      "id": 4,
      "title": "AWS App Studio introduces a prebuilt solutions catalog and cross-instance Import and Export",
      "link": "https://aws.amazon.com/blogs/machine-learning/aws-app-studio-introduces-a-prebuilt-solutions-catalog-and-cross-instance-import-and-export/",
      "description": "AWS App Studio is a generative AI-powered service that uses natural language to build business applications, empowering a new set of builders to create applications in minutes. With App Studio, technical professionals such as IT project managers, data engineers, enterprise architects, and solution architects can quickly develop applications tailored to their organization’s needs—without requiring deep software development skills. Common use cases range from inventory management and approval workflows to content management and operational portals, and beyond – App Studio adapts to streamline a wide variety of business applications.\nSince the general availability of App Studio in November 2024, customers across diverse industries have adopted it to build scalable, enterprise-grade applications, transforming their development processes and accelerating time-to-market. App Studio customers, including both enterprises and system integrators, have shared the need for portability and reusability across App Studio instances. Based on their experience, two areas of interests emerged:\n\nGetting started – New customers and builders asked to learn and explore the product through readily available examples and patterns that explain application building possibilities in App Studio.\nOptimizing time to value – Teams often validate use cases in a sandbox before moving to production. This highlights an interest in a more efficient approach to share and deploy applications across multiple App Studio instances.\n\nToday, App Studio announced two new features to accelerate application building:\n\nPrebuilt solutions catalog – Featuring a set of practical examples and common patterns (like S3 and Bedrock integration) to accelerate getting started and enable deployment of applications from the catalog to production environments in less than 15 minutes.\nCross-instance Import and Export – Enabling straightforward and self-service migration of App Studio applications across AWS Regions and AWS accounts.\n\nIn this post, we walk through how to use the prebuilt solutions catalog to get started quickly and use the Import and Export feature\nPrerequisites\nTo follow along with this post, you should have the following prerequisites:\n\nAccess to App Studio. For more information, see Setting up and signing in to App Studio.\nOptional: Review App Studio concepts to familiarize yourself with important App Studio concepts.\nOptional: An understanding of basic web development concepts, such as JavaScript syntax.\nOptional: Familiarity with AWS services.\n\nPrebuilt solutions catalog\nApp Studio is introducing a prebuilt solutions catalog to accelerate the way builders approach application building. This resource offers a diverse collection of prebuilt applications that can be seamlessly imported into your App Studio instance, serving as both a learning tool and a rapid deployment solution. By providing access to proven patterns and prebuilt solutions, App Studio significantly reduces the initial setup time for builders, enabling you to move from concept to production in less than 15 minutes.\nThe catalog includes a variety of practical use cases including a Product Adoption Tracker to manage customer feedback, track feature requests, and summarize meeting notes with AI. To import the Product Adoption Tracker, navigate to the prebuilt solutions catalog, copy an import code, and follow the import instructions in the next section.\n\nImport an application\nYou now have the ability to import an App Studio application from a different App Studio instance. Importing applications is available to all builders and admins.\nComplete the following steps to import an App Studio application:\n\nSign in and launch the App Studio instance where you want to import an application.\nChoose My applications in the navigation pane.\nChoose the dropdown menu next to Create app and choose Import app. \n\n\nEnter an import code from the prebuilt app catalog or that you generated by the export process outlined in the next section and choose Import. Depending on the application size, you might need to wait a few seconds for the import to finish. \nAfter completion, the application will be imported to your development environment. You can explore the debug panel at the bottom of the page to understand which custom connectors need to be connected to automations and entities. \n\nNow that we have successfully imported an application, let’s walk through how we can export our own applications to a different App Studio instance.\nExport an application\nYou now have the ability to export an App Studio application to a different App Studio instance. Generating an application export creates a static snapshot with all artifacts needed to recreate the application—automations, components, and entities. After importing, you will need to reconnect custom connectors to automations and entities.\nApplication security and control are maintained through a robust permissions system. Only authorized application owners and co-owners can generate application exports and restrict which App Studio instances can import a given application. If needed, application owners can revoke access by deactivating the import link at any time.\nTo export an App Studio application, complete the following steps:\n\n\n\nSign in to the App Studio instance that you want to export an application from.\nChoose My applications in the navigation pane.\nChoose the dropdown menu next to Edit and choose Export. \nTo restrict which App Studio instances can import this application, configure application import permissions: \n    \nAnyone with the import code can import this application – Grant import permissions to all instances. Only select this option if you want anyone with the import code to have access to import your application.\nOnly specified App Studio instances can import this application – Provide the specific instance IDs that can import the application (multiple instances can be separated by commas). To find your instance ID, navigate to your instance’s account settings by choosing Account settings on the App Studio console.\n \nChoose Generate import code to generate a unique import code. \nTwo additional options for managing import codes are available after the application has been exported at least once to application owners and co-owners: \n    \nGenerate new import code – When you make updates to this application, you will need to generate a new import code by choosing Generate new code. Generating a new code invalidates the old code, but will not automatically refresh existing imported applications.\nDelete import code – To stop application import access, choose this option. Deleting the import code will invalidate the current code and prevent subsequent import attempts. Applications previously created using this code will continue to work.\n \n \n\n\nConsiderations\nThe following are some key considerations for using the prebuilt solutions catalog and importing and exporting applications across App Studio instances:\n\nThere is no cost associated with importing and exporting applications, including importing applications from the prebuilt solutions catalog.\nApplications cannot be imported into the same instance, but you can achieve a similar result of replicating functionality within an instance by duplicating apps, components, and pages.\nThere are no limits on the number of applications you can import or export. The maximum number of applications in an App Studio instance is subject to service quotas.\n\nConclusion\nJumpstart your app building workflow with App Studio’s prebuilt solutions catalog and Import and Export features. Effortlessly migrate applications across AWS instances, collaborate with teams, and transfer applications to clients. Start using App Studio’s prebuilt solutions catalog and Import and Export features today – we’re excited to see how you will use these features to accelerate your application building journey.\nTo learn more about App Studio, explore more features on the App Studio page. Get started with App Studio in the AWS Management Console. Experience the App Studio workshop for hands-on learning, and join the conversation in the #aws-app-studio channel in the AWS Developers Slack workspace.\nRead more about App Studio\n\nBuild and modify apps using natural language with App Studio, now generally available\nDiscover how customers are innovating with App Studio\n\nWatch App Studio demos\n\nThe Fastest and Easiest Way to Build Business Applications\nWatch how the NFL accelerated image processing with App Studio\n\n\n\nAbout the Authors\nUmesh Kalaspurkar is a Principal Solutions Architect at AWS based in New York, bringing over two decades of expertise in digital transformation and innovation across both enterprise and startup environments. He specializes in designing solutions that help organizations overcome their most pressing challenges. When not architecting cloud solutions, Umesh cherishes time spent with his children, carving down ski slopes, and exploring new destinations around the world.\nSamit Kumbhani is an AWS Senior Solutions Architect in the New York City area with over 18 years of experience. He currently partners with independent software vendors (ISVs) to build highly scalable, innovative, and secure cloud solutions. Outside of work, Samit enjoys playing cricket, traveling, and biking.\nHaoran (Hao) Su is a Senior Technical Account Manager in New York City with over 8 years of experience with the cloud. He collaborates with Software, Internet and Model providers (SWIM) and Digitally Native Businesses (DNB) to improve their financial and operational efficiency, and architectural resiliency. Outside of work, Hao enjoys international traveling, exercising, and streaming.\nAnshika Tandon is a Senior Product Manager – Technical at AWS with a decade of experience building AI and B2B SaaS products from concept to launch. She excels in cross-functional product leadership, focusing on delivering measurable business value through strategic initiatives. A global citizen having lived in 10 cities and visited 26 countries, Anshika balances her professional life with interests in skiing, travel, and performing in improv comedy shows.\nAlex (Tao) Jia is a Senior Product Marketing Manager at AWS, focusing on generative AI. With 15+ years in tech marketing, she drives products from concept to scale, shaping positioning, fostering adoption, and leading global go-to-market strategies. She has worked with enterprises and ISVs, reaching millions of developers. Outside work, Alex enjoys exploring technology’s impact on humanity through books, research, and conversations.",
      "date": "2025-04-01",
      "authors": "Umesh Kalaspurkar",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the new features introduced in AWS App Studio, a generative AI-powered service that simplifies business application development. It highlights the addition of a prebuilt solutions catalog and cross-instance import/export capabilities, aimed at expediting the application building process for users across various industries.",
      "takeaways": [
        "- AWS App Studio now includes a prebuilt solutions catalog that provides practical examples and patterns to accelerate application deployment and learning.",
        "- The cross-instance import and export feature allows users to easily migrate applications across different AWS Regions and accounts, enhancing flexibility and collaboration.",
        "- Users can quickly move from concept to production with App Studio, enabling faster development cycles and improved time-to-market for enterprise-grade applications."
      ]
    },
    {
      "id": 5,
      "title": "Build agentic systems with CrewAI and Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-agentic-systems-with-crewai-and-amazon-bedrock/",
      "description": "This post is co-authored with Joao Moura and Tony Kipkemboi from CrewAI.\nThe enterprise AI landscape is undergoing a seismic shift as agentic systems transition from experimental tools to mission-critical business assets. In 2025, AI agents are expected to become integral to business operations, with Deloitte predicting that 25% of enterprises using generative AI will deploy AI agents, growing to 50% by 2027. The global AI agent space is projected to surge from $5.1 billion in 2024 to $47.1 billion by 2030, reflecting the transformative potential of these technologies.\nIn this post, we explore how CrewAI’s open source agentic framework, combined with Amazon Bedrock, enables the creation of sophisticated multi-agent systems that can transform how businesses operate. Through practical examples and implementation details, we demonstrate how to build, deploy, and orchestrate AI agents that can tackle complex tasks with minimal human oversight. Although “agents” is the buzzword of 2025, it’s important to understand what an AI agent is and where deploying an agentic system could yield benefits.\nAgentic design\nAn AI agent is an autonomous, intelligent system that uses large language models (LLMs) and other AI capabilities to perform complex tasks with minimal human oversight. Unlike traditional software, which follows pre-defined rules, AI agents can operate independently, learn from their environment, adapt to changing conditions, and make contextual decisions. They are designed with modular components, such as reasoning engines, memory, cognitive skills, and tools, that enable them to execute sophisticated workflows. Traditional SaaS solutions are designed for horizontal scalability and general applicability, which makes them suitable for managing repetitive tasks across diverse sectors, but they often lack domain-specific intelligence and the flexibility to address unique challenges in dynamic environments. Agentic systems, on the other hand, are designed to bridge this gap by combining the flexibility of context-aware systems with domain knowledge. Consider a software development use case AI agents can generate, evaluate, and improve code, shifting software engineers’ focus from routine coding to more complex design challenges. For example, for the CrewAI git repository, pull requests are evaluated by a set of CrewAI agents who review code based on code documentation, consistency of implementation, and security considerations. Another use case can be seen in supply chain management, where traditional inventory systems might track stock levels, but lack the capability to anticipate supply chain disruptions or optimize procurement based on industry insights. In contrast, an agentic system can use real-time data (such as weather or geopolitical risks) to proactively reroute supply chains and reallocate resources. The following illustration describes the components of an agentic AI system:\n\nOverview of CrewAI\nCrewAI is an enterprise suite that includes a Python-based open source framework. It simplifies the creation and management of AI automations using either AI flows, multi-agent systems, or a combination of both, enabling agents to work together seamlessly, tackling complex tasks through collaborative intelligence. The following figure illustrates the capability of CrewAI’s enterprise offering:\n\nCrewAI’s design centers around the ability to build AI automation through flows and crews of AI agents. It excels at the relationship between agents and tasks, where each agent has a defined role, goal, and backstory, and can access specific tools to accomplish their objectives. This framework allows for autonomous inter-agent delegation, where agents can delegate tasks and inquire among themselves, enhancing problem-solving efficiency. This growth is fueled by the increasing demand for intelligent automation and personalized customer experiences across sectors like healthcare, finance, and retail.\nCrewAI’s agents are not only automating routine tasks, but also creating new roles that require advanced skills. CrewAI’s emphasis on team collaboration, through its modular design and simplicity principles, aims to transcend traditional automation, achieving a higher level of decision simplification, creativity enhancement, and addressing complex challenges.\nCrewAI key concepts\nCrewAI’s architecture is built on a modular framework comprising several key components that facilitate collaboration, delegation, and adaptive decision-making in multi-agent environments. Let’s explore each component in detail to understand how they enable multi-agent interactions.\nAt a high level, CrewAI creates two main ways to create agentic automations: flows and crews.\nFlows\nCrewAI Flows provide a structured, event-driven framework to orchestrate complex, multi-step AI automations seamlessly. Flows empower users to define sophisticated workflows that combine regular code, single LLM calls, and potentially multiple crews, through conditional logic, loops, and real-time state management. This flexibility allows businesses to build dynamic, intelligent automation pipelines that adapt to changing conditions and evolving business needs. The following figure illustrates the difference between Crews and Flows:\n\nWhen integrated with Amazon Bedrock, CrewAI Flows unlock even greater potential. Amazon Bedrock provides a robust foundation by enabling access to powerful foundation models (FMs).\nFor example, in a customer support scenario, a CrewAI Flow orchestrated through Amazon Bedrock could automatically route customer queries to specialized AI agent crews. These crews collaboratively diagnose customer issues, interact with backend systems for data retrieval, generate personalized responses, and dynamically escalate complex problems to human agents only when necessary.\nSimilarly, in financial services, a CrewAI Flow could monitor industry conditions, triggering agent-based analysis to proactively manage investment portfolios based on industry volatility and investor preferences.\nTogether, CrewAI Flows and Amazon Bedrock create a powerful synergy, enabling enterprises to implement adaptive, intelligent automation that addresses real-world complexities efficiently and at scale.\nCrews\nCrews in CrewAI are composed of several key components, which we discuss in this section.\nAgents\nAgents in CrewAI serve as autonomous entities designed to perform specific roles within a multi-agent system. These agents are equipped with various capabilities, including reasoning, memory, and the ability to interact dynamically with their environment. Each agent is defined by four main elements:\n\nRole – Determines the agent’s function and responsibilities within the system\nBackstory – Provides contextual information that guides the agent’s decision-making processes\nGoals – Specifies the objectives the agent aims to accomplish\nTools – Extends the capabilities of agents to access more information and take actions\n\nAgents in CrewAI are designed to work collaboratively, making autonomous decisions, delegating tasks, and using tools to execute complex workflows efficiently. They can communicate with each other, use external resources, and refine their strategies based on observed outcomes.\nTasks\nTasks in CrewAI are the fundamental building blocks that define specific actions an agent needs to perform to achieve its objectives. Tasks can be structured as standalone assignments or interdependent workflows that require multiple agents to collaborate. Each task includes key parameters, such as:\n\nDescription – Clearly defines what the task entails\nAgent assignment – Specifies which agent is responsible for executing the task\n\nTools\nTools in CrewAI provide agents with extended capabilities, enabling them to perform actions beyond their intrinsic reasoning abilities. These tools allow agents to interact with APIs, access databases, execute scripts, analyze data, and even communicate with other external systems. CrewAI supports a modular tool integration system where tools can be defined and assigned to specific agents, providing efficient and context-aware decision-making.\nProcess\nThe process layer in CrewAI governs how agents interact, coordinate, and delegate tasks. It makes sure that multi-agent workflows operate seamlessly by managing task execution, communication, and synchronization among agents.\nMore details on CrewAI concepts can be found in the CrewAI documentation.\nCrewAI enterprise suite\nFor businesses looking for tailored AI agent solutions, CrewAI provides an enterprise offering that includes dedicated support, advanced customization, and integration with enterprise-grade systems like Amazon Bedrock. This enables organizations to deploy AI agents at scale while maintaining security and compliance requirements.\nEnterprise customers get access to comprehensive monitoring tools that provide deep visibility into agent operations. This includes detailed logging of agent interactions, performance metrics, and system health indicators. The monitoring dashboard enables teams to track agent behavior, identify bottlenecks, and optimize multi-agent workflows in real time.\nReal-world enterprise impact\nCrewAI customers are already seeing significant returns by adopting agentic workflows in production. In this section, we provide a few real customer examples.\nLegacy code modernization\nA large enterprise customer needed to modernize their legacy ABAP and APEX code base, a typically time-consuming process requiring extensive manual effort for code updates and testing.\nMultiple CrewAI agents work in parallel to:\n\nAnalyze existing code base components\nGenerate modernized code in real time\nExecute tests in production environment\nProvide immediate feedback for iterations\n\nThe customer achieved approximately 70% improvement in code generation speed while maintaining quality through automated testing and feedback loops. The solution was containerized using Docker for consistent deployment and scalability. The following diagram illustrates the solution architecture.\n\nBack office automation at global CPG company\nA leading CPG company automated their back-office operations by connecting their existing applications and data stores to CrewAI agents that:\n\nResearch industry conditions\nAnalyze pricing data\nSummarize findings\nExecute decisions\n\nThe implementation resulted in a 75% reduction in processing time by automating the entire workflow from data analysis to action execution. The following diagram illustrates the solution architecture.\n\nGet started with CrewAI and Amazon Bedrock\nAmazon Bedrock integration with CrewAI enables the creation of production-grade AI agents powered by state-of-the-art language models.\nThe following is a code snippet on how to set up this integration:\n\nfrom crewai import Agent, Crew, Process, Task, LLM\nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool\nimport os\n\n# Configure Bedrock LLM\nllm = LLM(\n    model=\"bedrock/anthropic. anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n    aws_region_name=os.getenv('AWS_REGION_NAME')\n)\n\n# Create an agent with Bedrock as the LLM provider\nsecurity_analyst = Agent(\n    config=agents_config['security_analyst'],\n    tools=[SerperDevTool(), ScrapeWebsiteTool()],\n    llm=llm\n)\n\nCheck out the CrewAI LLM documentation for detailed instructions on how to configure LLMs with your AI agents.\nAmazon Bedrock provides several key advantages for CrewAI applications:\n\nAccess to state-of-the-art language models such as Anthropic’s Claude and Amazon Nova – These models provide the cognitive capabilities that power agent decision-making. The models enable agents to understand complex instructions, generate human-like responses, and make nuanced decisions based on context.\nEnterprise-grade security and compliance features – This is crucial for organizations that need to maintain strict control over their data and enforce compliance with various regulations.\nScalability and reliability backed by AWS infrastructure – This means your agent systems can handle increasing workloads while maintaining consistent performance.\n\nAmazon Bedrock Agents and Amazon Bedrock Knowledge Bases as native CrewAI Tools\nAmazon Bedrock Agents offers you the ability to build and configure autonomous agents in a fully managed and serverless manner on Amazon Bedrock. You don’t have to provision capacity, manage infrastructure, or write custom code. Amazon Bedrock manages prompt engineering, memory, monitoring, encryption, user permissions, and API invocation. BedrockInvokeAgentTool enables CrewAI agents to invoke Amazon Bedrock agents and use their capabilities within your workflows.\nWith Amazon Bedrock Knowledge Bases, you can securely connect FMs and agents to your company data to deliver more relevant, accurate, and customized responses. BedrockKBRetrieverTool enables CrewAI agents to retrieve information from Amazon Bedrock Knowledge Bases using natural language queries.\nThe following code shows an example for Amazon Bedrock Agents integration:\n\n\nfrom crewai import Agent, Task, Crew\n\nfrom crewai_tools.aws.bedrock.agents.invoke_agent_tool import BedrockInvokeAgentTool\n\n# Initialize the Bedrock Agents Tool\n\nagent_tool = BedrockInvokeAgentTool(\n    agent_id=\"your-agent-id\",\n    agent_alias_id=\"your-agent-alias-id\"\n)\n\n# Create an CrewAI agent that uses the Bedrock Agents Tool\n\naws_expert = Agent(\n    role='AWS Service Expert',\n    goal='Help users understand AWS services and quotas',\n    backstory='I am an expert in AWS services and can provide detailed information about them.',\n    tools=[agent_tool],\n    verbose=True\n)\n\n\nThe following code shows an example for Amazon Bedrock Knowledge Bases integration:\n\n\n# Create and configure the BedrockKB tool \nkb_tool = BedrockKBRetrieverTool(\n    knowledge_base_id=\"your-kb-id\",\n    number_of_results=5\n)\n\n# Create an CrewAI agent that uses the Bedrock Agents Tool\nresearcher = Agent(\n    role='Knowledge Base Researcher',\n    goal='Find information about company policies',\n    backstory='I am a researcher specialized in retrieving and analyzing company documentation.',\n    tools=[kb_tool],\n    verbose=True\n)\n\n\nOperational excellence through monitoring, tracing, and observability with CrewAI on AWS\nAs with any software application, achieving operational excellence is crucial when deploying agentic applications in production environments. These applications are complex systems comprising both deterministic and probabilistic components that interact either sequentially or in parallel. Therefore, comprehensive monitoring, traceability, and observability are essential factors for achieving operational excellence. This includes three key dimensions:\n\nApplication-level observability – Provides smooth operation of the entire system, including the agent orchestration framework CrewAI and potentially additional application components (such as a frontend)\nModel-level observability – Provides reliable model performance (including metrics like accuracy, latency, throughput, and more)\nAgent-level observability – Maintains efficient operations within single-agent or multi-agent systems\n\nWhen running agent-based applications with CrewAI and Amazon Bedrock on AWS, you gain access to a comprehensive set of built-in capabilities across these dimensions:\n\nApplication-level logs – Amazon CloudWatch automatically collects application-level logs and metrics from your application code running on your chosen AWS compute platform, such as AWS Lambda, Amazon Elastic Container Service (Amazon ECS), or Amazon Elastic Compute Cloud (Amazon EC2). The CrewAI framework provides application-level logging, configured at a minimal level by default. For more detailed insights, verbose logging can be enabled at the agent or crew level by setting verbose=True during initialization.\nModel-level invocation logs – Furthermore, CloudWatch automatically collects model-level invocation logs and metrics from Amazon Bedrock. This includes essential performance metrics.\nAgent-level observability – CrewAI seamlessly integrates with popular third-party monitoring and observability frameworks such as AgentOps, Arize, MLFlow, LangFuse, and others. These frameworks enable comprehensive tracing, debugging, monitoring, and optimization of the agent system’s performance.\n\nSolution overview\nEach AWS service has its own configuration nuances, and missing just one detail can lead to serious vulnerabilities. Traditional security assessments often demand multiple experts, coordinated schedules, and countless manual checks. With CrewAI Agents, you can streamline the entire process, automatically mapping your resources, analyzing configurations, and generating clear, prioritized remediation steps.\nThe following diagram illustrates the solution architecture.\n\nOur use case demo implements a specialized team of three agents, each with distinct responsibilities that mirror roles you might find in a professional security consulting firm:\n\nInfrastructure mapper – Acts as our system architect, methodically documenting AWS resources and their configurations. Like an experienced cloud architect, it creates a detailed inventory that serves as the foundation for our security analysis.\nSecurity analyst – Serves as our cybersecurity expert, examining the infrastructure map for potential vulnerabilities and researching current best practices. It brings deep knowledge of security threats and mitigation strategies.\nReport writer – Functions as our technical documentation specialist, synthesizing complex findings into clear, actionable recommendations. It makes sure that technical insights are communicated effectively to both technical and non-technical stakeholders.\n\nImplement the solution\nIn this section, we walk through the implementation of a security assessment multi-agent system. The code for this example is located on GitHub. Note that not all code artifacts of the solution are explicitly covered in this post.\nStep 1: Configure the Amazon Bedrock LLM\nWe’ve saved our environment variables in an .env file in our root directory before we pass them to the LLM class:\n\nfrom crewai import Agent, Crew, Process, Task, LLM \nfrom crewai.project import CrewBase, agent, crew, task \n\nfrom aws_infrastructure_security_audit_and_reporting.tools.aws_infrastructure_scanner_tool import AWSInfrastructureScannerTool \nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool \nimport os \n\n@CrewBase \nclass AwsInfrastructureSecurityAuditAndReportingCrew():  \n    \"\"\"AwsInfrastructureSecurityAuditAndReporting crew\"\"\" \n    def __init__(self) -> None: \n        self.llm = LLM( model=os.getenv('MODEL'),\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n        aws_region_name=os.getenv('AWS_REGION_NAME') \n    )\n\nStep 2: Define agents\nThese agents are already defined in the agents.yaml file, and we’re importing them into each agent function in the crew.py file:\n\n...\n# Configure AI Agents\n\n@agent \t \ndef infrastructure_mapper(self) -> Agent:\n    return Agent( \t \n        config=self.agents_config['infrastructure_mapper'],\n        tools=[AWSInfrastructureScannerTool()],\n        llm=self.llm \t \n    ) \t \t \n\n@agent \t \ndef security_analyst(self) -> Agent:\n    return Agent( \t \n        config=self.agents_config['security_analyst'], \t \n        tools=[SerperDevTool(), ScrapeWebsiteTool()],\n        llm=self.llm \t \n    ) \t \t\n\n@agent \t \ndef report_writer(self) -> Agent: \t \n    return Agent( \t \n        config=self.agents_config['report_writer'], \t \n        llm=self.llm \t \n    )\n\nStep 3: Define tasks for the agents\nSimilar to our agents in the preceding code, we import tasks.yaml into our crew.py file:\n\n...\n# Configure Tasks for the agents\n\n@task \ndef map_aws_infrastructure_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['map_aws_infrastructure_task']\n    ) \n\n@task \ndef exploratory_security_analysis_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['exploratory_security_analysis_task']\n    ) \n\n@task \ndef generate_report_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['generate_report_task'] \n    )\n\nStep 4: Create the AWS infrastructure scanner tool\nThis tool enables our agents to interact with AWS services and retrieve information they need to perform their analysis:\n\nclass AWSInfrastructureScannerTool(BaseTool):\n    name: str = \"AWS Infrastructure Scanner\"\n    description: str = (\n        \"A tool for scanning and mapping AWS infrastructure components and their     configurations. \"\n        \"Can retrieve detailed information about EC2 instances, S3 buckets, IAM configurations, \"\n        \"RDS instances, VPC settings, and security groups. Use this tool to gather information \"\n        \"about specific AWS services or get a complete infrastructure overview.\"\n    )\n    args_schema: Type[BaseModel] = AWSInfrastructureScannerInput\n\n    def _run(self, service: str, region: str) -> str:\n        try:\n            if service.lower() == 'all':\n                return json.dumps(self._scan_all_services(region), indent=2, cls=DateTimeEncoder)\n            return json.dumps(self._scan_service(service.lower(), region), indent=2, cls=DateTimeEncoder)\n        except Exception as e:\n            return f\"Error scanning AWS infrastructure: {str(e)}\"\n\n    def _scan_all_services(self, region: str) -> Dict:\n        return {\n            'ec2': self._scan_service('ec2', region),\n            's3': self._scan_service('s3', region),\n            'iam': self._scan_service('iam', region),\n            'rds': self._scan_service('rds', region),\n            'vpc': self._scan_service('vpc', region)\n        }                                       \n   \n   # More services can be added here\n\nStep 5: Assemble the security audit crew\nBring the components together in a coordinated crew to execute on the tasks:\n\n@crew\ndef crew(self) -> Crew:\n    \"\"\"Creates the AwsInfrastructureSecurityAuditAndReporting crew\"\"\"\n    return Crew(\n        agents=self.agents, # Automatically created by the @agent decorator\n        tasks=self.tasks, # Automatically created by the @task decorator\n        process=Process.sequential,\n        verbose=True,\n    )\n\nStep 6: Run the crew\nIn our main.py file, we import our crew and pass in inputs to the crew to run:\n\ndef run():\n    \"\"\"\n    Run the crew.\n    \"\"\"\n    inputs = {}\n    AwsInfrastructureSecurityAuditAndReportingCrew().crew().kickoff(inputs=inputs)\n\nThe final report will look something like the following code:\n\n```markdown\n### Executive Summary\n\nIn response to an urgent need for robust security within AWS infrastructure, this assessment identified several critical areas requiring immediate attention across EC2 Instances, S3 Buckets, and IAM Configurations. Our analysis revealed two high-priority issues that pose significant risks to the organization's security posture.\n\n### Risk Assessment Matrix\n\n| Security Component | Risk Description | Impact | Likelihood | Priority |\n|--------------------|------------------|---------|------------|----------|\n| S3 Buckets | Unintended public access | High | High | Critical |\n| EC2 Instances | SSRF through Metadata | High | Medium | High |\n| IAM Configurations | Permission sprawl | Medium | High | Medium |\n\n### Prioritized Remediation Roadmap\n\n1. **Immediate (0-30 days):**\n   - Enforce IMDSv2 on all EC2 instances\n   - Conduct S3 bucket permission audit and rectify public access issues\n   - Adjust security group rules to eliminate broad access\n\n2. **Short Term (30-60 days):**\n   - Conduct IAM policy audit to eliminate unused permissions\n   - Restrict RDS access to known IP ranges\n```\n\nThis implementation shows how CrewAI agents can work together to perform complex security assessments that would typically require multiple security professionals. The system is both scalable and customizable, allowing for adaptation to specific security requirements and compliance standards.\nConclusion\nIn this post, we demonstrated how to use CrewAI and Amazon Bedrock to build a sophisticated, automated security assessment system for AWS infrastructure. We explored how multiple AI agents can work together seamlessly to perform complex security audits, from infrastructure mapping to vulnerability analysis and report generation. Through our example implementation, we showcased how CrewAI’s framework enables the creation of specialized agents, each bringing unique capabilities to the security assessment process. By integrating with powerful language models using Amazon Bedrock, we created a system that can autonomously identify security risks, research solutions, and generate actionable recommendations.\nThe practical example we shared illustrates just one of many possible applications of CrewAI with Amazon Bedrock. The combination of CrewAI’s agent orchestration capabilities and advanced language models in Amazon Bedrock opens up numerous possibilities for building intelligent, autonomous systems that can tackle complex business challenges.\nWe encourage you to explore our code on GitHub and start building your own multi-agent systems using CrewAI and Amazon Bedrock. Whether you’re focused on security assessments, process automation, or other use cases, this powerful combination provides the tools you need to create sophisticated AI solutions that can scale with your needs.\n\nAbout the Authors\n Tony Kipkemboi is a Senior Developer Advocate and Partnerships Lead at CrewAI, where he empowers developers to build AI agents that drive business efficiency. A US Army veteran, Tony brings a diverse background in healthcare, data engineering, and AI. With a passion for innovation, he has spoken at events like PyCon US and contributes to the tech community through open source projects, tutorials, and thought leadership in AI agent development. Tony holds a Bachelor’s of Science in Health Sciences and is pursuing a Master’s in Computer Information Technology at the University of Pennsylvania.\nJoão (Joe) Moura is the Founder and CEO of CrewAI, the leading agent orchestration platform powering multi-agent automations at scale. With deep expertise in generative AI and enterprise solutions, João partners with global leaders like AWS, NVIDIA, IBM, and Meta AI to drive innovative AI strategies. Under his leadership, CrewAI has rapidly become essential infrastructure for top-tier companies and developers worldwide and used by most of the F500 in the US.\n Karan Singh is a Generative AI Specialist at AWS, where he works with top-tier third-party foundation model and agentic frameworks providers to develop and execute joint go-to-market strategies, enabling customers to effectively deploy and scale solutions to solve enterprise generative AI challenges. Karan holds a Bachelor’s of Science in Electrical Engineering from Manipal University, a Master’s in Science in Electrical Engineering from Northwestern University, and an MBA from the Haas School of Business at University of California, Berkeley.\nAris Tsakpinis is a Specialist Solutions Architect for Generative AI focusing on open source models on Amazon Bedrock and the broader generative AI open source ecosystem. Alongside his professional role, he is pursuing a PhD in Machine Learning Engineering at the University of Regensburg, where his research focuses on applied natural language processing in scientific domains.",
      "date": "2025-03-31",
      "authors": "Tony Kipkemboi, João (Joe) Moura",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the emergence of agentic systems in enterprise AI, specifically through the integration of CrewAI and Amazon Bedrock. It highlights the growing significance of AI agents in business operations, their capabilities to perform complex tasks autonomously, and provides practical examples of their applications across various sectors.",
      "takeaways": [
        "- The enterprise AI landscape is shifting towards the deployment of AI agents, with projections for significant growth in this area by 2030.",
        "- CrewAI offers a modular framework for building AI agents that can collaborate and autonomously perform complex workflows, surpassing traditional automation solutions.",
        "- The integration with Amazon Bedrock enhances the capabilities of these agents by providing access to advanced language models and powerful tools for various industry applications."
      ]
    },
    {
      "id": 6,
      "title": "Amazon Bedrock Guardrails image content filters provide industry-leading safeguards, helping customer block up to 88% of harmful multimodal content: Generally available today",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-guardrails-image-content-filters-provide-industry-leading-safeguards-helping-customer-block-up-to-88-of-harmful-multimodal-content-generally-available-today/",
      "description": "Amazon Bedrock Guardrails announces the general availability of image content filters, enabling you to moderate both image and text content in your generative AI applications. Previously limited to text-only filtering, this enhancement now provides comprehensive content moderation across both modalities. This new capability removes the heavy lifting required to build your own image safeguards or spend cycles on manual content moderation that can be error-prone and tedious.\nTero Hottinen, VP, Head of Strategic Partnerships at KONE, envisions the following use case:\n\n“In its ongoing evaluation, KONE recognizes the potential of Amazon Bedrock Guardrails as a key component in protecting generative AI applications, particularly for relevance and contextual grounding checks, as well as the multimodal safeguards. The company envisions integrating product design diagrams and manuals into its applications, with Amazon Bedrock Guardrails playing a crucial role in enabling more accurate diagnosis and analysis of multimodal content.”\n\n\nAmazon Bedrock Guardrails provides configurable safeguards to help customers block harmful or unwanted inputs and outputs for their generative AI applications. Customers can create custom Guardrails tailored to their specific use cases by implementing different policies to detect and filter harmful or unwanted content from both input prompts and model responses. Furthermore, customers can use Guardrails to detect model hallucinations and help make responses grounded and accurate. Through its standalone ApplyGuardrail API, Guardrails enables customers to apply consistent policies across any foundation model, including those hosted on Amazon Bedrock, self-hosted models, and third-party models. Bedrock Guardrails supports seamless integration with Bedrock Agents and Bedrock Knowledge Bases, enabling developers to enforce safeguards across various workflows, such as Retrieval Augmented Generation (RAG) systems and agentic applications.\nAmazon Bedrock Guardrails offers six distinct policies, including: content filters to detect and filter harmful material across several categories, including hate, insults, sexual content, violence, misconduct, and to prevent prompt attacks; topic filters to restrict specific subjects; sensitive information filters to block personally identifiable information (PII); word filters to block specific terms; contextual grounding checks to detect hallucinations and analyze response relevance; and Automated Reasoning checks (currently in gated preview) to identify, correct, and explain factual claims. With the new image content moderation capability, these safeguards now extend to both text and images, helping customer block up to 88% of harmful multimodal content. You can independently configure moderation for either image or text content (or both) with adjustable thresholds from low to high, helping you to build generative AI applications that align with your organization’s responsible AI policies.\nThis new capability is generally available in US East (N. Virginia), US West (Oregon), Europe (Frankfurt), and Asia Pacific (Tokyo) AWS Regions.\nIn this post, we discuss how to get started with image content filters in Amazon Bedrock Guardrails.\nSolution overview\nTo get started, create a guardrail on the AWS Management Console and configure the content filters for either text or image data or both. You can also use AWS SDKs to integrate this capability into your applications.\nCreate a guardrail\nTo create a guardrail, complete the following steps:\n\nOn the Amazon Bedrock console, under Safeguards in the navigation pane, choose Guardrails.\nChoose Create guardrail.\nIn the Configure content filters section, under Harmful categories and Prompt attacks, you can use the existing content filters to detect and block image data in addition to text data. \nAfter you’ve selected and configured the content filters you want to use, you can save the guardrail and start using it to help you block harmful or unwanted inputs and outputs for your generative AI applications.\n\nTest a guardrail with text generation\nTo test the new guardrail on the Amazon Bedrock console, select the guardrail and choose Test. You have two options: test the guardrail by choosing and invoking a model or test the guardrail without invoking a model by using the Amazon Bedrock Guardrails independent ApplyGuardail API.\nWith the ApplyGuardrail API, you can validate content at any point in your application flow before processing or serving results to the user. You can also use the API to evaluate inputs and outputs for self-managed (custom) or third-party FMs, regardless of the underlying infrastructure. For example, you could use the API to evaluate a Meta Llama 3.2 model hosted on Amazon SageMaker or a Mistral NeMo model running on your laptop.\nTest a guardrail by choosing and invoking a model\nSelect a model that supports image inputs or outputs, for example, Anthropic’s Claude 3.5 Sonnet. Verify that the prompt and response filters are enabled for image content. Then, provide a prompt, upload an image file, and choose Run.\n\nIn this example, Amazon Bedrock Guardrails intervened. Choose View trace for more details.\nThe guardrail trace provides a record of how safety measures were applied during an interaction. It shows whether Amazon Bedrock Guardrails intervened or not and what assessments were made on both input (prompt) and output (model response). In this example, the content filters blocked the input prompt because they detected violence in the image with medium confidence.\n\nTest a guardrail without invoking a model\nOn the Amazon Bedrock console, choose Use ApplyGuardail API, the independent API to test the guardrail without invoking a model. Choose whether you want to validate an input prompt or an example of a model generated output. Then, repeat the steps from the previous section. Verify that the prompt and response filters are enabled for image content, provide the content to validate, and choose Run.\n\nFor this example, we reused the same image and input prompt, and Amazon Bedrock Guardrails intervened again. Choose View trace again for more details.\n\nTest a guardrail with image generation\nNow, let’s test the Amazon Bedrock Guardrails multimodal toxicity detection with image generation use cases. The following is an example of using Amazon Bedrock Guardrails image content filters with an image generation use case. We generate an image using the Stability model on Amazon Bedrock using the InvokeModel API and the guardrail:\n\nguardrailIdentifier = <<guardrail_id>>\nguardrailVersion =\"1\"\n\nmodel_id = 'stability.sd3-5-large-v1:0'\noutput_images_folder = 'images/output'\n\nbody = json.dumps(\n    {\n        \"prompt\": \"A Gun\", #  for image generation (\"A gun\" should get blocked by violence)\n        \"output_format\": \"jpeg\"\n    }\n)\n\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\ntry:\n    print(\"Making a call to InvokeModel API for model: {}\".format(model_id))\n    response = bedrock_runtime.invoke_model(\n        body=body,\n        modelId=model_id,\n        trace='ENABLED',\n        guardrailIdentifier=guardrailIdentifier,\n        guardrailVersion=guardrailVersion\n    )\n    response_body = json.loads(response.get('body').read())\n    print(\"Received response from InvokeModel API (Request Id: {})\".format(response['ResponseMetadata']['RequestId']))\n    if 'images' in response_body and len(response_body['images']) > 0:\n        os.makedirs(output_images_folder, exist_ok=True)\n        images = response_body[\"images\"]\n        for image in images:\n            image_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n            image_file = os.path.join(output_images_folder, \"generated-image-{}.jpg\".format(image_id))\n            print(\"Saving generated image {} at {}\".format(image_id, image_file))\n            with open(image_file, 'wb') as image_file_descriptor:\n                image_file_descriptor.write(base64.b64decode(image.encode('utf-8')))\n    else:\n        print(\"No images generated from model\")\n    guardrail_trace = response_body['amazon-bedrock-trace']['guardrail']\n    guardrail_trace['modelOutput'] = ['<REDACTED>']\n    print(guardrail_trace['outputs'])\n    print(\"\\nGuardrail Trace: {}\".format(json.dumps(guardrail_trace, indent=2)))\nexcept botocore.exceptions.ClientError as err:\n    print(\"Failed while calling InvokeModel API with RequestId = {}\".format(err.response['ResponseMetadata']['RequestId']))\n    raise err\n\nYou can access the complete example from the GitHub repo.\nConclusion\nIn this post, we explored how Amazon Bedrock Guardrails’ new image content filters provide comprehensive multimodal content moderation capabilities. By extending beyond text-only filtering, this solution now helps customers block up to 88% of harmful or unwanted multimodal content across configurable categories including hate, insults, sexual content, violence, misconduct, and prompt attack detection. Guardrails can help organizations across healthcare, manufacturing, financial services, media, and education enhance brand safety without the burden of building custom safeguards or conducting error-prone manual evaluations.\nTo learn more, see Stop harmful content in models using Amazon Bedrock Guardrails.\n\nAbout the Authors\nSatveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services, specializing in Amazon Bedrock security. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer’s deep understanding of generative AI technologies and security principles allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value while maintaining robust security postures.\nShyam Srinivasan is on the Amazon Bedrock Guardrails product team. He cares about making the world a better place through technology and loves being part of this journey. In his spare time, Shyam likes to run long distances, travel around the world, and experience new cultures with family and friends.\nAntonio Rodriguez is a Principal Generative AI Specialist Solutions Architect at AWS. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.\nDr. Andrew Kane is the WW Tech Leader for Security and Compliance for AWS Generative AI Services, leading the delivery of under-the-hood technical assets for customers around security, as well as working with CISOs around the adoption of generative AI services within their organisations. Before joining AWS at the beginning of 2015, Andrew spent two decades working in the fields of signal processing, financial payments systems, weapons tracking, and editorial and publishing systems. He is a keen karate enthusiast (just one belt away from Black Belt) and is also an avid home-brewer, using automated brewing hardware and other IoT sensors. He was the legal licensee in his ancient (AD 1468) English countryside village pub until early 2020.",
      "date": "2025-03-28",
      "authors": "Satveer Khurpa",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "Amazon Bedrock Guardrails has launched image content filters that enhance content moderation in generative AI applications, allowing for the blocking of harmful multimodal content, including images and text, with a reported effectiveness of up to 88%. This new capability enables organizations to implement customized safeguards effortlessly, improving the safety and relevance of their AI-generated outputs.",
      "takeaways": [
        "- The new image content filters extend Amazon Bedrock's content moderation capabilities beyond text to include images, addressing various harmful content categories.",
        "- Customers can configure tailored guardrails to filter unwanted inputs and outputs, enhancing the application of responsible AI practices.",
        "- The solution integrates seamlessly with various generative AI models and workflows, facilitating broad use across different industries such as healthcare, finance, and education."
      ]
    },
    {
      "id": 7,
      "title": "Integrating custom dependencies in Amazon SageMaker Canvas workflows",
      "link": "https://aws.amazon.com/blogs/machine-learning/integrating-custom-dependencies-in-amazon-sagemaker-canvas-workflows/",
      "description": "When implementing machine learning (ML) workflows in Amazon SageMaker Canvas, organizations might need to consider external dependencies required for their specific use cases. Although SageMaker Canvas provides powerful no-code and low-code capabilities for rapid experimentation, some projects might require specialized dependencies and libraries that aren’t included by default in SageMaker Canvas. This post provides an example of how to incorporate code that relies on external dependencies into your SageMaker Canvas workflows.\nAmazon SageMaker Canvas is a low-code no-code (LCNC) ML platform that guides users through every stage of the ML journey, from initial data preparation to final model deployment. Without writing a single line of code, users can explore datasets, transform data, build models, and generate predictions.\nSageMaker Canvas offers comprehensive data wrangling capabilities that help you prepare your data, including:\n\nOver 300 built-in transformation steps\nFeature engineering capabilities\nData normalization and cleansing functions\nA custom code editor supporting Python, PySpark, and SparkSQL\n\nIn this post, we demonstrate how to incorporate dependencies stored in Amazon Simple Storage Service (Amazon S3) within an Amazon SageMaker Data Wrangler flow. Using this approach, you can run custom scripts that depend on modules not inherently supported by SageMaker Canvas.\nSolution overview\nTo showcase the integration of custom scripts and dependencies from Amazon S3 into SageMaker Canvas, we explore the following example workflow.\nThe solution follows three main steps:\n\nUpload custom scripts and dependencies to Amazon S3\nUse SageMaker Data Wrangler in SageMaker Canvas to transform your data using the uploaded code\nTrain and export the model\n\nThe following diagram is the architecture for the solution.\n\nIn this example, we work with two complementary datasets available in SageMaker Canvas that contain shipping information for computer screen deliveries. By joining these datasets, we create a comprehensive dataset that captures various shipping metrics and delivery outcomes. Our goal is to build a predictive model that can determine whether future shipments will arrive on time based on historical shipping patterns and characteristics.\nPrerequisites\nAs a prerequisite, you need access to Amazon S3 and Amazon SageMaker AI. If you don’t already have a SageMaker AI domain configured in your account, you also need permissions to create a SageMaker AI domain.\nCreate the data flow\nTo create the data flow, follow these steps:\n\nOn the Amazon SageMaker AI console, in the navigation pane, under Applications and IDEs, select Canvas, as shown in the following screenshot. You might need to create a SageMaker domain if you haven’t done so already.\nAfter your domain is created, choose Open Canvas.\n\n\nIn Canvas, select the Datasets tab and select canvas-sample-shipping-logs.csv, as shown in the following screenshot. After the preview appears, choose + Create a data flow.\n\nThe initial data flow will open with one source and one data type.\n\nAt the top right of the screen, and select Add data → tabular. Choose Canvas Datasets as the source and select canvas-sample-product-descriptions.csv.\nChoose Next as shown in the following screenshot. Then choose Import.\n\n\nAfter both datasets have been added, select the plus sign. From the dropdown menu, choose select Combine data. From the next dropdown menu, choose Join.\n\n\nTo perform an inner join on the ProductID column, in the right-hand menu, under Join type, choose Inner join. Under Join keys, choose ProductId, as shown in the following screenshot.\n\n\nAfter the datasets have been joined, select the plus sign. In the dropdown menu, select + Add transform. A preview of the dataset will open.\n\nThe dataset contains XShippingDistance (long) and YShippingDistance (long) columns. For our purposes, we want to use a custom function that will find the total distance using the X and Y coordinates and then drop the individual coordinate columns. For this example, we find the total distance using a function that relies on the mpmath library.\n\nTo call the custom function, select + Add transform. In the dropdown menu, select Custom transform. Change the editor to Python (Pandas) and try to run the following function from the Python editor:\n\n\nfrom mpmath import sqrt  # Import sqrt from mpmath\n\ndef calculate_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n\n    # Use mpmath's sqrt to calculate the total distance for each row\n    df[new_col] = df.apply(lambda row: float(sqrt(row[x_col]**2 + row[y_col]**2)), axis=1)\n    \n    # Drop the original x and y columns\n    df = df.drop(columns=[x_col, y_col])\n    \n    return df\n\ndf = calculate_total_distance(df)\n\n\nRunning the function produces the following error: ModuleNotFoundError: No module named ‘mpmath’, as shown in the following screenshot.\n\nThis error occurs because mpmath isn’t a module that is inherently supported by SageMaker Canvas. To use a function that relies on this module, we need to approach the use of a custom function differently.\nZip the script and dependencies\nTo use a function that relies on a module that isn’t natively supported in Canvas, the custom script must be zipped with the module(s) it relies on. For this example, we used our local integrated development environment (IDE) to create a script.py that relies on the mpmath library.\nThe script.py file contains two functions: one function that is compatible with the Python (Pandas) runtime (function calculate_total_distance), and one that is compatible with the Python (Pyspark) runtime (function udf_total_distance).\n\ndef calculate_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n    from npmath import sqrt  # Import sqrt from npmath\n\n    # Use npmath's sqrt to calculate the total distance for each row\n    df[new_col] = df.apply(lambda row: float(sqrt(row[x_col]**2 + row[y_col]**2)), axis=1)\n\n    # Drop the original x and y columns\n    df = df.drop(columns=[x_col, y_col])\n\n    return df\n\ndef udf_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import udf\n    from pyspark.sql.types import FloatType\n\n    spark = SparkSession.builder \\\n        .master(\"local\") \\\n        .appName(\"DistanceCalculation\") \\\n        .getOrCreate()\n\n    def calculate_distance(x, y):\n        import sys\n\n        # Add the path to npmath\n        mpmath_path = \"/tmp/maths\"\n        if mpmath_path not in sys.path:\n            sys.path.insert(0, mpmath_path)\n\n        from mpmath import sqrt\n        return float(sqrt(x**2 + y**2))\n\n    # Register and apply UDF\n    distance_udf = udf(calculate_distance, FloatType())\n    df = df.withColumn(new_col, distance_udf(df[x_col], df[y_col]))\n    df = df.drop(x_col, y_col)\n\n    return df\n\n\nTo make sure the script can run, install mpmath into the same directory as script.py by running pip install mpmath.\nRun zip -r my_project.zip to create a .zip file containing the function and the mpmath installation. The current directory now contains a .zip file, our Python script, and the installation our script depends on, as shown in the following screenshot.\n\nUpload to Amazon S3\nAfter creating the .zip file, upload it to an Amazon S3 bucket.\n\nAfter the zip file has been uploaded to Amazon S3, it’s accessible in SageMaker Canvas.\nRun the custom script\nReturn to the data flow in SageMaker Canvas and replace the prior custom function code with the following code and choose Update.\n\nimport zipfile\nimport boto3\nimport sys\nfrom pathlib import Path\nimport shutil\nimport importlib.util\n\n\ndef load_script_and_dependencies(bucket_name, zip_key, extract_to):\n    \"\"\"\n    Downloads a zip file from S3, unzips it, and ensures dependencies are available.\n\n    Args:\n        bucket_name (str): Name of the S3 bucket.\n        zip_key (str): Key for the .zip file in the bucket.\n        extract_to (str): Directory to extract files to.\n\n    Returns:\n        str: Path to the extracted folder containing the script and dependencies.\n    \"\"\"\n    \n    s3_client = boto3.client(\"s3\")\n    \n    # Local path for the zip file\n    zip_local_path = '/tmp/dependencies.zip'\n    \n    # Download the .zip file from S3\n    s3_client.download_file(bucket_name, zip_key, zip_local_path)\n    print(f\"Downloaded zip file from S3: {zip_key}\")\n\n    # Unzip the file\n    try:\n        with zipfile.ZipFile(zip_local_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_to)\n            print(f\"Extracted files to {extract_to}\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to extract zip file: {e}\")\n\n    # Add the extracted folder to Python path\n    if extract_to not in sys.path:\n      sys.path.insert(0, extract_to)\n          \n    return extract_to\n    \n\n\ndef call_function_from_script(script_path, function_name, df):\n    \"\"\"\n    Dynamically loads a function from a Python script using importlib.\n    \"\"\"\n    try:\n        # Get the script name from the path\n        module_name = script_path.split('/')[-1].replace('.py', '')\n        \n        # Load the module specification\n        spec = importlib.util.spec_from_file_location(module_name, script_path)\n        if spec is None:\n            raise ImportError(f\"Could not load specification for module {module_name}\")\n            \n        # Create the module\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[module_name] = module\n        \n        # Execute the module\n        spec.loader.exec_module(module)\n        \n        # Get the function from the module\n        if not hasattr(module, function_name):\n            raise AttributeError(f\"Function '{function_name}' not found in the script.\")\n            \n        loaded_function = getattr(module, function_name)\n\n        # Clean up: remove module from sys.modules after execution\n        del sys.modules[module_name]\n        \n        # Call the function\n        return loaded_function(df)\n        \n    except Exception as e:\n        raise RuntimeError(f\"Error loading or executing function: {e}\")\n\n\nbucket_name = 'canvasdatabuckett'  # S3 bucket name\nzip_key = 'functions/my_project.zip'  # S3 path to the zip file with our custom dependancy\nscript_name = 'script.py'  # Name of the script in the zip file\nfunction_name = 'udf' # Name of function to call from our script\nextract_to = '/tmp/maths' # Local path to our custom script and dependancies\n\n# Step 1: Load the script and dependencies\nextracted_path = load_script_and_dependencies(bucket_name, zip_key, extract_to)\n\n# Step 2: Call the function from the script\nscript_path = f\"{extracted_path}/{script_name}\"\ndf = call_function_from_script(script_path, function_name, df)\n\n\nThis example code unzips the .zip file and adds the required dependencies to the local path so they’re available to the function at run time. Because mpmath was added to the local path, you can now call a function that relies on this external library.\nThe preceding code runs using the Python (Pandas) runtime and calculate_total_distance function. To use the Python (Pyspark) runtime, update the function_name variable to call the udf_total_distance function instead.\nComplete the data flow\nAs a last step, remove irrelevant columns before training the model. Follow these steps:\n\nOn the SageMaker Canvas console, select + Add transform. From the dropdown menu, select Manage columns \nUnder Transform, choose Drop column. Under Columns to drop, add ProductId_0, ProductId_1, and OrderID, as shown in the following screenshot.\n\nThe final dataset should contain 13 columns. The complete data flow is pictured in the following image.\n\nTrain the model\nTo train the model, follow these steps:\n\nAt the top right of the page, select Create model and name your dataset and model.\nSelect Predictive analysis as the problem type and OnTimeDelivery as the target column, as shown in the screenshot below.\n\nWhen building the model you can choose to run a Quick build or a Standard build. A Quick build prioritizes speed over accuracy and produces a trained model in less than 20 minutes. A standard build prioritizes accuracy over latency but the model takes longer to train.\nResults\nAfter the model build is complete, you can view the model’s accuracy, along with metrics like F1, precision and recall. In the case of a standard build, the model achieved 94.5% accuracy.\n\nAfter the model training is complete, there are four ways you can use your model:\n\nDeploy the model directly from SageMaker Canvas to an endpoint\nAdd the model to the SageMaker Model Registry\nExport your model to a Jupyter Notebook\nSend your model to Amazon QuickSight for use in dashboard visualizations\n\nClean up\nTo manage costs and prevent additional workspace charges, choose Log out to sign out of SageMaker Canvas when you’re done using the application, as shown in the following screenshot. You can also configure SageMaker Canvas to automatically shut down when idle.\nIf you created an S3 bucket specifically for this example, you might also want to empty and delete your bucket.\n\nSummary\nIn this post, we demonstrated how you can upload custom dependencies to Amazon S3 and integrate them into SageMaker Canvas workflows. By walking through a practical example of implementing a custom distance calculation function with the mpmath library, we showed how to:\n\nPackage custom code and dependencies into a .zip file\nStore and access these dependencies from Amazon S3\nImplement custom data transformations in SageMaker Data Wrangler\nTrain a predictive model using the transformed data\n\nThis approach means that data scientists and analysts can extend SageMaker Canvas capabilities beyond the more than 300 included functions.\nTo try custom transforms yourself, refer to the Amazon SageMaker Canvas documentation and sign in to SageMaker Canvas today. For additional insights into how you can optimize your SageMaker Canvas implementation, we recommend exploring these related posts:\n\nSeamlessly transition between no-code and code-first machine learning with Amazon SageMaker Canvas and Amazon SageMaker Studio\nUnlock the power of data governance and no-code machine learning with Amazon SageMaker Canvas and Amazon DataZone\n\n\n\nAbout the Author\nNadhya Polanco is an Associate Solutions Architect at AWS based in Brussels, Belgium. In this role, she supports organizations looking to incorporate AI and Machine Learning into their workloads. In her free time, Nadhya enjoys indulging in her passion for coffee and exploring new destinations.",
      "date": "2025-03-27",
      "authors": "Nadhya Polanco",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses how to integrate custom dependencies into machine learning workflows using Amazon SageMaker Canvas, a low-code/no-code platform. It illustrates the process of uploading and utilizing external libraries, specifically demonstrating how to apply the mpmath library for distance calculations in a predictive modeling task.",
      "takeaways": [
        "- SageMaker Canvas allows for the integration of custom scripts and dependencies from Amazon S3, enabling users to enhance the platform's capabilities beyond the default offerings.",
        "- The workflow includes practical steps for packaging code and dependencies in a zip file, which can then be loaded and utilized within SageMaker Canvas for custom data transformations.",
        "- This approach empowers data scientists and analysts to implement complex transformations and models without needing extensive coding skills, facilitating faster experimentation and deployment of machine learning solutions."
      ]
    },
    {
      "id": 8,
      "title": "Generate training data and cost-effectively train categorical models with Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/generate-training-data-and-cost-effectively-train-categorical-models-with-amazon-bedrock/",
      "description": "In this post, we explore how you can use Amazon Bedrock to generate high-quality categorical ground truth data, which is crucial for training machine learning (ML) models in a cost-sensitive environment. Generative AI solutions can play an invaluable role during the model development phase by simplifying training and test data creation for multiclass classification supervised learning use cases. We dive deep into this process on how to use XML tags to structure the prompt and guide Amazon Bedrock in generating a balanced label dataset with high accuracy. We also showcase a real-world example for predicting the root cause category for support cases. This use case, solvable through ML, can enable support teams to better understand customer needs and optimize response strategies.\nBusiness challenge\nThe exploration and methodology described in this post addresses two key challenges: costs associated with generating a ground truth dataset for multiclass classification use cases can be prohibitive, and conventional approaches and synthetic dataset creation strategies for generating ground truth data are inadequate in generating balanced classes and meeting desired performance parameters for the real-world use cases.\nGround truth data generation is expensive and time consuming\nGround truth annotation needs to be accurate and consistent, often requiring massive time and expertise to ensure the dataset is balanced, diverse, and large enough for model training and testing. For a multiclass classification problem such as support case root cause categorization, this challenge compounds many fold.\nLet’s say the task at hand is to predict the root cause categories (Customer Education, Feature Request, Software Defect, Documentation Improvement, Security Awareness, and Billing Inquiry) for customer support cases. Based on our experiments using best-in-class supervised learning algorithms available in AutoGluon, we arrived at a 3,000 sample size for the training dataset for each category to attain an accuracy of 90%. This requirement translates into time and effort investment of trained personnel, who could be support engineers or other technical staff, to review tens of thousands of support cases to arrive at an even distribution of 3,000 per category. With each support case and the related correspondences averaging 5 minutes per review and assessment from a human labeler, this translates into 1,500 hours (5 minutes x 18,000 support cases) of work or 188 days considering an 8-hour workday. Besides the time in review and labeling, there is an upfront investment in training the labelers so the exercise split between 10 or more labelers is consistent. To break this down further, a ground truth labeling campaign split between 10 labelers would require close to 4 weeks to label 18,000 cases if the labelers spend 40 hours a week on the exercise.\nNot only is such an extended and effort-intensive campaign expensive, but it can cause inconsistent labeling for categories every time the labeler puts aside the task and resumes it later. The exercise also doesn’t guarantee a balanced labeled ground truth dataset because some root cause categories such as Customer Education could be far more common than Feature Request or Software Defect, thereby extending the campaign.\nConventional techniques to get balanced classes or synthetic data generation have shortfalls\nA balanced labeled dataset is critical for a multiclass classification use case to mitigate bias and make sure the model learns to accurately classify all classes, rather than favoring the majority class. If the dataset is imbalanced, with one or more classes having significantly fewer instances than others, the model might struggle to learn the patterns and features associated with the minority classes, leading to poor performance and biased predictions. This issue is particularly problematic in applications where accurate classification of minority classes is critical, such as medical diagnoses, fraud detection, or root cause categorization. For the use case of labeling the support root cause categories, it’s often harder to source examples for categories such as Software Defect, Feature Request, and Documentation Improvement for labeling than it is for Customer Education. This results in an imbalanced class distribution for training and test datasets.\nTo address this challenge, various techniques can be employed, including oversampling the minority classes, undersampling the majority classes, using ensemble methods that combine multiple classifiers trained on different subsets of the data, or synthetic data generation to augment minority classes. However, the ideal approach for achieving optimal performance is to start with a balanced and highly accurate labeled dataset for ground truth training.\nAlthough oversampling for minority classes means extended and expensive data labeling with humans who review the support cases, synthetic data generation to augment the minority classes poses its own challenges. For the multiclass classification problem to label support case data, synthetic data generation can quickly result in overfitting. This is because it can be difficult to synthesize real-world examples of technical case correspondences that contain complex content related to software configuration, implementation guidance, documentation references, technical troubleshooting, and the like.\nBecause ground truth labeling is expensive and synthetic data generation isn’t an option for use cases such as root cause prediction, the effort to train a model is often put aside. This results in a missed opportunity to review the root cause trends that can guide investment in the right areas such as education for customers, documentation improvement, or other efforts to reduce the case volume and improve customer experience.\nSolution overview\nThe preceding section discussed why conventional ground truth data generation techniques aren’t viable for certain supervised learning use cases and fall short in training a highly accurate model to predict the support case root cause in our example. Let’s look at how generative AI can help solve this problem.\nGenerative AI supports key use cases such as content creation, summarization, code generation, creative applications, data augmentation, natural language processing, scientific research, and many others. Amazon Bedrock is well-suited for this data augmentation exercise to generate high-quality ground truth data. Using highly tuned and custom tailored prompts with examples and techniques discussed in the following sections, support teams can pass the anonymized support case correspondence to Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock or other available large language models (LLMs) to predict the root cause label for a support case from one of the many categories (Customer Education, Feature Request, Software Defect, Documentation Improvement, Security Awareness, and Billing Inquiry). After achieving the desired accuracy, you can use this ground truth data in an ML pipeline with automated machine learning (AutoML) tools such as AutoGluon to train a model and inference the support cases.\nChecking LLM accuracy for ground truth data\nTo evaluate an LLM for the task of category labeling, the process begins by determining if labeled data is available. If labeled data exists, the next step is to check if the model’s use case produces discrete outcomes. Where discrete outcomes with labeled data exist, standard ML methods such as precision, recall, or other classic ML metrics can be used. These metrics provide high precision but are limited to specific use cases due to limited ground truth data.\nIf the use case doesn’t yield discrete outputs, task-specific metrics are more appropriate. These include metrics such as ROUGE or cosine similarity for text similarity, and specific benchmarks for assessing toxicity (Detoxify), prompt stereotyping (cross-entropy loss), or factual knowledge (HELM, LAMA).\nIf labeled data is unavailable, the next question is whether the testing process should be automated. The automation decision depends on the cost-accuracy trade-off because higher accuracy comes at a higher cost. For cases where automation is not required, human-in-the-Loop (HIL) approaches can be used. This involves manual evaluation based on predefined assessment rules (for example, ground truth), yielding high evaluation precision, but often is time-consuming and costly.\nWhen automation is preferred, using another LLM to assess outputs can be effective. Here, a reliable LLM can be instructed to rate generated outputs, providing automated scores and explanations. However, the precision of this method depends on the reliability of the chosen LLM. Each path represents a tailored approach based on the availability of labeled data and the need for automation, allowing for flexibility in assessing a wide range of FM applications.\nThe following figure illustrates an FM evaluation workflow.\n\nFor the use case, if a historic collection of 10,000 or more support cases labeled using Amazon SageMaker Ground Truth with HIL is available, it can be used for evaluating the accuracy of the LLM prediction. The key goal for generating new ground truth data using Amazon Bedrock should be to augment it for increasing diversity and increasing the training data size for AutoGluon training to arrive at a performant model that can be used for the final inference or root cause prediction. In the following sections, we explain how to take an incremental and measured approach to improve Anthropic’s Claude 3.5 Sonnet prediction accuracy through prompt engineering.\nPrompt engineering for FM accuracy and consistency\nPrompt engineering is the art and science of designing a prompt to get an LLM to produce the desired output. We suggest consulting LLM prompt engineering documentation such as Anthropic prompt engineering for experiments. Based on experiments conducted without a finely tuned and optimized prompt, we observed low accuracy rates of less than 60%. In the following sections, we provide a detailed explanation on how to construct your first prompt, and then gradually improve it to consistently achieve over 90% accuracy.\nDesigning the prompt\nBefore starting any scaled use of generative AI, you should have the following in place:\n\nA clear definition of the problem you are trying to solve along with the end goal.\nA way to test the model’s output for accuracy. The thumbs up/down technique to determine accuracy along with comparing with the 10,000 labeled dataset through SageMaker Ground Truth is well-suited for this exercise.\nA defined success criterion on how accurate the model needs to be.\n\nIt’s helpful to think of an LLM as a new employee who is very well read, but knows nothing about your culture, your norms, what you are trying to do, or why you are trying to do it. The LLM’s performance will depend on how precisely you can explain what you want. How would a skilled manager handle a very smart, but new and inexperienced employee? The manager would provide contextual background, explain the problem, explain the rules they should apply when analyzing the problem, and give some examples of what good looks like along with why it is good. Later, if they saw the employee making mistakes, they might try to simplify the problem and provide constructive feedback by giving examples of what not to do, and why. One difference is that an employee would understand the job they are being hired for, so we need to explicitly tell the LLM to assume the persona of a support employee.\nPrerequisites\nTo follow along with this post, set up Amazon SageMaker Studio to run Python in a notebook and interact with Amazon Bedrock. You also need the appropriate permissions to access Amazon Bedrock models.\nSet up SageMaker Studio\nComplete the following steps to set up SageMaker Studio:\n\nOn the SageMaker console, choose Studio under Applications and IDEs in the navigation pane.\nCreate a new SageMaker Studio instance if you haven’t already.\nIf prompted, set up a user profile for SageMaker Studio by providing a user name and specifying AWS Identity and Access Management (IAM) permissions.\nOpen a SageMaker Studio notebook: \n  \nChoose JupyterLab.\nCreate a private JupyterLab space.\nConfigure the space (set the instance type to ml.m5.large for optimal performance).\nLaunch the space.\nOn the File menu, choose New and Notebook to create a new notebook.\n \nConfigure SageMaker to meet your security and compliance objectives. Refer to Configure security in Amazon SageMaker AI for details.\n\nSet up permissions for Amazon Bedrock access\nMake sure you have the following permissions:\n\nIAM role with Amazon Bedrock permissions – Make sure that your SageMaker Studio execution role has the necessary permissions to access Amazon Bedrock. Attach the AmazonBedrockFullAccesspolicy or a custom policy with specific Amazon Bedrock permissions to your IAM role.\nAWS SDKs and authentication – Verify that your AWS credentials (usually from the SageMaker role) have Amazon Bedrock access. Refer to Getting started with the API to set up your environment to make Amazon Bedrock requests through the AWS API.\nModel access – Grant permission to use Anthropic’s Claude 3.5 Sonnet. For instructions, see Add or remove access to Amazon Bedrock foundation models.\n\nTest the code using the native inference API for Anthropic’s Claude\nThe following code uses the native inference API to send a text message to Anthropic’s Claude. The Python code invokes the Amazon Bedrock Runtime service:\n\nimport boto3\nimport json\nfrom datetime import datetime\nimport time\n\n# Create an Amazon Bedrock Runtime client in the AWS Region of your choice.\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\n# Set the model ID, e.g., Claude 3 Haiku.\nmodel_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n\n# Load the prompt from a file (showed and explained later in the blog)\nwith open('prompt.txt', 'r') as file:\ndata = file.read()\n\n\ndef callBedrock(body):\n# Format the request payload using the model's native structure.\n\nprompt = data + body;\n\n# The prompt is then truncated to the max input window size of Sonnet 3.5\nprompt = prompt[:180000]\n\n# Define parametres passed to the model. \nnative_request = {\n\"anthropic_version\": \"bedrock-2023-05-31\",\n\"max_tokens\": 512,\n\"temperature\": 0.2,\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": prompt}],\n}\n],\n}\n\n# Convert the native request to JSON.\nrequest = json.dumps(native_request)\n\ntry:\n# Invoke the model with the request.\nresponse = client.invoke_model(modelId=model_id, body=request)\n\nexcept (Exception) as e:\nprint(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n\n# Load the response returned from Amazon Bedrock into a json object\nmodel_response = json.loads(response[\"body\"].read())\n\n# Extract and print the response text.\nresponse_text = model_response[\"content\"][0][\"text\"]\nreturn response_text;\n\nConstruct the initial prompt\nWe demonstrate the approach for the specific use case for root cause prediction with a goal of achieving 90% accuracy. Start by creating a prompt similar to the prompt you would give to humans in natural language. This can be a simple description of each root cause label and why you would choose it, how to interpret the case correspondences, how to analyze and choose the corresponding root cause label, and provide examples for every category. Ask the model to also provide the reasoning to understand how it reached to certain decisions. It can be especially interesting to understand the reasoning for the decisions you don’t agree with. See the following example code:\n\nPlease familiarize yourself with these categories.  When you evaluate a case, evaluate the definitions in order and label the case with the first definition that fits.  If a case morphs from one type to another, choose the type the case started out as. \n Read the correspondence, especially the original request, and the last correspondence from the support agent to the customer. If there are lot of correspondences, or the case does not seem straightforward to infer, read the correspondences date stamped in order to understand what happened. If the case references documentation, read or skim the documentation to determine whether the documentation clearly supports what the support agent mentioned and whether it answers the customers issue.\n\nSoftware Defect:  “Software Defect” are cases where the application does not work as expected. The support agent confirms this through analysis and troubleshooting and mentions internal team is working on a fix or patch to address the bug or defect. \n\nAn example of Software Defect case is [Customer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\" Agent: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"]\n....  \n\nAnalyze the results\nWe recommend using a small sample (for example, 150) of random cases and run them through Anthropic’s Claude 3.5 Sonnet using the initial prompt, and manually check the initial results. You can load the input data and model output into Excel, and add the following columns for analysis:\n\nClaude Label – A calculated column with Anthropic’s Claude’s category\nLabel – True category after reviewing each case and selecting a specific root cause category to compare with the model’s prediction and derive an accuracy measurement\nClose Call – 1 or 0 so that you can take numerical averages\nNotes – For cases where there was something noteworthy about the case or inaccurate categorizations\nClaude Correct – A calculated column (0 or 1) based on whether our category matched the model’s output category\n\nAlthough the first run is expected to have low accuracy unfit for using the prompt for generating the ground truth data, the reasoning will help you understand why Anthropic’s Claude mislabeled the cases. In the example, many of the misses fell into these categories and the accuracy was only 61%:\n\nCases where Anthropic’s Claude categorized Customer Education cases as Software Defect because it interpreted the support agent instructions to reconfigure something as a workaround for a Software Defect.\nCases where users asked questions about billing that Anthropic’s Claude categorized as Customer Education. Although billing questions could also be Customer Education cases, we wanted these to be categorized as the more specific Billing Inquiry Likewise, although Security Awareness cases are also Customer Education, we wanted to categorize these as the more specific Security Awareness category.\n\nIterate on the prompt and make changes\nProviding the LLM explicit instructions on correcting these errors should result in a major boost in accuracy. We tested the following adjustments with Anthropic’s Claude:\n\nWe defined and assigned a persona with background information for the LLM: “You are a Support Agent and an expert on the enterprise application software. You will be classifying customer cases into categories…”\nWe ordered the categories from more deterministic and well-defined to less specific and instructed Anthropic’s Claude to evaluate the categories in the order they appear in the prompt.\nWe recommend using the Anthropic documentation suggestion to use XML tags and the enclosed root cause categories in light XML but not a formal XML document, with elements delimited with tags. It’s ideal to create categories as nodes with a separate sub-node for each category. The category node should consist of a name of the category, a description, and what the output would look like. The categories should be delimited by begin and end tags.\n\n\nYou are a Support Agent and an expert on the enterprise application software. You will be classifying the customer support cases into categories, based on the given interaction between an agent and a customer. You can only choose ONE Category from the list below. You follow instructions well, step by step, and evaluate the categories in the order they appear in the prompt when making a decision.\n\nThe categories are defined as:\n\n<categories>\n<category>\n<name>\n\"Software Defect\"\n</name>\n<description>\n“Software Defect” are cases where the application software does not work as expected. The agent confirms the application is not working as expected and may refer to internal team working on a fix or patch to address the bug or defect. The category includes common errors or failures related to performance, software version, functional defect, unexpected exception or usability bug when the customer is following the documented steps.\n</description>\n</category>\n...\n</categories>\n\n\nWe created a good examples node with at least one good example for every category. Each good example consisted of the example, the classification, and the reasoning:\n\nHere are some good examples with reasoning:\n\n<good examples>\n<example>\n<example data>\nCustomer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\"\nAgent: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"\n</example data\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nCustomer is reporting a data processing exception with a specific version and the agent confirms this is a regression and defect. The agent confirms that engineering is working to provide an emergency patch for the issue. \n</explanation>\n</example>\n...\n</good examples>\n\n\n\nWe created a bad examples node with examples of where the LLM miscategorized previous cases. The bad examples node should have the same set of fields as the good examples, such as example data, classification, explanation, but the explanation explained the error. The following is a snippet:\n\nHere are some examples for wrong classification with reasoning:\n\n<bad examples>\n\n    <example>\n        <example data>\n            Customer: \"We need the ability to create custom dashboards that can aggregate data across multiple tenants in real-time. Currently, we can only view metrics per individual tenant, which requires manual consolidation for our enterprise reporting needs.\"\nAgent: \"I understand your need for cross-tenant analytics. While the current functionality is limited to single-tenant views as designed, I've submitted your request to our product team as a high-priority feature enhancement. They'll evaluate it for inclusion in our 2025 roadmap. I'll update you when there's news about this capability.\"\n       </example data>\n    <example output>\n        <classification>\n            \"Software Defect\"\n        </classification>\n        <explanation>\n            Classification should be Feature Request and not Software Defect. The application does not have the function or capability being requested but it is working as documented or advertised. In the example, the agent mentions they have submitted with request to their product team to consider in the future roadmap.\n        </explanation>\n    </example>\n...\n<bad examples>\n\n\n\nWe also added instructions for how to format the output:\n\n\nGiven the above categories defined in XML, logically think through which category fits best and then complete the classification. Provide a response in XML with the following elements: classification, explanation (limited to 2 sentences). Return your results as this sample output XML below and do not append your thought process to the response.\n \n<response> \n<classification> Software Defect </classification>\n<explanation> The support case is for ETL Pipeline Performance Degradation where the customer reports their nightly data transformation job takes 6 hours to complete instead of 2 hours before but no changes to configuration occurred. The agent mentions Engineering confirmed memory leak in version 5.1.2 and are deploying a Hotfix indicating this is a Software Defect.\n</explanation> \n</response> \n\nTest with the new prompt\nThe preceding approach should result in an improved prediction accuracy. In our experiment, we saw 84% accuracy with the new prompt and the output was consistent and more straightforward to parse. Anthropic’s Claude followed the suggested output format in almost all cases. We wrote code to fix errors such as unexpected tags in the output and drop responses that could not be parsed.\nThe following is the code to parse the output:\n\n# This python script parses LLM output into a comma separated list with the SupportID, Category, Reason\n# Command line is python parse_llm_putput.py PathToLLMOutput.txt PathToParsedOutput.csv\n# Note:  It will overwrite the output file without confirming\n# it will write completion status and any error messages to stdout\n \nimport re\nimport sys\n \n# these tokens are based on the format of the claude output.\n# This will create three inputs CaseID, RootCause and Reasoning.  We will to extract them using re.match.\npattern = re.compile(\n    \"^([0-9]*).*<classification>(.*)</classification><explanation>(.*)</explanation>\"\n)\n \nendToken = \"</response>\"\ncheckToken = \"<classification>\"\n \nacceptableClassifications = [\n    \"Billing Inquiry\",\n    \"Documentation Improvement\",\n    \"Feature Request\",\n    \"Security Awareness\",\n    \"Software Defect\",\n    \"Customer Education\",\n]\n \ndef parseResponse(response):\n    # parsing is trivial withe regular expression groups\n    m = pattern.match(response)\n    return m\n \n# get the input and output files\nif len(sys.argv) != 3:\n    print(\"Command line error parse_llm_output.py inputfile outputfile\")\n    exit(1)\n \n# open the file\ninput = open(sys.argv[1], encoding=\"utf8\")\noutput = open(sys.argv[2], \"w\")\n \n# read the entire file in.  This works well with 30,000 responses, but would need to be adjusted for say 3,000,000 responses\nresponses = input.read()\n \n# get rid of the double quotes and newlines to avoid incorrect excel parsing and these are unnecessary\nresponses = responses.replace('\"', \"\")\nresponses = responses.replace(\"\\n\", \"\")\n \n# initialize our placeholder, and counters\nparsedChars = 0\nskipped = 0\ninvalid = 0\nresponseCount = 0\n \n# write the header\noutput.write(\"CaseID,RootCause,Reason\\n\")\n \n# find the first response\nindex = responses.find(endToken, parsedChars)\n \nwhile index > 0:\n    # extract the response\n    response = responses[parsedChars : index + len(endToken)]\n    # parse it\n    parsedResponse = parseResponse(response)\n \n    # is the response valid\n    if parsedResponse is None or len(response.split(checkToken)) != 2:\n        # this happens when there is a missing /response delimiter or some other formatting problem, it clutters up and the next response\n        skipped = skipped + 2\n    else:\n        # if we have a valid response write it to the file, enclose the reason in double quotes because it uses commas\n        if parsedResponse.group(2).lower() not in acceptableClassifications:\n            # make sure the classification is one we expect\n            print(\"Invalid Classification: {0}\".format(parsedResponse.group(2)))\n            invalid = invalid + 1\n        else:\n            # write a valid line to the output file\n            output.write(\n                '{0},{1},\"{2}\"\\n'.format(\n                    parsedResponse.group(1),\n                    parsedResponse.group(2),\n                    parsedResponse.group(3),\n                )\n            )\n \n    # move the pointer past where we parsed and update the counter\n    parsedChars = index + len(endToken)\n    responseCount = responseCount + 1\n \n    # find the next response\n    index = responses.find(endToken, parsedChars)\n \nprint(\"skipped {0} of {1} responses\".format(skipped, responseCount))\nprint(\"{0} of these were invalid\".format(invalid)) \n\nMost mislabeled cases were close calls or had very similar traits. For example, when a customer described a problem, the support agent suggested possible solutions and asked for logs in order to troubleshoot. However, the customer self-resolved the case and so the resolution details weren’t conclusive. For this scenario, the root cause prediction was inaccurate. In our experiment, Anthropic’s Claude labeled these cases as Software Defects, but the most likely scenario is that the customer figured it out for themselves and never followed up.\nContinued fine-tuning of the prompt to adjust examples and include such scenarios incrementally can help to get over 90% prediction accuracy, as we confirmed with our experimentation. The following code is an example of how to adjust the prompt and add a few more bad examples:\n\n<example>\n<example data>\nSubject: Unable to configure custom routing rules in application gateway\nCustomer: Our team can't set up routing rules in the application gateway. We've tried following the documentation but the traffic isn't being directed as expected. This is blocking our production deployment.\nAgent: I understand you're having difficulties with routing rules configuration. To better assist you, could you please provide:\nCurrent routing rule configuration\nApplication gateway logs\nExpected traffic flow diagram\n[No response from customer for 5 business days - Case closed by customer]\n</example data>\n    <example output>\n      <classification>\n       Software Defect\n      </classification>\n <explanation>\nClassification should be Customer Education and not Software Defect. The agent acknowledges the problem and asks the customer for additional information to troubleshoot, however, the customer does not reply and closes the case. Cases where the agent tells the customer how to solve the problem and provides documentation or asks for further details to troubleshoot but the customer self-resolves the case should be labeled Customer Education.\n</explanation>\n</example>\n\nWith the preceding adjustments and refinement to the prompt, we consistently obtained over 90% accuracy and noted that a few miscategorized cases were close calls where humans chose multiple categories including the one Anthropic’s Claude chose. See the appendix at the end of this post for the final prompt.\nRun batch inference at scale with AutoGluon Multimodal\nAs illustrated in the previous sections, by crafting a well-defined and tailored prompt, Amazon Bedrock can help automate generation of ground truth data with balanced categories. This ground truth data is necessary to train the supervised learning model for a multiclass classification use case. We suggest taking advantage of the preprocessing capabilities of SageMaker to further refine the fields, encoding them into a format that’s optimal for model ingestion. The manifest files can be set up as the catalyst, triggering an AWS Lambda function that sets entire SageMaker pipeline into action. This end-to-end process seamlessly handles data inference and stores the results in Amazon Simple Storage Service (Amazon S3). We recommend AutoGluon Multimodal for training and prediction and deploying a model for a batch inference pipeline to predict the root cause for new or updated support cases at scale on a daily cadence.\nClean up\nTo prevent unnecessary expenses, it’s essential to properly decommission all provisioned resources. This cleanup process involves stopping notebook instances and deleting JupyterLab spaces, SageMaker domains, S3 bucket, IAM role, and associated user profiles. Refer to Clean up Amazon SageMaker notebook instance resources for details.\nConclusion\nThis post explored how Amazon Bedrock and advanced prompt engineering can generate high-quality labeled data for training ML models. Specifically, we focused on a use case of predicting the root cause category for customer support cases, a multiclass classification problem. Traditional approaches to generating labeled data for such problems are often prohibitively expensive, time-consuming, and prone to class imbalances. Amazon Bedrock, guided by XML prompt engineering, demonstrated the ability to generate balanced labeled datasets, at a lower cost, with over 90% accuracy for the experiment, and can help overcome labeling challenges for training categorical models for real-world use cases.\nThe following are our key takeaways:\n\nGenerative AI can simplify labeled data generation for complex multiclass classification problems\nPrompt engineering is crucial for guiding LLMs to achieve desired outputs accurately\nAn iterative approach, incorporating good/bad examples and specific instructions, can significantly improve model performance\nThe generated labeled data can be integrated into ML pipelines for scalable inference and prediction using AutoML multimodal supervised learning algorithms for batch inference\n\nReview your ground truth training costs with respect to time and effort for HIL labeling and service costs and do a comparative analysis with Amazon Bedrock to plan your next categorical model training at scale.\nAppendix\nThe following code is the final prompt:\n\nYou are a Support Agent and an expert in the enterprise application software. You will be classifying the customer support cases into one of the 6 categories, based on the given interaction between the Support Agent and a customer. You can only choose ONE Category from the list below. You follow instructions well, step by step, and evaluate the categories in the order they appear in the prompt when making a decision. \n \nThe categories are defined as:\n \n<categories>\n \n<category>\n<name>\n\"Billing Inquiry\" \n</name>\n<description>\n“Billing Inquiry” cases are the ones related to Account or Billing inquiries and questions related to charges, savings, or discounts. It also includes requests to provide guidance on account closing, request for Credit, cancellation requests, billing questions, and questions about discounts.\n</description>\n</category>\n \n<category>\n<name>\n\"Security Awareness\" \n</name>\n<description>\n“Security Awareness” cases are the cases associated with a security related incident. Security Awareness cases include exposed credentials, mitigating a security vulnerability, DDoS attacks, security concerns related to malicious traffic. Note that general security questions where the agent is helping to educate the user on the best practice such as SSO or MFA configuration, Security guidelines, or setting permissions for users and roles should be labeled as Customer Education and not Security Awareness. \n</description>\n</category>\n \n<category>\n<name>\n\"Feature Request\" \n</name>\n<description>\n“Feature Request” are the cases where the customer is experiencing a limitation in the application software and asking for a feature they want to have. Customer highlights a limitation and is requesting for the capability. For a Feature Request case, the support agent typically acknowledges that the question or expectation is a feature request for the software. Agent may use words such as the functionality or feature does not exist or it is currently not supported. \n</description>\n</category>\n \n<category>\n<name>\n\"Software Defect\" \n</name>\n<description>\n“Software Defect” are cases where the application does not work as expected. The support agent confirms this through analysis and troubleshooting and mentions internal team is working on a fix or patch to address the bug or defect. \n</description>\n</category>\n \n<category>\n<name>\n\"Documentation Improvement\" \n</name>\n<description>\n“Documentation Improvement” are cases where there is a lack of documentation, incorrect documentation, or insufficient documentation and when the case is not attributed to a Software Defect or a Feature Request. In Documentation Improvement cases the agent acknowledges the application documentation is incomplete or not up to date, or that they will ask documentation team to improve the documentation. For Documentation Improvement cases, the agent may suggest a workaround that is not part of application documentation and does not reference the standard application documentation or link. References to workarounds or sources such as Github or Stack Overflow, when used as an example of a solution, are examples of a Documentation Improvement case because the details and examples are missing from the official documentation.\n</description>\n</category>\n \n<category>\n<name>\n\"Customer Education\" \n</name>\n<description>\n“Customer Education” cases are cases where the customer could have resolved the case information using the existing application documentation. In these cases, the agent is educating the customer they are not using the feature correctly or have an incorrect configuration, while guiding them to the documentation. Customer Education cases include scenario where an agent provides troubleshooting steps for a problem or answers a question and provides links to the official application documentation. User Education cases include cases when the customer asks for best practices and agent provides knowledge article links to the support center documentation. Customer Education also includes cases created by the agent or application developers to suggest and educate the customer on a change to reduce cost, improve security, or improve application performance. Customer Education cases include cases where the customer asks a question or requests help with an error or configuration and the agent guides them appropriately with steps or documentation links. Customer Education cases also include the cases where the customer is using an unsupported configuration or version that may be End Of Life (EOL). Customer Education cases also include inconclusive cases where the customer reported an issue with the application but the case is closed without resolution details.\n</description>\n</category>\n \n</categories>\n \nHere are some good examples with reasoning:\n \n<good examples>\n \n<example>\n<example data>\nCustomer: \"I noticed unexpected charges of $12,500 on our latest invoice, which is significantly higher than our usual $7,000 monthly spend. We haven't added new users, so I'm concerned about this increase.\"\nSupport: \"I understand your concern about the increased charges. Upon review, I see that 50 Premium Sales Cloud licenses were automatically activated on January 15th when your sandbox environments were refreshed. I can help adjust your sandbox configuration and discuss Enterprise License Agreement options to optimize costs.\"\nCustomer: \"Thank you for clarifying. Please tell me more about the Enterprise License options.\"\n</example data\n<example output>\n<classification>\n\"Billing Inquiry\"\n</classification>\n<explanation>\nCustomer is asking a question to clarify the unexpected increase in their billing statement charge and the agent explains why this occurred. The customer wants to learn more about ways to optimize costs.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"URGENT: We've detected unauthorized API calls from an unknown IP address accessing sensitive customer data in our production environment. Our monitoring shows 1000+ suspicious requests in the last hour.\"\nSupport: \"I understand the severity of this security incident. I've immediately revoked the compromised API credentials and initiated our security protocol. The suspicious traffic has been blocked. I'm escalating this to our Security team for forensic analysis. I'll stay engaged until this is resolved.\"\n</example data\n<example output>\n<classification>\n\"Security Awareness\"\n</classification>\n<explanation>\nCustomer reported unauthorized API calls and suspicious requests. The agent confirms revoking compromised API credentials and initiating the protocol.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"Is there a way to create custom notification templates for different user groups? We need department-specific alert formats, but I can only find a single global template option.\"\nSupport: \"I understand you're looking to customize notification templates per user group. Currently, this functionality isn't supported in our platform - we only offer the global template system. I'll submit this as a feature request to our product team. In the meantime, I can suggest using notification tags as a workaround.\"\nCustomer: \"Thanks, please add my vote for this feature.\"\n</example data\n<example output>\n<classification>\n\"Feature Request\"\n</classification>\n<explanation>\nCustomer is asking for a new feature to have custom notification templates for different user groups since they have a use case that is currently not supported by the application. The agent confirms the functionality does not exist and mentions submitting a feature request to the product team.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\"\nSupport: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"\n</example data\n<example output>\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nCustomer is reporting a data processing exception with a specific version and the agent confirms this is a regression and defect. The agent confirms that engineering is working to provide an emergency patch for the issue. \n</explanation>\n \n<example>\n<example data>\nCustomer: \"The data export function is failing consistently when we include custom fields. The export starts but crashes at 45% with error code DB-7721. This worked fine last week before the latest release.\"\nSupport: \"I've reproduced the issue in our test environment and confirmed this is a bug introduced in version 4.2.1. Our engineering team has identified the root cause - a query optimization error affecting custom field exports. They're working on a hotfix (patch 4.2.1.3).\"\nCustomer: \"Please notify when fixed.\"\n</example data>\n<example output>\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nThis is a Software Defect as the data export function is not working as expected to export the custom fields. The agent acknowledged the issue and confirmed engineering is working on a hotfix.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"I'm trying to implement the batch processing API but the documentation doesn't explain how to handle partial failures or provide retry examples. The current docs only show basic success scenarios.\"\nSupport: The documentation is lacking detailed error handling examples for batch processing. I'll submit this to our documentation team to add comprehensive retry logic examples and partial failure scenarios. For now, I can share a working code snippet that demonstrates proper error handling and retry mechanisms.\"\nCustomer: \"Thanks, the code example would help.\"\n</example data\n<example output>\n<classification>\nDocumentation Improvement\n</classification>\n<explanation>\nThe agent acknowledges the gap in the documentation and mentions they will pass on this to the documentation team for further improvements. Agent mentions providing a working code snippet with retry examples.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"We can't get our SSO integration working. The login keeps failing and we're not sure what's wrong with our configuration.\"\nSupport: \"I can help guide you through the SSO setup. Looking at your configuration, I notice the SAML assertion isn't properly formatted. Please follow our step-by-step SSO configuration guide here [link to docs]. Pay special attention to section 3.2 about SAML attributes. The guide includes validation steps to ensure proper integration.\"\nCustomer: \"Found the issue in section 3.2. Working now, thanks!\"\n</example data\n<example output>\n<classification>\nCustomer Education\n</classification>\n<explanation>\nCustomer is asking for help and guidance to get their SSO integration working. The agent went over the details and presented the steps along necessary along with the documentation links.\n</explanation>\n \n</good examples>\n \nHere are some examples for wrong classification with reasoning:\n \n<bad examples>\n \n<example>\n<example data>\nCustomer: \"We want to enhance our application security. Currently, each team member has individual login credentials. What's the recommended approach?\"\nSupport: \"recommend implementing SAML-based SSO with your existing identity provider. This will:\nCentralize authentication\nEnable MFA enforcement\nStreamline user provisioning\nEnhance security auditing\n</example data>\n<example output>\n<classification>\n\"Security Awareness\"\n</classification>\n<explanation>\nClassification should be Customer Education and not Security Awareness. General security questions where the agent is helping to educate the user such as Security guidelines and best practices, should be labeled as Customer Education.\n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"Our SAP invoices aren't syncing instantly with Salesforce opportunities. We've configured MuleSoft Composer as per documentation, but updates only happen intermittently.\"\nSupport: \"I understand you're looking for real-time synchronization. Currently, MuleSoft Composer's fastest sync interval is 15 minutes by design. While I can help optimize your current setup, I'll submit a feature request for real-time sync capability. Here's how to optimize the current polling interval: doc link\"\n</example data>\n<example output>\n<classification>\nCustomer Education\n</classification>\n<explanation>\nClassification should be Feature Request and not Customer Education. The agent tells the customer that fastest sync interval is 15 minutes by design. The agent also points out they will submit a Feature Request. Cases where the customer ask for features should be classified as Feature Request. \n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"Our sales ETL pipeline keeps timing out with error 'V_001' at the transform step. This was working perfectly before.\"\nSupport: \"I've analyzed your configuration. The timeout occurs because the transformation spans 5 years of data containing 23 cross-object formula fields and is running without filters. Please implement these optimization steps from our documentation: Document link on ETL performance\"\n</example data>\n<example output>\n<classification>\nSoftware Defect\n</classification>\n<explanation>\nClassification should be Customer Education and not Software Defect. The agent tells the user that timeout is caused by misconfiguration and needs to be restricted using filters. The agent provides documentation explaining how to troubleshoot the issue. Cases where the agent tells the user how to solve the problem and provides documentation should be labeled Customer Education.\n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"We are trying to deploy a custom workflow template but receiving this error: Resource handler returned message: 'Error: Multiple or missing values for mandatory single-value field, Field: ACTION_TYPE, Parameter: Workflow Action (Status Code: 400, Request ID: TKT-2481-49bc)' when deploying through Flow Designer.\"\nSupport: \"I've reviewed your Flow Designer deployment (instance: dev85xxx.xxx.com/flow/TKT-2481-49bc) which failed to create a Workflow Action resource. This error occurs when the action configuration is ambiguous. After checking the Flow Designer documentation [1], each Action Step in your template must define exactly one 'Action Type' attribute. The Flow Designer documentation [2] specifies that each workflow action requires a single, explicit action type definition. You cannot have multiple or undefined action types in a single step. This is similar to an issue reported in the Product Community [3]. Please review your workflow template and ensure each action step has exactly one defined Action Type. The documentation provides detailed configuration examples at [4]. Let me know if you need any clarification on implementing these changes.\n</example data>\n<example output>\n<classification>\nDocumentation Improvement\n</classification>\n<explanation>\nClassification should be Customer Education and not Documentation Improvement. The agent tells the user they have to change the action configuration and define an Action type attribute. Cases where the agent tells the user how to solve problem and provides documentation should be classified Customer Education.\n</explanation>\n</example>\n \n</bad examples>\n \nGiven the above categories defined in XML, logically think through which category fits best and then complete the classification. Provide a response in XML with the following elements: classification, explanation (limited to 2 sentences). Return your results as this sample output XML below and do not append your thought process to the response.\n \n<response> \n<classification> Software Defect </classification>\n<explanation> The support case is for ETL Pipeline Performance Degradation where the customer reports their nightly data transformation job takes 6 hours to complete instead of 2 hours before but no changes to configuration occurred. The agent mentions Engineering confirmed memory leak in version 5.1.2 and are deploying a Hotfix indicating this is a Software Defect.\n</explanation> \n</response> \n \nHere is the conversation you need to categorize:\n\n\n\nAbout the Authors\nSumeet Kumar is a Sr. Enterprise Support Manager at AWS leading the technical and strategic advisory team of TAM builders for automotive and manufacturing customers. He has diverse support operations experience and is passionate about creating innovative solutions using AI/ML.\nAndy Brand is a Principal Technical Account Manager at AWS, where he helps education customers develop secure, performant, and cost-effective cloud solutions. With over 40 years of experience building, operating, and supporting enterprise software, he has a proven track record of addressing complex challenges.\nTom Coombs is a Principal Technical Account Manager at AWS, based in Switzerland. In Tom’s role, he helps enterprise AWS customers operate effectively in the cloud. From a development background, he specializes in machine learning and sustainability.\nRamu Ponugumati is a Sr. Technical Account Manager and a specialist in analytics and AI/ML at AWS. He works with enterprise customers to modernize and cost optimize workloads, and helps them build reliable and secure applications on the AWS platform. Outside of work, he loves spending time with his family, playing badminton, and hiking.",
      "date": "2025-03-27",
      "authors": "Sumeet Kumar",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses how Amazon Bedrock can be utilized to generate high-quality categorical ground truth data for machine learning models, particularly in cost-sensitive environments. It highlights the role of generative AI in simplifying the training of multiclass classification models and showcases methods for prompt engineering and evaluation to achieve high accuracy.",
      "takeaways": [
        "- Amazon Bedrock offers a cost-effective solution for generating structured, balanced ground truth datasets essential for training supervised learning models in multiclass classification tasks.",
        "- Effective prompt engineering and iterative testing can significantly enhance the accuracy of large language models (LLMs) in categorizing support case data, demonstrating an improvement to over 90% accuracy.",
        "- The article emphasizes the importance of integrating generated labeled data into machine learning pipelines to enable scalable inference and prediction, optimizing both time and resource expenditure on data annotation."
      ]
    },
    {
      "id": 9,
      "title": "Enable Amazon Bedrock cross-Region inference in multi-account environments",
      "link": "https://aws.amazon.com/blogs/machine-learning/enable-amazon-bedrock-cross-region-inference-in-multi-account-environments/",
      "description": "Amazon Bedrock cross-Region inference capability that provides organizations with flexibility to access foundation models (FMs) across AWS Regions while maintaining optimal performance and availability. However, some enterprises implement strict Regional access controls through service control policies (SCPs) or AWS Control Tower to adhere to compliance requirements, inadvertently blocking cross-Region inference functionality in Amazon Bedrock. This creates a challenging situation where organizations must balance security controls with using AI capabilities.\nIn this post, we explore how to modify your Regional access controls to specifically allow Amazon Bedrock cross-Region inference while maintaining broader Regional restrictions for other AWS services. We provide practical examples for both SCP modifications and AWS Control Tower implementations.\nUnderstanding cross-Region inference\nWhen running model inference in on-demand mode, your requests might be restricted by service quotas or during peak usage times. Cross-Region inference enables you to seamlessly manage unplanned traffic bursts by utilizing compute across different Regions. With cross-Region inference, you can distribute traffic across multiple Regions, enabling higher throughput.\nMany organizations implement Regional access controls through:\n\nSCPs in AWS Organizations\nAWS Control Tower controls\nCustom AWS Identity and Access Management (IAM) policies\n\nThese controls typically deny access to all services in specific Regions for security, compliance, or cost management reasons. However, these broad denials also prevent Amazon Bedrock from functioning properly when it needs to access models in those Regions through cross-Region inference.\nHow Cross-Region inference works and interacts with SCPs\nCross-Region inference in Amazon Bedrock is a powerful feature that enables automatic cross-Region routing for inference requests. This capability is particularly beneficial for developers using on-demand inference mode, because it provides a seamless solution for achieving higher throughput and performance while effectively managing incoming traffic spikes in applications powered by Amazon Bedrock.\nWith cross-Region inference, developers can alleviate the need to predict demand fluctuations manually. Instead, the system dynamically routes traffic across multiple Regions, maintaining optimal resource utilization and performance. Importantly, cross-Region inference prioritizes the connected Amazon Bedrock API source Region when possible, helping minimize latency and improve overall responsiveness. This intelligent routing enhances applications’ reliability, performance, and efficiency without requiring constant oversight from development teams.\nAt its core, cross-Region inference operates on two key concepts: the source Region and the fulfillment Region. The source Region, also known as the origination Region, is where the inference request is initially invoked by the client. In contrast, the fulfillment Region is the Region that actually services the large language model (LLM) invocation request.\nCross-Region inference employs a proprietary custom routing logic that Amazon continuously evolves to provide the best inference experience for customers. This routing mechanism is intentionally heuristics-based, with a primary focus on providing high availability. By default, the service attempts to fulfill requests from the source Region, when possible, but it can seamlessly route requests to other Regions as needed. This intelligent routing considers factors such as Regional capacity, latency, and availability to make optimal decisions.\nAlthough cross-Region inference offers powerful flexibility, it requires access to models in all potential fulfillment Regions to function properly. This requirement is where SCPs can significantly impact cross-Region inference functionality.\nLet’s examine a scenario that highlights the critical interaction between cross-Region inference and SCPs. As illustrated in the following figure, we use two Regions, us-east-1 and us-west-2, and have denied all other Regions using an SCP that could have been implemented using AWS Organizations or an AWS Control Tower control.\n\nThe workflow consists of the following steps:\n\nA user makes an inference request to the us-east-1 Amazon Bedrock endpoint (source Region) using a cross-Region inference profile.\nThe Amazon Bedrock heuristics-based routing system evaluates available Regions for request fulfillment.\nus-west-2 and us-east-1 are allowed for Amazon Bedrock service access through SCPs, but us-east-2 is denied using the SCP.\nThis single Regional restriction (us-east-2) causes the cross-Region inference call to fail.\nEven though other Regions are available and allowed, the presence of one blocked Region (us-east-2) results in a failed request.\nThe client receives an error indicating they are not authorized to perform the action.\n\nThis behavior is by design; cross-Region inference service requires access to run inference in all potential fulfillment Regions to maintain its ability to optimally route requests. Attempts to use cross-Region inference will fail if any potential target Region is blocked by SCPs, regardless of other available Regions. To successfully implement cross-Region inference, organizations must make sure that their SCPs allow Amazon Bedrock api actions in all Regions where their target model is available. This means identifying all Regions where required models are hosted, modifying SCPs to allow minimal required Amazon Bedrock permissions in these Regions, and maintaining these permissions across all relevant Regions, even if some Regions are not primary operation zones. We will provide specific guidance on SCP modifications and AWS Control Tower implementations that enable cross-Region inference functionality in the following sections.\nUse case\nFor our sample use case, we use Regions us-east-1 and us-west-2. All other Regions are denied using the landing zone deny (GRREGIONDENY). The customer’s AWS accounts that are allowed to use Amazon Bedrock are under an Organizational Unit (OU) called Sandbox. We want to enable the accounts under the Sandbox OU to use Anthropic’s Claude 3.5 Sonnet v2 model using cross-Region inference. This model is available in us-east-1, us-east-2, and us-west-2, as shown in the following screenshot.\n\nIn the current state, when the user tries to use Anthropic’s Claude 3.5 Sonnet v2 model using cross-Region inference, they get an error stating the SCP is denying the action.\nModify existing SCPs to allow Amazon Bedrock cross-Region inference\nIf you aren’t using AWS Control Tower to govern the multi-account AWS environment, you can create a new SCP or modify an existing SCP to allow Amazon Bedrock cross-Region inference.\nThe following code is an example of how to modify an existing SCP that denies access to all services in specific Regions while allowing Amazon Bedrock inference through cross-Region inference for Anthropic’s Claude 3.5 Sonnet V2 model:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenySpecificRegionAllowCRI\",\n      \"Effect\": \"Deny\",\n      \"Action\": \"*\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestedRegion\": \"us-east-2\"\n        },\n        \"ArnNotLike\": {\n          \"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n        }\n      }\n    }\n  ]\n}\n\nThis policy effectively blocks all actions in the us-east-2 Region except for the specified resources. This is a deny-based policy, which means it should be used in conjunction with allow policies to define a full set of permissions.\nYou should review and adapt this example to your organization’s specific needs and security requirements before implementing it in a production environment.\nWhen implementing these policies, consider the following:\n\nCustomize the Region and allowed resources to fit your specific requirements\nTest thoroughly in your environment to make sure that it doesn’t unintentionally block necessary services or actions\nRemember that SCPs affect the users and roles in the accounts they’re attached to, including the root user\nService-linked roles are not affected by SCPs, allowing other AWS services to integrate with AWS Organizations\n\nImplementation using AWS Control Tower\nAWS Control Tower creates SCPs to manage permissions across your organization. Manually editing these SCPs is not recommended because it can cause drift in your AWS Control Tower environment. However, there are some approaches you can take to allow specific AWS services, which we discuss in the following sections.\nPrerequisites\nMake sure that you’re running the latest version of AWS Control Tower. If you’re using a version less than 3.x and have Regions denied through AWS Control Tower settings, you need to enable your AWS Control Tower version to update the Region deny settings. Refer to the following considerations related to AWS Control Tower upgrades from 2.x to 3.x.\nAdditionally, make sure that the Organization dashboard on AWS Control Tower doesn’t show policy drifts and that the OUs and accounts are in compliance.\nOption 1: Extend existing Region deny SCPs for cross-Region inference\nAWS Control Tower offers two primary Region deny controls to restrict access to AWS services based on Regions:\n\nGRREGIONDENY (landing zone Region deny control) – This control applies to the entire landing zone rather than specific OUs. When enabled, it disallows access to operations in global and Regional services outside of specified Regions, including all Regions where AWS Control Tower is not available and all Regions not selected for governance.\nMULTISERVICE.PV.1 (OU Region deny control) – This configurable control can be applied to specific OUs rather than the entire landing zone. It disallows access to unlisted operations in global and Regional AWS services outside of specified Regions for an organizational unit. This control is configurable. This control accepts one or more parameters, such as AllowedRegions, ExemptedPrincipalARNs, and ExemptedActions, which describe operations that are allowed for accounts that are part of this OU: \n  \nAllowedRegions – Specifies the Regions selected, in which the OU is allowed to operate. This parameter is mandatory.\nExemptedPrincipalARNs – Specifies the IAM principals that are exempt from this control, so that they are allowed to operate certain AWS services globally.\nExemptedActions – Specifies actions that are exempt from this control, so that the actions are allowed.\n \n\nWe will use the CT.MULTISERVICE.PV.1 control and configure it for our scenario.\n\nCreate an IAM role with an IAM policy that will allow Amazon Bedrock inference using cross-Region inference. Let’s name this IAM role Bedrock-Access-CRI. We will use this at a later step. This IAM role will be created in AWS accounts that are part of the Sandbox OU. \n  \n {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowBedrockInference\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:InvokeModelWithResponseStream\"\n            ],\n            \"Resource\": [\n                \"arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n                \"arn:aws:bedrock:*::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0\"\n            ]\n        }\n    ]\n}\n \n\n\nNavigate to the Landing zone settings page and choose Modify settings.\nEnable the Region, us-east-2 in our case, and leave the rest of the settings unchanged.\nChoose Update landing zone to complete the changes.\n\nThe updates can take up to 60 minutes or more depending on the size of the Organization. This will update the landing zone Region deny settings (GRREGIONDENY) to include the Region us-east-2 to govern the Region.\n\nWhen the landing zone setup is complete, review the Organization settings to make sure that there are no pending updates for AWS accounts across the OUs. If you see pending updates, complete updating them and make sure the status for the account status shows Enrolled.\nOn the AWS Control Tower console, choose All controls under Controls library in the navigation pane to see a list of controls.\nLocate MULTISERVICE.PV.1 and choose the policy to open the control. \nChoose Control actions followed by Enable to start the configuration.\nOn the Select an OU page, select the OU you want to apply this control to. For our use case, we use the Sandbox OU.\nChoose Next. \nOn the Specify Region access page, select the Regions to allow access for the OU. For our use case, we select us-west-2 and us-east-1.\n\nWe don’t select us-east-2 because we want to deny all services on us-east-2 and only allow Amazon Bedrock inference through cross-Region inference.\n\nChoose Next.\nOn the Add service actions – optional page, add the Amazon Bedrock actions to the NotActions We add bedrock:Invoke* to allow Amazon Bedrock InvokeModel actions.\nChoose Next. \nOn the Specify configurations and tags – optional page, add the IAM role we created earlier under Exempted principals and choose Next. \nReview the configuration and choose Enable control.\n\nAfter the control is enabled, you can review the configuration by choosing OUs enabled, Accounts, Artifacts, and the Regions tab.\nThis completes the configuration. You can test the Amazon Bedrock inference with Anthropic’s Sonnet 3.5 v2 using the Amazon Bedrock console or the API by assuming the custom IAM role mentioned in the previous step (Bedrock-Access-CRI).\nYou will see that you can make Amazon Bedrock inference calls to only Anthropic’s Sonnet 3.5 v2 model using cross-Region inference from all of the three Regions (us-east-1, us-east-2, and us-west-2). Attempts to access other services on us-east-2 are blocked due to the CT.MULTISERVICE.PV.1 control you configured earlier.\nBy following these approaches, you can safely extend the permissions managed by AWS Control Tower without causing drift or compromising your governance controls.\nOption 2: Enable the denied Region using AWS Control Tower and conditionally block using an SCP\nIn this option, we enable the denied Region (us-east-2) and create a new SCP to conditionally block us-east-2 while allowing Amazon Bedrock inference through cross-Region inference.\n\nNavigate to the Landing zone settings page and choose Modify settings.\nEnable the Region, us-east-2 in our case, and leave the rest of the settings unchanged.\nChoose Update landing zone to complete the changes.\n\nThe updates can take up to 60 minutes or more depending on the size of the Organization. You can monitor the status of this update on the console.\n\nWhen the landing zone setup is complete, review the Organization settings to make sure that there are no pending updates for AWS accounts across the OUs. If you see pending updates, complete updating them and make sure the status for the account status shows Enrolled.\nOn the AWS Control Tower console, choose Service Control Policies under Policies in the navigation pane.\nCreate a new SCP with the sample policy shown earlier. This SCP denies all actions for us-east-2 while allowing Amazon Bedrock inference using a CRI profile ARN for Anthropic’s Claude Sonnet 3.5 v2.\nApply the SCP to the specific OU. In this scenario, we use the Sandbox OU.\n\nBecause you’re creating a new SCP and not modifying the existing SCPs created by AWS Control Tower, you will not see a drift in the AWS Control Tower state.\nYou can now test the update by running a few inference calls using the Amazon Bedrock console or the AWS Command Line Interface (AWS CLI). You will see that you can make Amazon Bedrock inference calls to only Anthropic’s Sonnet 3.5 v2 model using cross-Region inference from all three of the Regions (us-east-1, us-east-2, and us-west-2). Access to other AWS services on us-east-2 will be denied.\nUsing Customizations for AWS Control Tower to deploy SCPs\nThe recommended way to add custom SCPs is through the Customizations for AWS Control Tower (CfCT) solution:\n\nDeploy the CfCT solution in your management account.\nCreate a configuration package with your custom SCPs.\n\nThe following screenshot shows an example SCP that denies a specific Region while allowing calls to Amazon Bedrock using cross-Region inference for Anthropic’s Sonnet 3.5 v2 model.\n\n\nPrepare a manifest.yaml file that defines your policies.\n\nThe following screenshot shows an example manifest.yaml that defines the resources targeting the Sandbox OU.\n\n\nDeploy your custom SCPs to specific OUs.\n\nSummary\nAmazon Bedrock cross-Region inference provides valuable flexibility for organizations looking to use FMs across Regions. By carefully modifying your service control policies or AWS Control Tower controls, you can enable this functionality while maintaining your broader Regional access restrictions.\nThis approach allows you to:\n\nMaintain compliance with Regional access requirements\nTake advantage of the full capabilities of Amazon Bedrock\nSimplify your application architecture by accessing models from your primary Region\n\nThere is no additional cost associated with cross-Region inference, including the failover capabilities provided by this feature. This includes management, data transfer, encryption, network usage, and potential differences in price per million token per model. You pay the same price per token of the individual models in your source Region.\nAs AI and machine learning capabilities continue to evolve, finding the right balance between security controls and innovation enablement will remain a key challenge for organizations. The approach outlined in this post provides a practical solution to this specific challenge.\nFor more information, refer to Increase throughput with cross-region inference.\n\nAbout the Authors\nSatveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer’s deep understanding of generative AI technologies allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value.\nRamesh Venkataraman is a Solutions Architect who enjoys working with customers to solve their technical challenges using AWS services. Outside of work, Ramesh enjoys following stack overflow questions and answers them in any way he can.\nDhawal Patel is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and artificial intelligence. He focuses on deep learning, including NLP and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker.\nSumit Kumar is a Principal Product Manager, Technical at AWS Bedrock team, based in Seattle. He has over 12 years of product management experience across a variety of domains and is passionate about AI/ML. Outside of work, Sumit loves to travel and enjoys playing cricket and lawn tennis.",
      "date": "2025-03-27",
      "authors": "Satveer Khurpa",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses Amazon Bedrock's new cross-Region inference capability that allows organizations to access foundation models across different AWS regions while addressing compliance and security challenges associated with regional access controls. It provides guidance on modifying service control policies (SCPs) and AWS Control Tower settings to enable cross-Region inference without compromising security requirements.",
      "takeaways": [
        "- Cross-Region inference in Amazon Bedrock enables higher throughput and performance by dynamically routing traffic across multiple AWS regions.",
        "- Organizations must ensure that their SCPs allow access to all regions where their AI models are hosted to avoid failures in inference requests.",
        "- The article offers practical examples of modifying SCPs and AWS Control Tower configurations to facilitate cross-Region inference while maintaining compliance and security controls."
      ]
    },
    {
      "id": 10,
      "title": "Amazon SageMaker JumpStart adds fine-tuning support for models in a private model hub",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-jumpstart-adds-fine-tuning-support-for-models-in-a-private-model-hub/",
      "description": "Amazon SageMaker JumpStart is a machine learning (ML) hub that provides pre-trained models, solution templates, and algorithms to help developers quickly get started with machine learning. Within SageMaker JumpStart, the private model hub feature allows organizations to create their own internal repository of ML models, enabling teams to share and manage models securely within their organization.\nToday, we are announcing an enhanced private hub feature with several new capabilities that give organizations greater control over their ML assets. These enhancements include the ability to fine-tune SageMaker JumpStart models directly within the private hub, support for adding and managing custom-trained models, deep linking capabilities for associated notebooks, and improved model version management. These new features streamline the ML workflow by combining the convenience of pre-built solutions with the flexibility of custom development, while maintaining enterprise-grade security and governance.\nFor enterprise customers, the ability to curate and fine-tune both pre-built and custom models is crucial for successful AI implementation. Model curation provides quality control, compliance, and security while preventing duplicate efforts across teams. When enterprises fine-tune curated models, they can specialize general-purpose solutions for their specific industry needs and gain competitive advantages through improved performance on their proprietary data. Similarly, the ability to fine-tune custom models enables organizations to continuously improve their AI solutions, adapt to changing business conditions, and preserve institutional knowledge, while maintaining cost-efficiency.\nA common enterprise scenario involves centralized data science teams developing foundation models (FMs), evaluating the performance against open source FMs, and iterating on performance. After they develop their custom FM, it can serve as a baseline for the entire organization, and individual departments—such as legal, finance, or customer service—can fine-tune these models using their department-specific data that might be subject to different privacy requirements or access controls. This hub-and-spoke approach to model development maximizes resource efficiency while allowing for specialized optimization at the department level. This comprehensive approach to model management, now supported by the enhanced private hub features in SageMaker JumpStart, enables enterprises to balance standardization with customization while maintaining proper governance and control over their ML assets.\nSolution overview\nSageMaker JumpStart has introduced several new enhancements to its private model hub feature, allowing administrators greater control and flexibility in managing their organization’s ML models. These enhancements include:\n\nFine-tuning of models referenced in the private hub – Administrators can now add models from the SageMaker JumpStart catalog to their private hub and fine-tune them using Amazon SageMaker training jobs, without having to create the models from scratch.\nSupport for custom models – In addition to the pre-trained SageMaker JumpStart models, administrators can now add their own custom-trained models to the private hub and fine-tune them as needed.\nDeep linking of notebooks – Administrators can now deep link to specific notebooks associated with the models in the private hub, making it straightforward for users to access and work with the models.\nUpdating models in the private hub – The private hub now supports updating models over time as new versions or iterations become available, allowing organizations to stay current with the latest model improvements.\n\nThese new capabilities give AWS customers more control over their ML infrastructure and enable faster model deployment and experimentation, while still maintaining the appropriate access controls and permissions within their organization.\nIn the following sections, we provide guidance on how to use these new private model hub features using the Amazon SageMaker SDK and Amazon SageMaker Studio console.\nTo learn more about how to manage models using private hubs, see Manage Amazon SageMaker JumpStart foundation model access with private hubs.\nPrerequisites\nTo use the SageMaker Python SDK and run the code associated with this post, you need the following prerequisites:\n\nAn AWS account that contains your AWS resources\nAn AWS Identity and Access Management (IAM) role with access to SageMaker Studio notebooks\nSageMaker JumpStart enabled in a SageMaker Studio domain\n\nCreate a private hub, curate models, and configure access control\nThis section provides a step-by-step guide for administrators to create a private hub, curate models, and configure access control for your organization’s users.\n\nBecause the feature has been integrated in the latest SageMaker Python SDK, to use the model granular access control feature with a private hub, let’s first update the SageMaker Python SDK: \n  \n!pip3 install sagemaker —force-reinstall —quiet\n \nNext, import the SageMaker and Boto3 libraries: \n  \nimport boto3 from sagemaker\nimport Session from sagemaker.session\nimport Hub\n \nConfigure your private hub: \n  \nHUB_NAME=\"CompanyHub\"\nHUB_DISPLAY_NAME=\"Allowlisted Models\"\nHUB_DESCRIPTION=\"These are allowlisted models taken from the SageMaker Public Hub\"\nREGION=\"<your_region_name>\" # for example, \"us-west-2\"\n \n\nIn the preceding code, HUB_NAME specifies the name of your hub. HUB_DISPLAY_NAME is the display name for your hub that will be shown to users in UI experiences. HUB_DESCRIPTION is the description for your hub that will be shown to users.\nUse an AWS Region where SageMaker JumpStart is available, as of March 2025: us-west-2, us-east-1, us-east-2, eu-west-1, eu-central-1, eu-central-2, eu-north-1, eu-south-2, me-south-1, me-central-1, ap-south-1, ap-south-2, eu-west-3, af-south-1, sa-east-1, ap-east-1, ap-northeast-2, ap-northeast-3, ap-southeast-3, ap-southeast-4, ap-southeast-5, ap-southeast-7, eu-west-2, eu-south-1, ap-northeast-1, us-west-1, ap-southeast-1, ap-southeast-2, ca-central-1, ca-west-1, cn-north-1, cn-northwest-1, il-central-1, mx-central-1, us-gov-east-1, us-gov-west-1.\n\nSet up a Boto3 client for SageMaker: \n  \nsm_client = boto3.client('sagemaker')\nsession = Session(sagemaker_client=sm_client)\nsession.get_caller_identity_arn()\n \nCheck if the following policies have been already added to your admin IAM role; if not, you can add them as inline policies (use the Region configured in Step 3): \n  \n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:GetObjectTagging\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>\",\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>/*\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n \n\nIn addition to setting up IAM permissions to the admin role, you need to scope down permissions for your users so they can’t access public contents.\n\nUse the following policy to deny access to the public hub for your users. These can be added as inline policies in the user’s IAM role (use the Region configured in Step 3): \n  \n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\": \"Deny\",\n            \"Resource\": [\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>\",\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>/*\"\n            ],\n            \"Condition\": {\n                \"StringNotLike\": {\"s3:prefix\": [\"*.ipynb\", \"*/eula.txt\"]}\n            }\n        },\n        {\n            \"Action\": \"sagemaker:*\",\n            \"Effect\": \"Deny\",\n            \"Resource\": [\n                \"arn:aws:sagemaker:<REGION>:aws:hub/SageMakerPublicHub\",\n                \"arn:aws:sagemaker:<REGION>:aws:hub-content/SageMakerPublicHub/*/*\"\n            ]\n        }\n    ]\n}\n \n\nAfter you have set up the private hub configuration and permissions, you’re ready to create the private hub.\n\nUse the following code to create the private hub within your AWS account in the Region you specified earlier: \n  \nhub = Hub(hub_name=HUB_NAME, sagemaker_session=session)\n\ntry:\n  hub.create(\n      description=HUB_DESCRIPTION,\n      display_name=HUB_DISPLAY_NAME\n  )\n  print(f\"Successfully created Hub with name {HUB_NAME} in {REGION}\")\nexcept Exception as e:\n  if \"ResourceInUse\" in str(e):\n    print(f\"A hub with the name {HUB_NAME} already exists in your account.\")\n  else:\n    raise e\n \nUse describe() to verify the configuration of your hub. After your private hub is set up, you can add a reference to models from the SageMaker JumpStart public hub to your private hub. No model artifacts need to be managed by the customer. The SageMaker team will manage version or security updates. For a list of available models, refer to Built-in Algorithms with pre-trained Model Table.\nTo search programmatically, run the following command: \n  \nfrom sagemaker.jumpstart.filters import Or\n\nfilter_value = Or(\n\"framework == meta\",\n\"framework == deepseek\"\n)\nmodels = []\nnext_token = None\n\nwhile True:\n    response = hub.list_sagemaker_public_hub_models(\n        filter=filter_value,\n        next_token=next_token\n    )\n    models.extend(response[\"hub_content_summaries\"])\n    next_token = response.get(\"next_token\")\n    \n    if not next_token:\n        break\nprint(models)\n \n\nThe filter argument is optional. For a list of filters you can apply, refer to the following GitHub repo.\n\nUse the retrieved models from the preceding command to create model references for your private hub: \n  \nfor model in models:\n    print(f\"Adding {model.get('hub_content_name')} to Hub\")\n    hub.create_model_reference(model_arn=model.get(\"hub_content_arn\"), \n                               model_name=model.get(\"hub_content_name\"))\n \n\nThe SageMaker JumpStart private hub offers other useful features for managing and interacting with the curated models. Administrators can check the metadata of a specific model using the hub.describe_model(model_name=<model_name>) command. To list the available models in the private hub, you can use a simple loop:\n\nresponse = hub.list_models()\nmodels = response[\"hub_content_summaries\"]\nwhile response[\"next_token\"]:\n    response = hub.list_models(next_token=response[\"next_token\"])\n    models.extend(response[\"hub_content_summaries\"])\n\nfor model in models:\n    print(model.get('HubContentArn'))\n\nIf you need to remove a specific model reference from the private hub, use the following command:\n\nhub.delete_model_reference(\"<model_name>\")\n\nIf you want to delete the private hub from your account and Region, you will need to delete all the HubContents first, then delete the private hub. Use the following code:\n\nfor model in models:\n    hub.delete_model_reference(model_name=model.get('HubContentName'))\n    \nhub.delete()\n\nFine-tune models referenced in the private hub\nThis section walks through how to interact with allowlisted models in SageMaker JumpStart. We demonstrate how to list available models, identify a model from the public hub, and fine-tune the model using the SageMaker Python SDK as well as the SageMaker Studio UI.\nUser experience using the SageMaker Python SDK\nTo interact with your models using the SageMaker Python SDK, complete the following steps:\n\nJust like the admin process, the first step is to force reinstall the SageMaker Python SDK: \n  \n!pip3 install sagemaker —force-reinstall —quiet\n \nWhen interacting with the SageMaker SDK functions, add references to the hub_arn: \n  \nmodel_id=\"meta-vlm-llama-3-2-11b-vision\"\nmodel_version=\"2.1.8\"\nhub_arn=\"<YourHubARN>\"\n\nfrom sagemaker import hyperparameters\n\nmy_hyperparameters = hyperparameters.retrieve_default(\n    model_id=model_id, model_version=model_version, hub_arn=hub_arn\n)\nprint(my_hyperparameters)\nhyperparameters.validate(\n    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters, hub_arn=hub_arn\n)\n \nYou can then start a training job by specifying the model ID, version, and hub name: \n  \nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nestimator = JumpStartEstimator(\n    model_id=model_id,\n    hub_name=hub_arn,\n    model_version=model_version,\n    environment={\"accept_eula\": \"false\"},  # Please change {\"accept_eula\": \"true\"}\n    disable_output_compression=True,\n    instance_type=\"ml.p4d.24xlarge\",\n    hyperparameters=my_hyperparameters,\n)\nestimator.fit({\"training\": train_data_location})\n \n\nFor a custom model, see the example notebooks in GitHub.\nUser experience in SageMaker Studio\nComplete the following steps to interact with allowlisted models using SageMaker Studio:\n\nOn the SageMaker Studio console, choose JumpStart in the navigation pane or in the Prebuilt and automated solutions section. \nChoose one of model hubs you have access to.\n\nIf the user has access to multiple hubs, you will see a list of hubs, as shown in the following screenshot.\n\nIf the user has access to only one hub, you will be redirected to the model list.\n\n\nTo fine-tune a model, choose Train (this option will be enabled if it’s supported). \nModify your training job configurations like training data, instance type, and hyperparameters, and choose Submit. \n\nDeep link notebooks in the private hub\nYou can now also access the notebook associated with the model in your curated hub.\n\nChoose your model, then choose Preview notebooks. \nChoose Open in JupyterLab to start the deep link workflow. \nSelect a running JupyterLab space and choose Open notebook.\n\nYou will need to upgrade your space to use a SageMaker distribution of at least 2.4.1. For more information on how to upgrade your SageMaker distribution, see Update the SageMaker Distribution Image.\n\nThis will automatically open the selected notebook in your JupyterLab instance, with your private HubName inputted into the necessary classes.\n\nUpdate models in the private hub\nModify your existing private HubContent by calling the new sagemaker:UpdateHubContent API. You can now update an existing HubContent version in-place without needing to delete and re-add it. We don’t support updating the HubContentDocument at this time because there can be backward-incompatible changes that are introduced that fundamentally alter the performance and usage of the model itself. Refer to the public API documentation for more details.\n\nclient.update_hub_content(\n    hub_content_name=\"my-model\",\n    hub_content_version=\"1.0.0\",\n    hub_content_type=\"Model\",\n    hub_name=\"my-hub\",\n    support_status=\"DEPRECATED\"\n)\n\nAdditionally, you can modify your ModelReferences by calling the new sagemaker:UpdateHubContentReference API. Refer to the public API documentation for more usage details.\n\nclient.update_hub_content_reference(\n    hub_content_name=\"your-model\",\n    hub_content_type=\"ModelReference\",\n    hub_name=\"my-hub\",\n    min_version=\"1.2.0\"\n)\n\nConclusion\nThis post demonstrated the new enhancements to the SageMaker JumpStart private model hub feature, which gives enterprise customers greater control and flexibility in managing their ML assets. The key capabilities introduced include the ability to fine-tune pre-built SageMaker JumpStart models directly within the private hub, support for importing and fine-tuning custom-trained models, deep linking to associated notebooks for streamlined access and collaboration, and improved model version management through APIs. These features enable enterprises to curate a centralized repository of trusted, specialized ML models, while still providing the flexibility for individual teams and departments to fine-tune and adapt these models to their specific needs. The seamless integration with SageMaker Studio further streamlines the model development and deployment workflow, empowering enterprises to accelerate their ML initiatives while maintaining the appropriate security and control over their ML assets.\nNow that you’ve seen how the enhanced private model hub features in Amazon SageMaker JumpStart can give your organization greater control and flexibility over managing your machine learning assets, start leveraging these capabilities to curate a centralized repository of trusted models and accelerate your AI initiatives.\n\nAbout the Authors\nMarc Karp is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.\nNiris Okram is a senior academic research specialist solutions architect at AWS. He has extensive experience working with public, private and research customers on various fields related to cloud. He is passionate about designing and building systems to accelerate the customer’s mission on AWS cloud.\nBenjamin Crabtree is a software engineer with the Amazon SageMaker and Bedrock teams. He is passionate about democratizing the new and frequent breakthroughs in AI. Ben received his undergraduate degree from the University of Michigan and now lives in Brooklyn, NY.\nBanu Nagasundaram leads product, engineering, and strategic partnerships for SageMaker JumpStart, SageMaker’s machine learning and GenAI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.",
      "date": "2025-03-26",
      "authors": "Marc Karp",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses enhancements to the Amazon SageMaker JumpStart private model hub, enabling organizations to fine-tune pre-trained models, manage custom models, and streamline model versioning and access through deep linking of notebooks. These features aim to provide enterprises with greater control over their machine learning assets while ensuring security and compliance.",
      "takeaways": [
        "- Organizations can now fine-tune both pre-trained and custom models within a secure private model hub, boosting performance tailored to specific needs.",
        "- New deep linking capabilities facilitate easier access to associated notebooks, improving collaboration and workflow among data science teams.",
        "- Enhanced model version management and governance features allow enterprises to maintain control over their machine learning assets while adapting to evolving business requirements."
      ]
    },
    {
      "id": 11,
      "title": "Universally Instance-Optimal Mechanisms for Private Statistical Estimation",
      "link": "https://machinelearning.apple.com/research/universally-instance-optimal-mechanisms",
      "description": "We consider the problem of instance-optimal statistical estimation under the constraint of differential privacy where mechanisms must adapt to the difficulty of the input dataset. We prove a\nnew instance specific lower bound using a new divergence and show it characterizes the local minimax optimal rates for private statistical estimation. We propose two new mechanisms that are\nuniversally instance-optimal for general estimation problems up to logarithmic factors. Our first\nmechanism, the total variation mechanism, builds on the exponential mechanism with stable approximations of the total…",
      "date": "2025-04-02",
      "authors": [],
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "term": "Apple: Machine Learning",
      "summary": "The article discusses instance-optimal statistical estimation under differential privacy, introducing new mechanisms that adapt to the difficulty of the input dataset. It presents a lower bound characterization using a new divergence and proposes two universally instance-optimal mechanisms for general estimation problems.",
      "takeaways": [
        "- The research provides a new instance-specific lower bound that enhances understanding of local minimax optimal rates in private statistical estimation.",
        "- Two innovative mechanisms are introduced, contributing to improving statistical estimation while maintaining differential privacy.",
        "- The findings showcase advancements in adapting mechanisms to the complexity of input datasets, which could be crucial for AI applications requiring privacy considerations."
      ]
    },
    {
      "id": 12,
      "title": "ChatGPT’s New Image Generator Is Melting GPUs and Redefining Creativity",
      "link": "https://www.marketingaiinstitute.com/blog/chatgpt-4o-image-generation",
      "description": "\n  \n\nOpenAI just set the AI world on fire again—this time by rolling out a brand-new image generation capability within GPT‑4o that has users everywhere buzzing. \n",
      "date": "2025-04-01",
      "authors": "Mike Kaput",
      "journal": "marketingaiinstitute.com",
      "therapyArea": "AI Marketing and Advertising",
      "term": "Marketing AI Institute",
      "summary": "OpenAI has introduced a new image generation feature within their GPT-4o model, creating a significant buzz in the AI community as it showcases cutting-edge capabilities in creative technology. This release is expected to push the limits of GPU usage and redefine the landscape of artistic expression through AI.",
      "takeaways": [
        "- The new image generation feature in GPT-4o highlights advancements in AI creativity and capability.",
        "- Increased demand for GPU resources indicates the intensive computational requirements of new AI models.",
        "- This development may influence various industries, including marketing and advertising, by allowing for novel forms of content creation."
      ]
    },
    {
      "id": 13,
      "title": "[The AI Show Episode 142]: ChatGPT’s New Image Generator, Studio Ghibli Craze and Backlash, Gemini 2.5, OpenAI Academy, 4o Updates, Vibe Marketing & xAI Acquires X",
      "link": "https://www.marketingaiinstitute.com/blog/the-ai-show-episode-142",
      "description": "\n  \n\nThis week, Paul and Mike are together again with a 60+-minute podcast episode focused on another wild week in AI.\nFrom ChatGPT’s jaw-dropping new image generator and the viral Studio Ghibli craze (and controversy) to Google’s Gemini 2.5 update and the launch of OpenAI Academy—there’s no shortage of major moves. Plus: updates to GPT-4o, the rise of “vibe marketing,” xAI’s acquisition of X, and what it all means for the future of work, creativity, and coding.\nListen or watch below—and see below for show notes, timestamps, articles discussed, and the transcript.\n",
      "date": "2025-04-01",
      "authors": "Claire Prudhomme",
      "journal": "marketingaiinstitute.com",
      "therapyArea": "AI Marketing and Advertising",
      "term": "Marketing AI Institute",
      "summary": "The podcast episode covers significant updates in the AI field, including ChatGPT's new image generator, updates to Google's Gemini 2.5, the launch of OpenAI Academy, and discussions on emerging marketing strategies like \"vibe marketing\" and xAI's acquisition of X. The hosts explore how these developments may impact creativity, work, and coding.",
      "takeaways": [
        "- ChatGPT introduced a new image generator, showcasing advancements in AI's application for creative tasks.",
        "- Google's Gemini 2.5 update signifies ongoing improvements in AI model capabilities.",
        "- The concept of \"vibe marketing\" reflects a shift in advertising strategies driven by AI innovations."
      ]
    },
    {
      "id": 14,
      "title": "[The AI Show Episode 141]: Road to AGI (and Beyond) #1 — The AI Timeline is Accelerating",
      "link": "https://www.marketingaiinstitute.com/blog/the-ai-show-episode-141",
      "description": "\n  \n\nThe future of AI is arriving faster than most are ready for. \nIn this kickoff episode of The Road to AGI series, Paul Roetzer shares why Artificial General Intelligence (AGI) may be only a few years away, why the definition of AGI itself is a moving target, and how leaders can prepare for profound disruption—sooner than they think. \n",
      "date": "2025-03-27",
      "authors": "Claire Prudhomme",
      "journal": "marketingaiinstitute.com",
      "therapyArea": "AI Marketing and Advertising",
      "term": "Marketing AI Institute",
      "summary": "The AI Show Episode 141 discusses the rapid progression towards Artificial General Intelligence (AGI) and emphasizes the need for leaders to prepare for the potential disruptions that will accompany this technological advancement. The episode highlights that the timeline for AGI might be shorter than anticipated, with the definition of AGI continuously evolving.",
      "takeaways": [
        "- The timeline for achieving AGI is accelerating, potentially occurring within a few years.",
        "- The definition of AGI is fluid and may change as technology progresses.",
        "- Leaders in various industries, including pharmaceuticals, should actively prepare for the significant changes and disruptions associated with the arrival of AGI."
      ]
    },
    {
      "id": 15,
      "title": "LLMs for Explainable AI: A Comprehensive Survey",
      "link": "https://arxiv.org/abs/2504.00125",
      "description": "arXiv:2504.00125v1 Announce Type: new \nAbstract: Large Language Models (LLMs) offer a promising approach to enhancing Explainable AI (XAI) by transforming complex machine learning outputs into easy-to-understand narratives, making model predictions more accessible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the-art neural networks and deep learning models, are often seen as \"black boxes\" due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decisions from AI models, which leads to less effective decision-making processes, reduced accountabilities, and unclear potential biases. A challenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. With the development of Large Language Models, we want to explore the possibilities of using human language-based models, LLMs, for model explainabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM-generated explanation, discusses the corresponding challenges and limitations, and examines real-world applications. Finally, we discuss future directions by emphasizing the need for more interpretable, automated, user-centric, and multidisciplinary approaches for XAI via LLMs.",
      "date": "2025-04-02",
      "authors": "Ahsan Bilal, David Ebert, Beiyu Lin",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article surveys the use of Large Language Models (LLMs) in enhancing Explainable AI (XAI), highlighting how LLMs can transform complex machine learning outputs into understandable narratives, thus improving model interpretability and user trust. It also discusses challenges, limitations, and future directions for integrating LLMs in XAI applications.",
      "takeaways": [
        "- LLMs have the potential to make opaque AI models more interpretable by generating human-readable explanations of their predictions.",
        "- The challenge of trust in AI systems can be addressed through improved explainability, which is critical for effective decision-making.",
        "- The article emphasizes the need for multidisciplinary approaches to enhance the usability and interpretability of AI models in real-world applications."
      ]
    },
    {
      "id": 16,
      "title": "Hawkeye:Efficient Reasoning with Model Collaboration",
      "link": "https://arxiv.org/abs/2504.00424",
      "description": "arXiv:2504.00424v1 Announce Type: new \nAbstract: Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to the generation of excessive intermediate reasoning tokens, which introduce semantic redundancy and overly detailed reasoning steps. Moreover, computational expense and latency are significant concerns, as the cost scales with the number of output tokens, including those intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel post-training and inference framework where a large model produces concise CoT instructions to guide a smaller model in response generation. HAWKEYE quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able to expand responses while reducing token usage and computational cost significantly. Our evaluation shows that HAWKEYE can achieve comparable response quality using only 35% of the full CoTs, while improving clarity, coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can accelerate end-to-end reasoning by up to 3.4x on complex math tasks while reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the models will be available soon.",
      "date": "2025-04-02",
      "authors": "Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, Haonan Li, Qirong Ho",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents HAWKEYE, a novel framework for improving the efficiency of Chain-of-Thought (CoT) reasoning in large language models by guiding smaller models to produce high-quality responses using fewer intermediate tokens. This approach significantly reduces computational costs while maintaining response clarity and coherence.",
      "takeaways": [
        "- HAWKEYE reduces semantic redundancy in CoT reasoning, allowing for higher quality output with fewer tokens.",
        "- It can enhance reasoning efficiency by accelerating response generation by up to 3.4 times on complex tasks.",
        "- The framework can decrease inference costs by up to 60% while improving clarity and conciseness in responses."
      ]
    },
    {
      "id": 17,
      "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?",
      "link": "https://arxiv.org/abs/2504.00509",
      "description": "arXiv:2504.00509v1 Announce Type: new \nAbstract: The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.",
      "date": "2025-04-02",
      "authors": "Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, Xiaowen Guo, Jiecao Chen",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article discusses a novel benchmark, RoR-Bench, designed to assess the performance of large language models (LLMs) on elementary school-level reasoning tasks. The authors find that leading LLMs exhibit significant recitation behavior, failing to demonstrate true reasoning by showing up to a 60% performance drop when conditions of the problems are slightly altered.",
      "takeaways": [
        "- The study introduces RoR-Bench, which highlights the limitations of current LLMs in true reasoning tasks.",
        "- Existing cutting-edge LLMs, including OpenAI-o1 and DeepSeek-R1, show severe performance drops on simple arithmetic and reasoning problems under changed conditions.",
        "- The findings challenge the perception of LLMs' intelligence and call for a reassessment of their capabilities in reasoning tasks."
      ]
    },
    {
      "id": 18,
      "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute",
      "link": "https://arxiv.org/abs/2504.00762",
      "description": "arXiv:2504.00762v1 Announce Type: new \nAbstract: This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated-sampling-then-voting framework, with a novel twist: incorporating multiple models, even weaker ones, to leverage their complementary strengths that potentially arise from diverse training data and paradigms. By using consistency as a signal, our strategy dynamically switches between models. Theoretical analysis highlights the efficiency and performance advantages of our strategy. Extensive experiments on six datasets demonstrate that our strategy not only outperforms self-consistency and state-of-the-art multi-agent debate approaches, but also significantly reduces inference costs. Additionally, ModelSwitch requires only a few comparable LLMs to achieve optimal performance and can be extended with verification methods, demonstrating the potential of leveraging multiple LLMs in the generation-verification paradigm.",
      "date": "2025-04-02",
      "authors": "Jianhao Chen, Zishuo Xun, Bocheng Zhou, Han Qi, Qiaosheng Zhang, Yang Chen, Wei Hu, Yuzhong Qu, Wanli Ouyang, Shuyue Hu",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article proposes a cost-effective method for enhancing large language model (LLM) performance by employing a repeated-sampling-then-voting framework that utilizes multiple models, including weaker ones. The approach focuses on dynamic model selection based on performance consistency, demonstrating advantages in efficiency and reduced inference costs across various datasets.",
      "takeaways": [
        "- The strategy allows for improved performance by leveraging the strengths of multiple LLMs, even those with lower individual capabilities.",
        "- The framework enhances test-time efficiency and reduces computational costs while achieving optimal results.",
        "- The findings suggest a promising direction for model adaptation and verification in AI systems by integrating diverse models into a cohesive strategy."
      ]
    },
    {
      "id": 19,
      "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
      "link": "https://arxiv.org/abs/2504.00907",
      "description": "arXiv:2504.00907v1 Announce Type: new \nAbstract: Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3\\%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.",
      "date": "2025-04-02",
      "authors": "Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article introduces a novel reinforcement learning approach to improve multimodal large language models (MLLMs) for embodied agents. The proposed method allows these agents to ask relevant clarification questions when given ambiguous instructions, effectively enhancing their task execution in real-world environments.",
      "takeaways": [
        "- The study presents the \"Ask-to-Act\" task that focuses on enhancing an agent's capability to interpret ambiguous user instructions.",
        "- The approach fine-tunes MLLMs as vision-language-action policies using online reinforcement learning, showcasing an innovative way to train agents without extensive human demonstrations.",
        "- Results indicate that the RL-finetuned MLLM significantly outperforms existing baselines, achieving greater generalization to new scenes and tasks."
      ]
    },
    {
      "id": 20,
      "title": "Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices",
      "link": "https://arxiv.org/abs/2504.00002",
      "description": "arXiv:2504.00002v1 Announce Type: cross \nAbstract: Recent advancements in large language models (LLMs) have prompted interest in deploying these models on mobile devices to enable new applications without relying on cloud connectivity. However, the efficiency constraints of deploying LLMs on resource-limited devices present significant challenges. In this paper, we conduct a comprehensive measurement study to evaluate the efficiency tradeoffs between mobile-based, edge-based, and cloud-based deployments for LLM applications. We implement AutoLife-Lite, a simplified LLM-based application that analyzes smartphone sensor data to infer user location and activity contexts. Our experiments reveal that: (1) Only small-size LLMs (<4B parameters) can run successfully on powerful mobile devices, though they exhibit quality limitations compared to larger models; (2) Model compression is effective in lower the hardware requirement, but may lead to significant performance degradation; (3) The latency to run LLMs on mobile devices with meaningful output is significant (>30 seconds), while cloud services demonstrate better time efficiency (<10 seconds); (4) Edge deployments offer intermediate tradeoffs between latency and model capabilities, with different results on CPU-based and GPU-based settings. These findings provide valuable insights for system designers on the current limitations and future directions for on-device LLM applications.",
      "date": "2025-04-02",
      "authors": "Xiao Yan, Yi Ding",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article examines the challenges of deploying large language models (LLMs) on mobile devices, highlighting the efficiency trade-offs compared to edge and cloud-based deployments. Through a study using a simplified LLM application, the paper discusses performance limitations, model compression effects, and latency issues inherent in mobile implementations.",
      "takeaways": [
        "- Only small LLMs (<4B parameters) can efficiently run on powerful mobile devices, although they perform worse than larger models.",
        "- Model compression can reduce hardware requirements but often leads to performance degradation.",
        "- Mobile deployments lead to significant latency (>30 seconds) compared to quicker cloud services (<10 seconds), with edge computing presenting a middle ground in trade-offs."
      ]
    },
    {
      "id": 21,
      "title": "Deep Learning-Based Hypoglycemia Classification Across Multiple Prediction Horizons",
      "link": "https://arxiv.org/abs/2504.00009",
      "description": "arXiv:2504.00009v1 Announce Type: cross \nAbstract: Type 1 diabetes (T1D) management can be significantly enhanced through the use of predictive machine learning (ML) algorithms, which can mitigate the risk of adverse events like hypoglycemia. Hypoglycemia, characterized by blood glucose levels below 70 mg/dL, is a life-threatening condition typically caused by excessive insulin administration, missed meals, or physical activity. Its asymptomatic nature impedes timely intervention, making ML models crucial for early detection. This study integrates short- (up to 2h) and long-term (up to 24h) prediction horizons (PHs) within a single classification model to enhance decision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h, 2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified model classifying up to 4h before hypoglycemia is compared. We trained ResNet and LSTM models on glucose levels, insulin doses, and acceleration data. The results demonstrate the superiority of the LSTM models when classifying nine classes. In particular, subject-specific models yielded better performance but achieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%, respectively. A population-based six-class model improved the results with at least 60% of events detected. In contrast, longer PHs remain challenging with the current approach and may be considered with different models.",
      "date": "2025-04-02",
      "authors": "Beyza Cinar, Jennifer Daniel Onwuchekwa, Maria Maleshkova",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This study explores the development of a deep learning model for predicting hypoglycemia in type 1 diabetes patients by integrating both short- and long-term prediction horizons within a single classification framework. By utilizing ResNet and LSTM models trained on glucose levels, insulin doses, and activity data, the research aims to enhance early detection and decision support for managing hypoglycemia.",
      "takeaways": [
        "- The research improves hypoglycemia prediction by combining multiple prediction horizons within a single model.",
        "- LSTM models outperformed ResNet in classifying hypoglycemic events, particularly when personalized for specific subjects.",
        "- A population-based model enhanced detection rates of hypoglycemia events, though longer prediction horizons remain challenging."
      ]
    },
    {
      "id": 22,
      "title": "ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding",
      "link": "https://arxiv.org/abs/2504.00019",
      "description": "arXiv:2504.00019v1 Announce Type: cross \nAbstract: Language models (LMs) have become a staple of the code-writing toolbox. Their pre-training recipe has, however, remained stagnant over recent years, barring the occasional changes in data sourcing and filtering strategies. In particular, research exploring modifications to Code-LMs' pre-training objectives, geared towards improving data efficiency and better disentangling between syntax and semantics, has been noticeably sparse, especially compared with corresponding efforts in natural language LMs. In this work, we examine grounding on obfuscated code as a means of helping Code-LMs look beyond the surface-form syntax and enhance their pre-training sample efficiency. To this end, we compile ObscuraX, a dataset of approximately 55M source and obfuscated code pairs in seven languages. Subsequently, we pre-train ObscuraCoder models, ranging in size from 255M to 2.8B parameters, on a 272B-token corpus that includes ObscuraX and demonstrate that our obfuscation-based pre-training recipe leads to consistent improvements in Code-LMs' abilities compared to both vanilla autoregressive pre-training as well as existing de-obfuscation (DOBF) objectives. ObscuraCoder demonstrates sizeable gains across multiple tests of syntactic and semantic code understanding, along with improved capabilities in multilingual code completion, multilingual code commit summarization, and multi-purpose library-oriented code generation.",
      "date": "2025-04-02",
      "authors": "Indraneil Paul, Haoyi Yang, Goran Glava\\v{s}, Kristian Kersting, Iryna Gurevych",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents ObscuraCoder, a new approach to pre-training code language models using obfuscation techniques to enhance their efficiency and understanding of syntax and semantics in programming code. By compiling a dataset of obfuscated code, the authors report improved performance in various code-related tasks compared to traditional pre-training methods.",
      "takeaways": [
        "- ObscuraCoder leverages obfuscation to enhance the pre-training objectives of code language models, addressing the stagnation in their training methodologies.",
        "- The newly created dataset, ObscuraX, consists of 55 million source and obfuscated code pairs, which is instrumental in training the models.",
        "- The results indicate significant improvements in multilingual code completion and understanding, demonstrating the potential of obfuscation in advancing AI applications in programming tasks."
      ]
    },
    {
      "id": 23,
      "title": "Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation",
      "link": "https://arxiv.org/abs/2504.00020",
      "description": "arXiv:2504.00020v1 Announce Type: cross \nAbstract: Recent breakthroughs in single-cell technology have ushered in unparalleled opportunities to decode the molecular intricacy of intricate biological systems, especially those linked to diseases unique to humans. However, these progressions have also ushered in novel obstacles-specifically, the efficient annotation of extensive, long-tailed single-cell data pertaining to disease conditions. To effectively surmount this challenge, we introduce Celler, a state-of-the-art generative pre-training model crafted specifically for the annotation of single-cell data. Celler incorporates two groundbreaking elements: First, we introduced the Gaussian Inflation (GInf) Loss function. By dynamically adjusting sample weights, GInf Loss significantly enhances the model's ability to learn from rare categories while reducing the risk of overfitting for common categories. Secondly, we introduce an innovative Hard Data Mining (HDM) strategy into the training process, specifically targeting the challenging-to-learn minority data samples, which significantly improved the model's predictive accuracy. Additionally, to further advance research in this field, we have constructed a large-scale single-cell dataset: Celler-75, which encompasses 40 million cells distributed across 80 human tissues and 75 specific diseases. This dataset provides critical support for comprehensively exploring the potential of single-cell technology in disease research. Our code is available at https://github.com/AI4science-ym/HiCeller.",
      "date": "2025-04-02",
      "authors": "Huan Zhao, Yiming Liu, Jina Yao, Ling Xiong, Zexin Zhou, Zixing Zhang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces Celler, a generative pre-training model designed for the annotation of long-tailed single-cell data related to diseases. It features innovative strategies such as the Gaussian Inflation Loss function and Hard Data Mining to improve model accuracy and tackle challenges associated with rare and common data categories.",
      "takeaways": [
        "- Celler enhances single-cell annotation through novel techniques tailored for handling extensive biological data.",
        "- The incorporation of the Gaussian Inflation Loss function allows for better learning from rare disease categories while combating overfitting.",
        "- The creation of the Celler-75 dataset, encompassing 40 million cells and various tissues and diseases, offers significant resources for ongoing research in single-cell technology and its applications in disease understanding."
      ]
    },
    {
      "id": 24,
      "title": "Diffusion models applied to skin and oral cancer classification",
      "link": "https://arxiv.org/abs/2504.00026",
      "description": "arXiv:2504.00026v1 Announce Type: cross \nAbstract: This study investigates the application of diffusion models in medical image classification (DiffMIC), focusing on skin and oral lesions. Utilizing the datasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the diffusion model demonstrated competitive performance compared to state-of-the-art deep learning models like Convolutional Neural Networks (CNNs) and Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved a balanced accuracy of 0.6457 for six-class classification and 0.8357 for binary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it attained a balanced accuracy of 0.9050. These results suggest that diffusion models are viable models for classifying medical images of skin and oral lesions. In addition, we investigate the robustness of the model trained on PAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA dataset.",
      "date": "2025-04-02",
      "authors": "Jos\\'e J. M. Uliana, Renato A. Krohling",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This study explores the use of diffusion models for the classification of skin and oral lesions, comparing their performance against traditional deep learning models. Results indicate that diffusion models can effectively classify medical images, achieving notable accuracy on the tested datasets.",
      "takeaways": [
        "- Diffusion models show competitive performance in medical image classification, particularly for skin and oral cancers.",
        "- The model achieved high balanced accuracy scores, demonstrating its potential as a viable alternative to CNNs and Transformers.",
        "- The research emphasizes the adaptability of the diffusion model for different datasets and its robustness when tested on clinical images."
      ]
    },
    {
      "id": 25,
      "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit",
      "link": "https://arxiv.org/abs/2504.00027",
      "description": "arXiv:2504.00027v1 Announce Type: cross \nAbstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).",
      "date": "2025-04-02",
      "authors": "Muhammad Ahmad, Humaira Farid, Iqra Ameer, Muhammad Muzamil, Ameer Hamza Muhammad Jalal, Ildar Batyrshin, Grigori Sidorov",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents a novel system, ONER-2025, that utilizes Natural Language Processing (NLP) to perform Named Entity Recognition specifically aimed at extracting information related to opioid use from Reddit. The research includes the creation of a unique annotated dataset, an analysis of linguistic challenges in opioid discussions, and a real-time monitoring system to identify overdose events, achieving high accuracy with modern transformer-based models.",
      "takeaways": [
        "- The ONER-2025 system offers innovative insights into opioid use by analyzing unstructured data from social media, potentially improving public health responses.",
        "- The study highlights the challenges of processing casual language and slang in social media contexts, which is crucial for accurate data interpretation.",
        "- The high performance of the transformer-based models (97% accuracy) illustrates the effectiveness of advanced AI techniques in understanding complex social media dialogue related to public health issues."
      ]
    },
    {
      "id": 26,
      "title": "Generating Structured Plan Representation of Procedures with LLMs",
      "link": "https://arxiv.org/abs/2504.00029",
      "description": "arXiv:2504.00029v1 Announce Type: cross \nAbstract: In this paper, we address the challenges of managing Standard Operating Procedures (SOPs), which often suffer from inconsistencies in language, format, and execution, leading to operational inefficiencies. Traditional process modeling demands significant manual effort, domain expertise, and familiarity with complex languages like Business Process Modeling Notation (BPMN), creating barriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a novel approach that leverages Large Language Models (LLMs) to transform SOPs into decision-tree-based structured representations. SOPStruct produces a standardized representation of SOPs across different domains, reduces cognitive load, and improves user comprehension by effectively capturing task dependencies and ensuring sequential integrity. Our approach enables leveraging the structured information to automate workflows as well as empower the human users. By organizing procedures into logical graphs, SOPStruct facilitates backtracking and error correction, offering a scalable solution for process optimization. We employ a novel evaluation framework, combining deterministic methods with the Planning Domain Definition Language (PDDL) to verify graph soundness, and non-deterministic assessment by an LLM to ensure completeness. We empirically validate the robustness of our LLM-based structured SOP representation methodology across SOPs from different domains and varying levels of complexity. Despite the current lack of automation readiness in many organizations, our research highlights the transformative potential of LLMs to streamline process modeling, paving the way for future advancements in automated procedure optimization.",
      "date": "2025-04-02",
      "authors": "Deepeka Garg, Sihan Zeng, Sumitra Ganesh, Leo Ardon",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This paper introduces SOP Structuring (SOPStruct), a new methodology that utilizes Large Language Models (LLMs) to convert Standard Operating Procedures (SOPs) into structured representations, thereby addressing inconsistencies and inefficiencies in process management. The approach aims to simplify the modeling process for non-technical users and enhance procedure optimization through standardized decision-tree representations.",
      "takeaways": [
        "- SOPStruct enhances the understanding and usability of SOPs by transforming them into decision-tree-based representations.",
        "- The methodology employs LLMs to streamline process modeling, making it accessible to users without technical expertise.",
        "- The research demonstrates the potential of LLMs in automating workflows and improving operational efficiency within various domains."
      ]
    },
    {
      "id": 27,
      "title": "Token-Driven GammaTune: Adaptive Calibration for Enchanced Speculative Decoding",
      "link": "https://arxiv.org/abs/2504.00030",
      "description": "arXiv:2504.00030v1 Announce Type: cross \nAbstract: Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%) with \\textit{GammaTune+}, while reducing performance variance. This makes \\textit{GammaTune} a robust and efficient solution for real-world deployment.",
      "date": "2025-04-02",
      "authors": "Aayush Gautam, Susav Shrestha, Narasimha Annapareddy",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents GammaTune and GammaTune+, which are adaptive algorithms for optimizing speculative decoding in large language models (LLMs). By adjusting the speculation length based on token acceptance rates, these methods significantly improve inference speed and reduce performance variance during model deployment.",
      "takeaways": [
        "- GammaTune and GammaTune+ are proposed as training-free adaptive algorithms for speculative decoding in LLMs.",
        "- The algorithms achieve an average speedup of 15% and 16% respectively compared to traditional methods.",
        "- These techniques enhance the efficiency of LLMs for real-world applications by minimizing wasted computation and reducing performance variance."
      ]
    },
    {
      "id": 28,
      "title": "MiZero: The Shadowy Defender Against Text Style Infringements",
      "link": "https://arxiv.org/abs/2504.00035",
      "description": "arXiv:2504.00035v1 Announce Type: cross \nAbstract: In-Context Learning (ICL) and efficient fine-tuning methods significantly enhanced the efficiency of applying Large Language Models (LLMs) to downstream tasks. However, they also raise concerns about the imitation and infringement of personal creative data. Current methods for data copyright protection primarily focuses on content security but lacks effectiveness in protecting the copyrights of text styles. In this paper, we introduce a novel implicit zero-watermarking scheme, namely MiZero. This scheme establishes a precise watermark domain to protect the copyrighted style, surpassing traditional watermarking methods that distort the style characteristics. Specifically, we employ LLMs to extract condensed-lists utilizing the designed instance delimitation mechanism. These lists guide MiZero in generating the watermark. Extensive experiments demonstrate that MiZero effectively verifies text style copyright ownership against AI imitation.",
      "date": "2025-04-02",
      "authors": "Ziwei Zhang, Juan Wen, Wanli Peng, Zhengxian Wu, Yinghan Zhou, Yiming Xue",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents MiZero, a novel implicit zero-watermarking scheme designed to protect the copyrights of text styles in creative content, addressing concerns related to imitation and infringement when using Large Language Models (LLMs). It highlights the mechanism by which MiZero generates robust watermarks that do not distort style characteristics, validating copyright ownership against AI imitations.",
      "takeaways": [
        "- MiZero introduces a new approach to watermarking that specifically targets the protection of text styles, which current methods fail to adequately secure.",
        "- The method leverages LLMs to create concise lists that help in generating effective watermarks while maintaining the integrity of the original style.",
        "- Extensive experiments validate the efficacy of MiZero in proving copyright ownership of text styles against unauthorized imitation by AI systems."
      ]
    },
    {
      "id": 29,
      "title": "Improving Diseases Predictions Utilizing External Bio-Banks",
      "link": "https://arxiv.org/abs/2504.00036",
      "description": "arXiv:2504.00036v1 Announce Type: cross \nAbstract: Machine learning has been successfully used in critical domains, such as medicine. However, extracting meaningful insights from biomedical data is often constrained by the lack of their available disease labels. In this research, we demonstrate how machine learning can be leveraged to enhance explainability and uncover biologically meaningful associations, even when predictive improvements in disease modeling are limited. We train LightGBM models from scratch on our dataset (10K) to impute metabolomics features and apply them to the UK Biobank (UKBB) for downstream analysis. The imputed metabolomics features are then used in survival analysis to assess their impact on disease-related risk factors. As a result, our approach successfully identified biologically relevant connections that were not previously known to the predictive models. Additionally, we applied a genome-wide association study (GWAS) on key metabolomics features, revealing a link between vascular dementia and smoking. Although being a well-established epidemiological relationship, this link was not embedded in the model's training data, which validated the method's ability to extract meaningful signals. Furthermore, by integrating survival models as inputs in the 10K data, we uncovered associations between metabolic substances and obesity, demonstrating the ability to infer disease risk for future patients without requiring direct outcome labels. These findings highlight the potential of leveraging external bio-banks to extract valuable biomedical insights, even in data-limited scenarios. Our results demonstrate that machine learning models trained on smaller datasets can still be used to uncover real biological associations when carefully integrated with survival analysis and genetic studies.",
      "date": "2025-04-02",
      "authors": "Hido Pinto, Eran Segal",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article discusses how machine learning can enhance disease prediction models by leveraging external bio-banks, focusing on metabolomics feature imputation and survival analysis. The study reveals biologically meaningful associations, including links between vascular dementia and smoking, and demonstrates the ability to infer disease risk using smaller datasets.",
      "takeaways": [
        "- Machine learning can extract valuable insights from biomedical data even when disease labels are scarce.",
        "- By integrating metabolomics features and survival analysis, researchers can uncover significant biological associations.",
        "- The study demonstrates a successful application of machine learning in predicting disease risk without direct outcome labels, highlighting its potential in data-limited scenarios."
      ]
    },
    {
      "id": 30,
      "title": "Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs",
      "link": "https://arxiv.org/abs/2504.00048",
      "description": "arXiv:2504.00048v1 Announce Type: cross \nAbstract: The growing adoption of large language models (LLMs) in business applications has amplified interest in Natural Language to SQL (NL2SQL) solutions, in which there is competing demand for high performance and efficiency. Domain- and customer-specific requirements further complicate the problem. To address this conundrum, we introduce Distill-C, a distilled customization framework tailored for NL2SQL tasks. Distill-C utilizes large teacher LLMs to produce high-quality synthetic data through a robust and scalable pipeline. Finetuning smaller and open-source LLMs on this synthesized data enables them to rival or outperform teacher models an order of magnitude larger. Evaluated on multiple challenging benchmarks, Distill-C achieves an average improvement of 36% in execution accuracy compared to the base models from three distinct LLM families. Additionally, on three internal customer benchmarks, Distill-C demonstrates a 22.6% performance improvement over the base models. Our results demonstrate that Distill-C is an effective, high-performing and generalizable approach for deploying lightweight yet powerful NL2SQL models, delivering exceptional accuracies while maintaining low computational cost.",
      "date": "2025-04-02",
      "authors": "Cong Duy Vu Hoang, Gioacchino Tangari, Clemence Lanfranchi, Dalu Guo, Paul Cayet, Steve Siu, Don Dharmasiri, Yuan-Fang Li, Long Duong, Damien Hilloulin, Rhicheek Patra, Sungpack Hong, Hassan Chafi",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents Distill-C, a novel distilled customization framework aimed at enhancing Natural Language to SQL (NL2SQL) capabilities using large language models (LLMs). It demonstrates that by leveraging synthesized data from larger teacher LLMs, smaller LLMs can achieve significant performance improvements in NL2SQL tasks.",
      "takeaways": [
        "- Distill-C offers a high-performing framework for improving NL2SQL models by utilizing data generated from large teacher LLMs.",
        "- The approach allows smaller, open-source LLMs to rival larger models in execution accuracy, achieving an average improvement of 36% on benchmark tasks.",
        "- The framework exhibits strong performance on internal customer benchmarks, with a 22.6% increase in accuracy, making it a cost-effective solution for deploying NLP applications in business settings."
      ]
    },
    {
      "id": 31,
      "title": "JudgeLRM: Large Reasoning Models as a Judge",
      "link": "https://arxiv.org/abs/2504.00050",
      "description": "arXiv:2504.00050v1 Announce Type: cross \nAbstract: The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.",
      "date": "2025-04-02",
      "authors": "Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The paper introduces JudgeLRM, a series of large reasoning models (LRMs) designed to evaluate complex reasoning tasks, and compares their performance to traditional supervised fine-tuning (SFT) and other reasoning models. The findings suggest that JudgeLRM models, particularly in their larger configurations, show significant improvements in performance, especially in tasks demanding intricate reasoning.",
      "takeaways": [
        "- JudgeLRM models leverage reinforcement learning (RL) with specific judge-wise rewards to enhance evaluation capabilities.",
        "- The study found a negative correlation between SFT performance gains and reasoning demands, highlighting the challenges of using traditional methods for complex reasoning tasks.",
        "- JudgeLRM-3B and JudgeLRM-7B models outperform leading models such as GPT-4 and DeepSeek-R1 in evaluation tasks requiring deep reasoning, showcasing advancements in AI for evaluative purposes."
      ]
    },
    {
      "id": 32,
      "title": "Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records",
      "link": "https://arxiv.org/abs/2504.00053",
      "description": "arXiv:2504.00053v1 Announce Type: cross \nAbstract: Objective: Electronic health records (EHR) are widely available to complement administrative data-based disease surveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive and requires extensive manual labelling of disease outcomes. This study developed an efficient strategy based on advanced large language models to identify multiple conditions from EHR clinical notes. Methods: We linked a cardiac registry cohort in 2015 with an EHR system in Alberta, Canada. We developed a pipeline that leveraged a generative large language model (LLM) to analyze, understand, and interpret EHR notes by prompts based on specific diagnosis, treatment management, and clinical guidelines. The pipeline was applied to detect acute myocardial infarction (AMI), diabetes, and hypertension. The performance was compared against clinician-validated diagnoses as the reference standard and widely adopted International Classification of Diseases (ICD) codes-based methods. Results: The study cohort accounted for 3,088 patients and 551,095 clinical notes. The prevalence was 55.4%, 27.7%, 65.9% and for AMI, diabetes, and hypertension, respectively. The performance of the LLM-based pipeline for detecting conditions varied: AMI had 88% sensitivity, 63% specificity, and 77% positive predictive value (PPV); diabetes had 91% sensitivity, 86% specificity, and 71% PPV; and hypertension had 94% sensitivity, 32% specificity, and 72% PPV. Compared with ICD codes, the LLM-based method demonstrated improved sensitivity and negative predictive value across all conditions. The monthly percentage trends from the detected cases by LLM and reference standard showed consistent patterns.",
      "date": "2025-04-02",
      "authors": "Jie Pan, Seungwon Lee, Cheligeer Cheligeer, Elliot A. Martin, Kiarash Riazi, Hude Quan, Na Li",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This study presents a novel approach that integrates large language models (LLMs) with human expertise to effectively identify diseases like acute myocardial infarction, diabetes, and hypertension from electronic health records. By comparing the LLM's performance with clinician-validated diagnoses, the results indicate that leveraging LLMs can enhance disease detection in healthcare settings.",
      "takeaways": [
        "- The study developed an LLM-based pipeline that significantly improves the detection of diseases from EHR notes compared to traditional ICD codes.",
        "- The LLM demonstrated high sensitivity in detecting conditions, particularly achieving 91% sensitivity for diabetes and 94% for hypertension.",
        "- LLM methods showed consistent monthly trends in detected cases, indicating reliability and potential for real-world applications in disease surveillance."
      ]
    },
    {
      "id": 33,
      "title": "CF-CAM: Gradient Perturbation Mitigation and Feature Stabilization for Reliable Interpretability",
      "link": "https://arxiv.org/abs/2504.00060",
      "description": "arXiv:2504.00060v1 Announce Type: cross \nAbstract: As deep learning continues to advance, the opacity of neural network decision-making remains a critical challenge, limiting trust and applicability in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged as a key approach to visualizing model decisions, yet existing methods face inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to gradient perturbations, leading to unstable and unreliable explanations. Conversely, gradient-free approaches mitigate gradient instability but incur significant computational overhead and inference latency. To address these limitations, we propose Cluster Filter Class Activation Map (CF-CAM), a novel framework that reintroduces gradient-based weighting while enhancing robustness against gradient noise. CF-CAM employs a hierarchical importance weighting strategy to balance discriminative feature preservation and noise elimination. A density-aware channel clustering via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups semantically relevant feature channels and discard noise-prone activations. Additionally, cluster-conditioned gradient filtering leverages bilateral filters to refine gradient signals, preserving edge-aware localization while suppressing noise impact. Experiment results demonstrate that CF-CAM achieves superior interpretability performance while maintaining resilience to gradient perturbations, outperforming state-of-the-art CAM methods in faithfulness and robustness. By effectively mitigating gradient instability without excessive computational cost, CF-CAM provides a reliable solution for enhancing the interpretability of deep neural networks in critical applications such as medical diagnosis and autonomous driving.",
      "date": "2025-04-02",
      "authors": "Hongjie He, Xu Pan, Yudong Yao",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents CF-CAM, a new framework designed to improve the reliability of interpretability in deep learning models by balancing the trade-offs of gradient-based and gradient-free Class Activation Mapping techniques. By employing a novel clustering approach and robust gradient filtering, CF-CAM enhances the interpretability of neural networks in high-stakes applications like medical diagnosis while reducing computational costs.",
      "takeaways": [
        "- CF-CAM introduces a new hierarchical importance weighting strategy that improves the stability of model interpretability against gradient perturbations.",
        "- The use of a density-aware clustering method allows for effective separation of relevant features from noise in model activations.",
        "- CF-CAM demonstrates superior performance over existing CAM methods in terms of robustness and interpretability, particularly beneficial for critical applications in healthcare and autonomous systems."
      ]
    },
    {
      "id": 34,
      "title": "Evaluating the Feasibility and Accuracy of Large Language Models for Medical History-Taking in Obstetrics and Gynecology",
      "link": "https://arxiv.org/abs/2504.00061",
      "description": "arXiv:2504.00061v1 Announce Type: cross \nAbstract: Effective physician-patient communications in pre-diagnostic environments, and most specifically in complex and sensitive medical areas such as infertility, are critical but consume a lot of time and, therefore, cause clinic workflows to become inefficient. Recent advancements in Large Language Models (LLMs) offer a potential solution for automating conversational medical history-taking and improving diagnostic accuracy. This study evaluates the feasibility and performance of LLMs in those tasks for infertility cases. An AI-driven conversational system was developed to simulate physician-patient interactions with ChatGPT-4o and ChatGPT-4o-mini. A total of 70 real-world infertility cases were processed, generating 420 diagnostic histories. Model performance was assessed using F1 score, Differential Diagnosis (DDs) Accuracy, and Accuracy of Infertility Type Judgment (ITJ). ChatGPT-4o-mini outperformed ChatGPT-4o in information extraction accuracy (F1 score: 0.9258 vs. 0.9029, p = 0.045, d = 0.244) and demonstrated higher completeness in medical history-taking (97.58% vs. 77.11%), suggesting that ChatGPT-4o-mini is more effective in extracting detailed patient information, which is critical for improving diagnostic accuracy. In contrast, ChatGPT-4o performed slightly better in differential diagnosis accuracy (2.0524 vs. 2.0048, p > 0.05). ITJ accuracy was higher in ChatGPT-4o-mini (0.6476 vs. 0.5905) but with lower consistency (Cronbach's $\\alpha$ = 0.562), suggesting variability in classification reliability. Both models demonstrated strong feasibility in automating infertility history-taking, with ChatGPT-4o-mini excelling in completeness and extraction accuracy. In future studies, expert validation for accuracy and dependability in a clinical setting, AI model fine-tuning, and larger datasets with a mix of cases of infertility have to be prioritized.",
      "date": "2025-04-02",
      "authors": "Dou Liu, Ying Long, Sophia Zuoqiu, Tian Tang, Rong Yin",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This study evaluates the use of Large Language Models (LLMs) for automating medical history-taking in obstetrics and gynecology, particularly in infertility cases. It compares the performance of two models, ChatGPT-4o and ChatGPT-4o-mini, finding that the latter is more effective in extracting detailed patient information and improving diagnostic accuracy.",
      "takeaways": [
        "- ChatGPT-4o-mini outperformed ChatGPT-4o in information extraction accuracy and completeness of medical history-taking.",
        "- Both models showed strong feasibility for automating infertility history-taking, with potential benefits for clinic workflow efficiency.",
        "- Future research should focus on expert validation, model fine-tuning, and expanding the dataset to improve reliability and applicability in clinical settings."
      ]
    },
    {
      "id": 35,
      "title": "Lorentzian Graph Isomorphic Network",
      "link": "https://arxiv.org/abs/2504.00142",
      "description": "arXiv:2504.00142v1 Announce Type: cross \nAbstract: We introduce the Lorentzian Graph Isomorphic Network (LGIN), a novel graph neural network (GNN) designed to operate in hyperbolic spaces, leveraging the Lorentzian model to enhance graph representation learning. Existing GNNs primarily operate in Euclidean spaces, which can limit their ability to capture hierarchical and multi-relational structures inherent to complex graphs. LGIN addresses this by incorporating curvature-aware aggregation functions that preserve the Lorentzian metric tensor, ensuring embeddings remain constrained within the hyperbolic space by proposing a new update rule that effectively captures both local neighborhood interactions and global structural properties, enabling LGIN to distinguish non-isomorphic graphs with expressiveness at least as powerful as the Weisfeiler-Lehman test. Through extensive evaluation across nine benchmark datasets, including molecular and protein structures, LGIN consistently outperforms or matches state-of-the-art GNNs, demonstrating its robustness and efficacy in modeling complex graph structures. To the best of our knowledge, this is the first study to extend the concept of a powerful graph neural network to Riemannian manifolds, paving the way for future advancements in hyperbolic graph learning. The code for our paper can be found at https://github.com/Deceptrax123/LGIN.",
      "date": "2025-04-02",
      "authors": "Srinitish Srinivasan, Omkumar CU",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces the Lorentzian Graph Isomorphic Network (LGIN), a new graph neural network that operates in hyperbolic spaces to enhance graph representation learning, particularly for complex graph structures such as molecular and protein data. LGIN achieves improved performance over traditional GNNs by incorporating curvature-aware aggregation functions and a novel update rule.",
      "takeaways": [
        "- LGIN is designed for hyperbolic spaces, which allows it to better capture hierarchical and multi-relational structures in complex graphs.",
        "- The network's performance is evaluated across nine benchmark datasets, showing consistent superiority or parity with state-of-the-art graph neural networks.",
        "- This research represents a significant advancement in graph neural network techniques by extending functionality to Riemannian manifolds, providing a foundation for further developments in hyperbolic graph learning."
      ]
    },
    {
      "id": 36,
      "title": "Does \"Reasoning\" with Large Language Models Improve Recognizing, Generating, and Reframing Unhelpful Thoughts?",
      "link": "https://arxiv.org/abs/2504.00163",
      "description": "arXiv:2504.00163v1 Announce Type: cross \nAbstract: Cognitive Reframing, a core element of Cognitive Behavioral Therapy (CBT), helps individuals reinterpret negative experiences by finding positive meaning. Recent advances in Large Language Models (LLMs) have demonstrated improved performance through reasoning-based strategies. This inspires a promising direction of leveraging the reasoning capabilities of LLMs to improve CBT and mental reframing by simulating the process of critical thinking, potentially enabling more effective recognition, generation, and reframing of cognitive distortions. In this work, we investigate the role of various reasoning methods, including pre-trained reasoning LLMs and augmented reasoning strategies such as CoT and self-consistency in enhancing LLMs' ability to perform cognitive reframing tasks. We find that augmented reasoning methods, even when applied to \"outdated\" LLMs like GPT-3.5, consistently outperform state-of-the-art pretrained reasoning models on recognizing, generating and reframing unhelpful thoughts.",
      "date": "2025-04-02",
      "authors": "Yilin Qi, Dong Won Lee, Cynthia Breazeal, Hae Won Park",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores how reasoning techniques in Large Language Models (LLMs) can improve Cognitive Behavioral Therapy (CBT) by enhancing the recognition, generation, and reframing of unhelpful thoughts. The study demonstrates that augmented reasoning methods can significantly boost the performance of LLMs, even those that are considered outdated.",
      "takeaways": [
        "- The study investigates the application of reasoning methods in LLMs for enhancing mental health interventions like CBT.",
        "- Augmented reasoning strategies, including Chain of Thought (CoT) and self-consistency, improve LLMs' performance in cognitive reframing tasks.",
        "- The findings suggest that even older models like GPT-3.5 can achieve state-of-the-art performance when augmented with effective reasoning techniques."
      ]
    },
    {
      "id": 37,
      "title": "Backdoor Detection through Replicated Execution of Outsourced Training",
      "link": "https://arxiv.org/abs/2504.00170",
      "description": "arXiv:2504.00170v1 Announce Type: cross \nAbstract: It is common practice to outsource the training of machine learning models to cloud providers. Clients who do so gain from the cloud's economies of scale, but implicitly assume trust: the server should not deviate from the client's training procedure. A malicious server may, for instance, seek to insert backdoors in the model. Detecting a backdoored model without prior knowledge of both the backdoor attack and its accompanying trigger remains a challenging problem. In this paper, we show that a client with access to multiple cloud providers can replicate a subset of training steps across multiple servers to detect deviation from the training procedure in a similar manner to differential testing. Assuming some cloud-provided servers are benign, we identify malicious servers by the substantial difference between model updates required for backdooring and those resulting from clean training. Perhaps the strongest advantage of our approach is its suitability to clients that have limited-to-no local compute capability to perform training; we leverage the existence of multiple cloud providers to identify malicious updates without expensive human labeling or heavy computation. We demonstrate the capabilities of our approach on an outsourced supervised learning task where $50\\%$ of the cloud providers insert their own backdoor; our approach is able to correctly identify $99.6\\%$ of them. In essence, our approach is successful because it replaces the signature-based paradigm taken by existing approaches with an anomaly-based detection paradigm. Furthermore, our approach is robust to several attacks from adaptive adversaries utilizing knowledge of our detection scheme.",
      "date": "2025-04-02",
      "authors": "Hengrui Jia, Sierra Wyllie, Akram Bin Sediq, Ahmed Ibrahim, Nicolas Papernot",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents a novel approach for detecting backdoor attacks in outsourced machine learning model training by using replicated execution across multiple cloud providers. The method identifies malicious updates by analyzing discrepancies in model updates, demonstrating a high detection accuracy without requiring significant local computation or human labeling.",
      "takeaways": [
        "- The proposed method leverages the use of multiple cloud providers to enhance detection of backdoored models.",
        "- It introduces an anomaly-based detection paradigm, contrasting with traditional signature-based methods.",
        "- The approach achieved a detection accuracy of 99.6% in identifying malicious servers during an outsourced supervised learning task."
      ]
    },
    {
      "id": 38,
      "title": "Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier",
      "link": "https://arxiv.org/abs/2504.00178",
      "description": "arXiv:2504.00178v1 Announce Type: cross \nAbstract: Pre-tokenization, the initial step in many modern tokenization pipelines, segments text into smaller units called pretokens, typically splitting on whitespace and punctuation. While this process encourages having full, individual words as tokens, it introduces a fundamental limitation in most tokenization algorithms such as Byte Pair Encoding (BPE). Specifically, pre-tokenization causes the distribution of tokens in a corpus to heavily skew towards common, full-length words. This skewed distribution limits the benefits of expanding to larger vocabularies, since the additional tokens appear with progressively lower counts. To overcome this barrier, we propose BoundlessBPE, a modified BPE algorithm that relaxes the pretoken boundary constraint. Our approach selectively merges two complete pretokens into a larger unit we term a superword. Superwords are not necessarily semantically cohesive. For example, the pretokens \" of\" and \" the\" might be combined to form the superword \" of the\". This merging strategy results in a substantially more uniform distribution of tokens across a corpus than standard BPE, and compresses text more effectively, with an approximate 20% increase in bytes per token.",
      "date": "2025-04-02",
      "authors": "Craig W. Schmidt, Varshini Reddy, Chris Tanner, Yuval Pinter",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents BoundlessBPE, a novel modification of the Byte Pair Encoding algorithm that overcomes the limitations of pre-tokenization by merging pretokens into larger units called superwords. This technique achieves a more uniform distribution of tokens and enhances text compression by approximately 20%.",
      "takeaways": [
        "- BoundlessBPE addresses the pre-tokenization barrier in traditional tokenization pipelines.",
        "- The introduction of superwords allows for greater vocabulary expansion and reduces the skewness of token distribution.",
        "- The improved method offers notable benefits in text compression efficiency, increasing bytes per token significantly."
      ]
    },
    {
      "id": 39,
      "title": "MultiMorph: On-demand Atlas Construction",
      "link": "https://arxiv.org/abs/2504.00247",
      "description": "arXiv:2504.00247v1 Announce Type: cross \nAbstract: We present MultiMorph, a fast and efficient method for constructing anatomical atlases on the fly. Atlases capture the canonical structure of a collection of images and are essential for quantifying anatomical variability across populations. However, current atlas construction methods often require days to weeks of computation, thereby discouraging rapid experimentation. As a result, many scientific studies rely on suboptimal, precomputed atlases from mismatched populations, negatively impacting downstream analyses. MultiMorph addresses these challenges with a feedforward model that rapidly produces high-quality, population-specific atlases in a single forward pass for any 3D brain dataset, without any fine-tuning or optimization. MultiMorph is based on a linear group-interaction layer that aggregates and shares features within the group of input images. Further, by leveraging auxiliary synthetic data, MultiMorph generalizes to new imaging modalities and population groups at test-time. Experimentally, MultiMorph outperforms state-of-the-art optimization-based and learning-based atlas construction methods in both small and large population settings, with a 100-fold reduction in time. This makes MultiMorph an accessible framework for biomedical researchers without machine learning expertise, enabling rapid, high-quality atlas generation for diverse studies.",
      "date": "2025-04-02",
      "authors": "S. Mazdak Abulnaga, Andrew Hoopes, Neel Dey, Malte Hoffmann, Marianne Rakic, Bruce Fischl, John Guttag, Adrian Dalca",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents MultiMorph, a novel method for constructing anatomical atlases rapidly and efficiently from 3D brain datasets. This approach allows researchers to generate high-quality, population-specific atlases in real-time, significantly reducing computation time compared to existing methods.",
      "takeaways": [
        "- MultiMorph uses a feedforward model that allows for atlas construction in a single forward pass, optimizing both speed and quality.",
        "- The method achieves a 100-fold reduction in computation time, making it accessible for researchers without extensive machine learning expertise.",
        "- By incorporating auxiliary synthetic data, MultiMorph can adapt to different imaging modalities and population groups, enhancing its versatility in biomedical research."
      ]
    },
    {
      "id": 40,
      "title": "ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning",
      "link": "https://arxiv.org/abs/2504.00254",
      "description": "arXiv:2504.00254v1 Announce Type: cross \nAbstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique for fine-tuning large-scale pre-trained models with minimal parameter updates. However, existing methods rely on fixed ranks or focus solely on either rank pruning or expansion, failing to adapt ranks dynamically to match the importance of different layers during training. In this work, we propose ElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and expands ranks based on gradient-derived importance scores. To the best of our knowledge, ElaLoRA is the first method that enables both rank pruning and expansion during fine-tuning. Experiments across multiple benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods across different parameter budgets. Furthermore, our studies validate that layers receiving higher rank allocations contribute more significantly to model performance, providing theoretical justification for our adaptive strategy. By introducing a principled and adaptive rank allocation mechanism, ElaLoRA offers a scalable and efficient fine-tuning solution, particularly suited for resource-constrained environments.",
      "date": "2025-04-02",
      "authors": "Huandong Chang, Zicheng Ma, Mingyuan Ma, Zhenting Qi, Andrew Sabot, Hong Jiang, H. T. Kung",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces ElaLoRA, a novel adaptive low-rank adaptation framework that improves the fine-tuning of large-scale pre-trained models by dynamically adjusting ranks based on the importance of different layers. This method shows superior performance compared to existing parameter-efficient fine-tuning techniques.",
      "takeaways": [
        "- ElaLoRA is the first framework to allow for both rank pruning and expansion during model fine-tuning.",
        "- The method utilizes gradient-derived importance scores to dynamically adjust rank allocations, leading to better model performance.",
        "- The results demonstrate that higher rank allocations are correlated with significant contributions to overall model effectiveness, making ElaLoRA a valuable tool for efficient model training in resource-constrained settings."
      ]
    },
    {
      "id": 41,
      "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
      "link": "https://arxiv.org/abs/2504.00255",
      "description": "arXiv:2504.00255v1 Announce Type: cross \nAbstract: This study evaluates large language models (LLMs) in generating code from algorithm descriptions from recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implement solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs and Reasoning LLMs as foundational models. The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We will open-source our benchmark, and code at https://github.com/xyzCS/SciReplicate-Bench.",
      "date": "2025-04-02",
      "authors": "Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents SciReplicate-Bench, a benchmark designed to evaluate large language models (LLMs) in their ability to generate code from algorithm descriptions found in recent NLP research papers. The work introduces a multi-agent framework called Sci-Reproducer, which utilizes a Paper Agent for understanding algorithmic concepts and a Code Agent for implementing solutions, while also analyzing challenges in algorithm reproduction.",
      "takeaways": [
        "- SciReplicate-Bench consists of 100 tasks derived from 36 NLP papers, aiming to assess the capabilities of LLMs in coding and algorithm comprehension.",
        "- The introduced multi-agent framework, Sci-Reproducer, facilitates the interpretation of algorithmic concepts and address coding dependencies.",
        "- The evaluation highlights significant barriers such as missing or inconsistent algorithm descriptions that hinder successful code reproduction, with the best-performing LLM achieving only 39% execution accuracy."
      ]
    },
    {
      "id": 42,
      "title": "Digital Twins in Biopharmaceutical Manufacturing: Review and Perspective on Human-Machine Collaborative Intelligence",
      "link": "https://arxiv.org/abs/2504.00286",
      "description": "arXiv:2504.00286v1 Announce Type: cross \nAbstract: The biopharmaceutical industry is increasingly developing digital twins to digitalize and automate the manufacturing process in response to the growing market demands. However, this shift presents significant challenges for human operators, as the complexity and volume of information can overwhelm their ability to manage the process effectively. These issues are compounded when digital twins are designed without considering interaction and collaboration with operators, who are responsible for monitoring processes and assessing situations, particularly during abnormalities. Our review of current trends in biopharma digital twin development reveals a predominant focus on technology and often overlooks the critical role of human operators. To bridge this gap, this article proposes a collaborative intelligence framework that emphasizes the integration of operators with digital twins. Approaches to system design that can enhance operator trust and human-machine interface usability are presented. Moreover, innovative training programs for preparing operators to understand and utilize digital twins are discussed. The framework outlined in this article aims to enhance collaboration between operators and digital twins effectively by using their full capabilities to boost resilience and productivity in biopharmaceutical manufacturing.",
      "date": "2025-04-02",
      "authors": "Mohammed Aatif Shahab, Francesco Destro, Richard D. Braatz",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses the development of digital twins in biopharmaceutical manufacturing, emphasizing the need for a human-machine collaborative intelligence framework to improve operator interaction and usability. It highlights the challenges faced by operators due to the complexity of digital twin systems and proposes design approaches and training programs to enhance effective collaboration and productivity.",
      "takeaways": [
        "- Digital twins are being increasingly utilized in biopharmaceutical manufacturing to automate processes but pose challenges for human operators.",
        "- The proposed collaborative intelligence framework aims to integrate operators more effectively with digital twins, enhancing their usability and trust.",
        "- Innovative training programs are suggested to equip operators with the skills to work effectively with digital twin technologies, thereby boosting resilience and productivity in manufacturing."
      ]
    },
    {
      "id": 43,
      "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead",
      "link": "https://arxiv.org/abs/2504.00294",
      "description": "arXiv:2504.00294v1 Announce Type: cross \nAbstract: Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.",
      "date": "2025-04-02",
      "authors": "Vidhisha Balachandran, Jingya Chen, Lingjiao Chen, Shivam Garg, Neel Joshi, Yash Lara, John Langford, Besmira Nushi, Vibhav Vineet, Yue Wu, Safoora Yousefi",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses inference-time scaling techniques that can improve the performance of large language models (LLMs) on complex reasoning tasks. It evaluates the efficacy of these scaling methods across various state-of-the-art models and identifies both benefits and limitations based on empirical analysis of multiple challenging tasks.",
      "takeaways": [
        "- Inference-time scaling can enhance reasoning capabilities of LLMs, but its effectiveness varies with task complexity.",
        "- Simply increasing token lengths does not guarantee improved model accuracy on challenging problems.",
        "- Models show significant performance gains when inference is scaled with strong feedback or perfect verifiers, indicating potential for further advancements."
      ]
    },
    {
      "id": 44,
      "title": "FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization",
      "link": "https://arxiv.org/abs/2504.00308",
      "description": "arXiv:2504.00308v1 Announce Type: cross \nAbstract: Federated Learning (FL) enables distributed training on edge devices but faces significant challenges due to resource constraints in edge environments, impacting both communication and computational efficiency. Existing iterative pruning techniques improve communication efficiency but are limited by their centralized design, which struggles with FL's decentralized and data-imbalanced nature, resulting in suboptimal sparsity levels. To address these issues, we propose FedPaI, a novel efficient FL framework that leverages Pruning at Initialization (PaI) to achieve extreme sparsity. FedPaI identifies optimal sparse connections at an early stage, maximizing model capacity and significantly reducing communication and computation overhead by fixing sparsity patterns at the start of training. To adapt to diverse hardware and software environments, FedPaI supports both structured and unstructured pruning. Additionally, we introduce personalized client-side pruning mechanisms for improved learning capacity and sparsity-aware server-side aggregation for enhanced efficiency. Experimental results demonstrate that FedPaI consistently outperforms existing efficient FL that applies conventional iterative pruning with significant leading in efficiency and model accuracy. For the first time, our proposed FedPaI achieves an extreme sparsity level of up to 98% without compromising the model accuracy compared to unpruned baselines, even under challenging non-IID settings. By employing our FedPaI with joint optimization of model learning capacity and sparsity, FL applications can benefit from faster convergence and accelerate the training by 6.4 to 7.9 times.",
      "date": "2025-04-02",
      "authors": "Haonan Wang, Zeli Liu, Kajimusugura Hoshino, Tuo Zhang, John Paul Walters, Stephen Crago",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces FedPaI, a novel framework that enhances federated learning by achieving extreme sparsity through a technique called Pruning at Initialization (PaI). This approach significantly improves communication and computation efficiency while maintaining model accuracy, making it particularly useful for resource-constrained edge environments.",
      "takeaways": [
        "- FedPaI can achieve up to 98% sparsity without compromising model accuracy, addressing the inefficiencies of conventional iterative pruning in federated learning.",
        "- The framework supports both structured and unstructured pruning, allowing it to adapt to various hardware and software environments.",
        "- By optimizing learning capacity and sparsity simultaneously, FedPaI enables faster convergence and training acceleration, demonstrating a speed improvement of 6.4 to 7.9 times."
      ]
    },
    {
      "id": 45,
      "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training",
      "link": "https://arxiv.org/abs/2504.00310",
      "description": "arXiv:2504.00310v1 Announce Type: cross \nAbstract: Large language models have revolutionized natural language processing with their surprising capability to understand and generate human-like text. However, many of these models inherit and further amplify the biases present in their training data, raising ethical and fairness concerns. The detection and mitigation of such biases are vital to ensuring that LLMs act responsibly and equitably across diverse domains. This work investigates Knowledge Graph-Augmented Training (KGAT) as a novel method to mitigate bias in LLM. Using structured domain-specific knowledge from real-world knowledge graphs, we improve the understanding of the model and reduce biased output. Public datasets for bias assessment include Gender Shades, Bias in Bios, and FairFace, while metrics such as demographic parity and equal opportunity facilitate rigorous detection. We also performed targeted mitigation strategies to correct biased associations, leading to a significant drop in biased output and improved bias metrics. Equipped with real-world datasets and knowledge graphs, our framework is both scalable and effective, paving the way toward responsible deployment in sensitive and high-stakes applications.",
      "date": "2025-04-02",
      "authors": "Rajeev Kumar, Harishankar Kumar, Kumari Shalini",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores a novel approach called Knowledge Graph-Augmented Training (KGAT) to detect and mitigate biases in large language models (LLMs). By integrating structured knowledge from real-world knowledge graphs, the authors demonstrate that this method can significantly reduce biased outputs in LLMs while ensuring responsible deployment in sensitive domains.",
      "takeaways": [
        "- The study addresses the critical issue of bias in LLMs and proposes a method to enhance model fairness using structured knowledge.",
        "- The use of public datasets for bias assessment highlights the importance of rigorous evaluation metrics, such as demographic parity and equal opportunity.",
        "- The framework presented is scalable and effective, suggesting potential applications in high-stakes fields, including the pharmaceutical industry."
      ]
    },
    {
      "id": 46,
      "title": "SeizureTransformer: Scaling U-Net with Transformer for Simultaneous Time-Step Level Seizure Detection from Long EEG Recordings",
      "link": "https://arxiv.org/abs/2504.00336",
      "description": "arXiv:2504.00336v1 Announce Type: cross \nAbstract: Epilepsy is a common neurological disorder that affects around 65 million people worldwide. Detecting seizures quickly and accurately is vital, given the prevalence and severity of the associated complications. Recently, deep learning-based automated seizure detection methods have emerged as solutions; however, most existing methods require extensive post-processing and do not effectively handle the crucial long-range patterns in EEG data. In this work, we propose SeizureTransformer, a simple model comprised of (i) a deep encoder comprising 1D convolutions (ii) a residual CNN stack and a transformer encoder to embed previous output into high-level representation with contextual information, and (iii) streamlined decoder which converts these features into a sequence of probabilities, directly indicating the presence or absence of seizures at every time step. Extensive experiments on public and private EEG seizure detection datasets demonstrate that our model significantly outperforms existing approaches (ranked in the first place in the 2025 \"seizure detection challenge\" organized in the International Conference on Artificial Intelligence in Epilepsy and Other Neurological Disorders), underscoring its potential for real-time, precise seizure detection.",
      "date": "2025-04-02",
      "authors": "Kerui Wu, Ziyue Zhao, B\\\"ulent Yener",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces SeizureTransformer, a novel deep learning model that enhances seizure detection from long EEG recordings by combining a convolutional encoder with transformer components. The model has demonstrated superior performance in accurately identifying seizures in real-time compared to existing methods, highlighting its potential for significant impact in clinical settings.",
      "takeaways": [
        "- SeizureTransformer effectively integrates 1D convolutional layers and transformer encoders to improve the capture of long-range patterns in EEG data.",
        "- The model achieved first place in the 2025 \"seizure detection challenge,\" indicating its robustness and effectiveness in real-world applications.",
        "- This advancement showcases the potential role of AI, particularly deep learning techniques, in enhancing healthcare, specifically in the management of epilepsy."
      ]
    },
    {
      "id": 47,
      "title": "Agentic Multimodal AI for Hyperpersonalized B2B and B2C Advertising in Competitive Markets: An AI-Driven Competitive Advertising Framework",
      "link": "https://arxiv.org/abs/2504.00338",
      "description": "arXiv:2504.00338v1 Announce Type: cross \nAbstract: The growing use of foundation models (FMs) in real-world applications demands adaptive, reliable, and efficient strategies for dynamic markets. In the chemical industry, AI-discovered materials drive innovation, but commercial success hinges on market adoption, requiring FM-driven advertising frameworks that operate in-the-wild. We present a multilingual, multimodal AI framework for autonomous, hyper-personalized advertising in B2B and B2C markets. By integrating retrieval-augmented generation (RAG), multimodal reasoning, and adaptive persona-based targeting, our system generates culturally relevant, market-aware ads tailored to shifting consumer behaviors and competition. Validation combines real-world product experiments with a Simulated Humanistic Colony of Agents to model consumer personas, optimize strategies at scale, and ensure privacy compliance. Synthetic experiments mirror real-world scenarios, enabling cost-effective testing of ad strategies without risky A/B tests. Combining structured retrieval-augmented reasoning with in-context learning (ICL), the framework boosts engagement, prevents market cannibalization, and maximizes ROAS. This work bridges AI-driven innovation and market adoption, advancing multimodal FM deployment for high-stakes decision-making in commercial marketing.",
      "date": "2025-04-02",
      "authors": "Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents an AI-driven competitive advertising framework using a multilingual, multimodal approach to create hyper-personalized ads for B2B and B2C markets. It emphasizes the integration of advanced AI techniques like retrieval-augmented generation and adaptive persona-based targeting to optimize advertising strategies based on consumer behaviors and market dynamics.",
      "takeaways": [
        "- The framework utilizes foundation models to enhance advertising effectiveness in competitive markets by tailoring ads to individual consumer personas.",
        "- It employs a combination of real-world product experiments and simulated models to validate strategies, reducing the risk associated with traditional A/B testing.",
        "- The integration of multimodal reasoning and in-context learning enables engagement improvement and maximizes return on ad spend (ROAS), thus facilitating better market adoption of AI-generated materials."
      ]
    },
    {
      "id": 48,
      "title": "Integrated LLM-Based Intrusion Detection with Secure Slicing xApp for Securing O-RAN-Enabled Wireless Network Deployments",
      "link": "https://arxiv.org/abs/2504.00341",
      "description": "arXiv:2504.00341v1 Announce Type: cross \nAbstract: The Open Radio Access Network (O-RAN) architecture is reshaping telecommunications by promoting openness, flexibility, and intelligent closed-loop optimization. By decoupling hardware and software and enabling multi-vendor deployments, O-RAN reduces costs, enhances performance, and allows rapid adaptation to new technologies. A key innovation is intelligent network slicing, which partitions networks into isolated slices tailored for specific use cases or quality of service requirements. The RAN Intelligent Controller further optimizes resource allocation, ensuring efficient utilization and improved service quality for user equipment (UEs). However, the modular and dynamic nature of O-RAN expands the threat surface, necessitating advanced security measures to maintain network integrity, confidentiality, and availability. Intrusion detection systems have become essential for identifying and mitigating attacks. This research explores using large language models (LLMs) to generate security recommendations based on the temporal traffic patterns of connected UEs. The paper introduces an LLM-driven intrusion detection framework and demonstrates its efficacy through experimental deployments, comparing non fine-tuned and fine-tuned models for task-specific accuracy.",
      "date": "2025-04-02",
      "authors": "Joshua Moore, Aly Sabri Abdalla, Prabesh Khanal, Vuk Marojevic",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents an LLM-driven intrusion detection framework designed to enhance the security of O-RAN-enabled wireless networks by utilizing large language models to generate security recommendations from temporal traffic patterns. The paper evaluates the effectiveness of fine-tuned versus non-fine-tuned LLMs in achieving task-specific accuracy for intrusion detection.",
      "takeaways": [
        "- The integration of large language models into network security systems can improve the accuracy and effectiveness of intrusion detection mechanisms.",
        "- O-RAN architecture requires innovative security solutions due to its increased threat surface from modular and dynamic network components.",
        "- The research highlights the importance of fine-tuning models for specific security tasks, demonstrating better performance in identifying potential attacks."
      ]
    },
    {
      "id": 49,
      "title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)",
      "link": "https://arxiv.org/abs/2504.00374",
      "description": "arXiv:2504.00374v1 Announce Type: cross \nAbstract: In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.",
      "date": "2025-04-02",
      "authors": "Mahak Agarwal, Divyam Khanna",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores the challenge of discerning truth from falsehood in multi-agent debates conducted by Large Language Models (LLMs). It introduces the Confidence-Weighted Persuasion Override Rate (CW-POR) as a metric to measure the extent to which LLMs can be deceived and their confidence in falsehoods, highlighting the necessity for improved model calibration to combat misinformation.",
      "takeaways": [
        "- The study reveals that even smaller LLMs can convincingly present false arguments that can override truthful responses.",
        "- The Confidence-Weighted Persuasion Override Rate (CW-POR) is a novel metric introduced to assess the reliability of LLMs in debate scenarios.",
        "- The findings stress the importance of adversarial testing and robust calibration of LLMs to reduce the risk of them confidently propagating misinformation."
      ]
    },
    {
      "id": 50,
      "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
      "link": "https://arxiv.org/abs/2504.00406",
      "description": "arXiv:2504.00406v1 Announce Type: cross \nAbstract: Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
      "date": "2025-04-02",
      "authors": "Jiuzhou Han, Wray Buntine, Ehsan Shareghi",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces VerifiAgent, a unified verification agent designed to enhance the reliability of language model reasoning by integrating two levels of verification—meta-verification and tool-based adaptive verification. This approach allows the agent to autonomously select verification tools suitable for various reasoning types, improving accuracy and reducing computational costs.",
      "takeaways": [
        "- VerifiAgent combines meta-verification and adaptive verification to improve the consistency and completeness of model responses.",
        "- It autonomously selects appropriate verification tools tailored to specific reasoning types, enhancing efficiency across different verification tasks.",
        "- Experimental results show that VerifiAgent outperforms existing verification methods and is effective in inference scaling, yielding better results with fewer samples."
      ]
    },
    {
      "id": 51,
      "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding",
      "link": "https://arxiv.org/abs/2504.00409",
      "description": "arXiv:2504.00409v1 Announce Type: cross \nAbstract: Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding.",
      "date": "2025-04-02",
      "authors": "Mohanakrishnan Hariharan",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This paper discusses advancements in large language models (LLMs) by incorporating advanced natural language understanding techniques, including semantic parsing and contextual reinforcement learning. It highlights the importance of structured knowledge integration and transformer architectures to address challenges like ambiguity and factual inconsistencies in NLP tasks.",
      "takeaways": [
        "- The integration of semantic parsing and knowledge graphs enhances the semantic understanding of large language models.",
        "- Techniques such as retrieval-augmented generation (RAG) and contrastive learning are essential for improving contextual coherence and reducing hallucinations in AI systems.",
        "- Future research should focus on bridging the gap between statistical models and true natural language understanding to elevate AI's performance in complex NLP tasks."
      ]
    },
    {
      "id": 52,
      "title": "No Free Lunch with Guardrails",
      "link": "https://arxiv.org/abs/2504.00441",
      "description": "arXiv:2504.00441v1 Announce Type: cross \nAbstract: As large language models (LLMs) and generative AI become widely adopted, guardrails have emerged as a key tool to ensure their safe use. However, adding guardrails isn't without tradeoffs; stronger security measures can reduce usability, while more flexible systems may leave gaps for adversarial attacks. In this work, we explore whether current guardrails effectively prevent misuse while maintaining practical utility. We introduce a framework to evaluate these tradeoffs, measuring how different guardrails balance risk, security, and usability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails; strengthening security often comes at the cost of usability. To address this, we propose a blueprint for designing better guardrails that minimize risk while maintaining usability. We evaluate various industry guardrails, including Azure Content Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI, Nemo Guardrails, and our own custom-built guardrails. Additionally, we assess how LLMs like GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest respond under different system prompts, including simple prompts, detailed prompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study provides a clear comparison of how different guardrails perform, highlighting the challenges in balancing security and usability.",
      "date": "2025-04-02",
      "authors": "Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article discusses the tradeoffs involved in implementing guardrails for large language models (LLMs) and generative AI, emphasizing the balance between security and usability. It introduces a framework for evaluating guardrails and provides insights into various industry standards while proposing a blueprint for improved designs.",
      "takeaways": [
        "- Guardrails are essential for the safe use of AI systems but may compromise usability when security is strengthened.",
        "- The study evaluates the effectiveness of existing guardrails from various industry leaders, providing comparative insights into their performance.",
        "- A new blueprint is proposed to guide the design of guardrails that minimize risks while maintaining user-friendly interactions with AI models."
      ]
    },
    {
      "id": 53,
      "title": "MetaLoRA: Tensor-Enhanced Adaptive Low-Rank Fine-tuning",
      "link": "https://arxiv.org/abs/2504.00460",
      "description": "arXiv:2504.00460v1 Announce Type: cross \nAbstract: There has been a significant increase in the deployment of neural network models, presenting substantial challenges in model adaptation and fine-tuning. Efficient adaptation is crucial in maintaining model performance across diverse tasks and domains. While Low-Rank Adaptation (LoRA) has emerged as a promising parameter-efficient fine-tuning method, its fixed parameter nature limits its ability to handle dynamic task requirements effectively. Adapting models to new tasks can be challenging due to the need for extensive fine-tuning. Current LoRA variants primarily focus on general parameter reduction while overlooking the importance of dynamic parameter adjustment and meta-learning capabilities. Moreover, existing approaches mainly address static adaptations, neglecting the potential benefits of task-aware parameter generation in handling diverse task distributions. To address these limitations, this Ph.D. research proposes a LoRA generation approach to model task relationships and introduces MetaLoRA, a novel parameter-efficient adaptation framework incorporating meta-learning principles. This work develops a comprehensive architecture that integrates meta-parameter generation with adaptive low-rank decomposition, enabling efficient handling of both task-specific and task-agnostic features. MetaLoRA accurately captures task patterns by incorporating meta-learning mechanisms and dynamic parameter adjustment strategies. To our knowledge, this research represents the first attempt to provide a meta-learning enhanced LoRA variant, offering improved adaptation capability while maintaining computational efficiency in model fine-tuning.",
      "date": "2025-04-02",
      "authors": "Maolin Wang, Xiangyu Zhao",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents MetaLoRA, a novel framework for efficient model fine-tuning using an enhanced Low-Rank Adaptation method that integrates meta-learning principles and dynamic parameter adjustment, aiming to improve adaptation capabilities across varied tasks. This research is particularly significant as it addresses the limitations of existing fine-tuning methods by introducing a mechanism for task-aware parameter generation.",
      "takeaways": [
        "- MetaLoRA improves on the traditional Low-Rank Adaptation (LoRA) by incorporating meta-learning principles for better adaptability to new tasks.",
        "- The framework allows for dynamic adjustments of parameters, enhancing performance across diverse tasks and domains.",
        "- This work represents a significant advancement in model fine-tuning techniques, making them more effective while maintaining computational efficiency."
      ]
    },
    {
      "id": 54,
      "title": "Towards Adaptive AI Governance: Comparative Insights from the U.S., EU, and Asia",
      "link": "https://arxiv.org/abs/2504.00652",
      "description": "arXiv:2504.00652v1 Announce Type: cross \nAbstract: Artificial intelligence (AI) trends vary significantly across global regions, shaping the trajectory of innovation, regulation, and societal impact. This variation influences how different regions approach AI development, balancing technological progress with ethical and regulatory considerations. This study conducts a comparative analysis of AI trends in the United States (US), the European Union (EU), and Asia, focusing on three key dimensions: generative AI, ethical oversight, and industrial applications. The US prioritizes market-driven innovation with minimal regulatory constraints, the EU enforces a precautionary risk-based framework emphasizing ethical safeguards, and Asia employs state-guided AI strategies that balance rapid deployment with regulatory oversight. Although these approaches reflect different economic models and policy priorities, their divergence poses challenges to international collaboration, regulatory harmonization, and the development of global AI standards. To address these challenges, this paper synthesizes regional strengths to propose an adaptive AI governance framework that integrates risk-tiered oversight, innovation accelerators, and strategic alignment mechanisms. By bridging governance gaps, this study offers actionable insights for fostering responsible AI development while ensuring a balance between technological progress, ethical imperatives, and regulatory coherence.",
      "date": "2025-04-02",
      "authors": "Vikram Kulothungan, Deepti Gupta",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article explores the varying trends in artificial intelligence governance across the U.S., EU, and Asia, highlighting the balance between innovation and ethical regulation in each region. It proposes an adaptive governance framework aimed at harmonizing regulatory efforts while promoting responsible AI development.",
      "takeaways": [
        "- The U.S. focuses on market-driven innovation with few regulatory constraints, while the EU emphasizes ethical safeguards within a precautionary framework.",
        "- Asia's approach to AI combines state-guided strategies enabling rapid deployment alongside regulatory oversight.",
        "- The paper suggests the need for an adaptive AI governance framework to navigate international collaboration and regulatory harmonization challenges."
      ]
    },
    {
      "id": 55,
      "title": "DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism",
      "link": "https://arxiv.org/abs/2504.00661",
      "description": "arXiv:2504.00661v1 Announce Type: cross \nAbstract: Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DynMoLE achieves substantial performance improvements, outperforming LoRA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DynMoLE's key components.",
      "date": "2025-04-02",
      "authors": "Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, Meng Xiao, Mingjie Tang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents DynMoLE, a novel hybrid routing mechanism designed to enhance the Mixture of LoRA Experts fine-tuning method for large language models. By utilizing Tsallis entropy for dynamic expert selection, DynMoLE improves predictive accuracy and stability while achieving superior performance on commonsense reasoning benchmarks.",
      "takeaways": [
        "- DynMoLE improves upon existing Mixture of LoRA Experts methods by addressing expert selection diversity and router uncertainty.",
        "- The proposed hybrid routing mechanism leads to faster convergence and enhanced model performance, demonstrated by a 9.6% improvement over LoRA and a 2.3% improvement over the previous state-of-the-art.",
        "- An auxiliary loss based on Tsallis entropy is introduced to guide model training, further improving stability and performance."
      ]
    },
    {
      "id": 56,
      "title": "Command A: An Enterprise-Ready Large Language Model",
      "link": "https://arxiv.org/abs/2504.00698",
      "description": "arXiv:2504.00698v1 Announce Type: cross \nAbstract: In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.",
      "date": "2025-04-02",
      "authors": "Team Cohere,  Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Rapha\\\"el Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Bj\\\"orn Bebensee, Neeral Beladia, Walter Beller-Morales, Alexandre B\\'erard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagn\\'e, Juli\\'an Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D'souza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Th\\'eo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Erg\\\"un, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gall\\'e, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofst\\\"atter, Sungjin Hong, Sara Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kry\\'sci\\'nski, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Lukas Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, C\\'ecile Robert-Michon, Aur\\'elien Rodriguez, Sudip Roy, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Chang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, Ahmet \\\"Ust\\\"un, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, Zhoujie Zhao",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents Command A, a large language model designed for enterprise applications, featuring multilingual capabilities and advanced Retrieval Augmented Generation functions. It details the model's training approach and performance evaluations, emphasizing its efficiency and effectiveness in handling business-related tasks.",
      "takeaways": [
        "- Command A is optimized for real-world enterprise use cases and supports 23 languages, enhancing its utility in global business environments.",
        "- The model incorporates innovative training techniques, including decentralised training and self-refinement algorithms, which contribute to its performance and efficiency.",
        "- The report offers weights for Command A and a related model, Command R7B, for research purposes, enabling further exploration and application in various contexts."
      ]
    },
    {
      "id": 57,
      "title": "LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models",
      "link": "https://arxiv.org/abs/2504.00752",
      "description": "arXiv:2504.00752v1 Announce Type: cross \nAbstract: Extracting structured information from unstructured text is crucial for modeling real-world processes, but traditional schema mining relies on semi-structured data, limiting scalability. This paper introduces schema-miner, a novel tool that combines large language models with human feedback to automate and refine schema extraction. Through an iterative workflow, it organizes properties from text, incorporates expert input, and integrates domain-specific ontologies for semantic depth. Applied to materials science--specifically atomic layer deposition--schema-miner demonstrates that expert-guided LLMs generate semantically rich schemas suitable for diverse real-world applications.",
      "date": "2025-04-02",
      "authors": "Sameer Sadruddin, Jennifer D'Souza, Eleni Poupaki, Alex Watkins, Hamed Babaei Giglou, Anisa Rula, Bora Karasulu, S\\\"oren Auer, Adrie Mackus, Erwin Kessels",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents \"schema-miner,\" a novel tool that utilizes large language models (LLMs) combined with human feedback to enhance the extraction of structured information from unstructured text. It emphasizes its application in materials science and showcases how expert guidance leads to the creation of semantically rich schemas that can be utilized in various real-world scenarios.",
      "takeaways": [
        "- The integration of large language models with human input significantly improves the schema extraction process from unstructured text.",
        "- The iterative workflow of schema-miner allows for the continuous refinement of schemas, incorporating expert knowledge and domain-specific ontologies.",
        "- The approach demonstrates its applicability in specific fields like materials science, highlighting the versatility of LLMs in generating useful outcomes for diverse applications."
      ]
    },
    {
      "id": 58,
      "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models",
      "link": "https://arxiv.org/abs/2504.00869",
      "description": "arXiv:2504.00869v1 Announce Type: cross \nAbstract: Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.",
      "date": "2025-04-02",
      "authors": "Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores the technique of test-time scaling to enhance the reasoning capabilities of large language models specifically within the medical domain. It provides a comprehensive evaluation demonstrating that test-time scaling can significantly improve medical reasoning performance, even with comparatively smaller models.",
      "takeaways": [
        "- Test-time scaling has been shown to effectively enhance the reasoning capabilities of large language models in medical tasks.",
        "- An optimal reasoning token budget (around 4K) is identified, beyond which the performance may degrade, indicating the importance of balance in model reasoning.",
        "- The study highlights that increasing data scale, improving data quality, and expanding model capacity are crucial for advancing medical knowledge grounding, which in turn improves performance on challenging medical benchmarks."
      ]
    },
    {
      "id": 59,
      "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
      "link": "https://arxiv.org/abs/2504.00883",
      "description": "arXiv:2504.00883v1 Announce Type: cross \nAbstract: Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.",
      "date": "2025-04-02",
      "authors": "Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents an in-depth study on enhancing the visual-spatial reasoning capabilities of multi-modal large language models (MLLMs) through a training method called R1-Zero-like training. The authors demonstrate improvements in performance using the vsGRPO model fine-tuned from Qwen2-VL, showcasing advancements in reasoning for AI agents in physical environments.",
      "takeaways": [
        "- The study identifies limitations in visual-spatial reasoning activation for smaller MLLMs through conventional prompting methods and proposes a novel training approach.",
        "- The vsGRPO-2B and vsGRPO-7B models outperform existing models, including GPT-4o and LLaVA-NeXT-Video-72B, indicating significant advancements in multi-modal reasoning performance.",
        "- The research introduces the VSI-100k dataset and highlights the importance of specific training techniques, such as GRPO and KL penalty, to enhance model capabilities."
      ]
    },
    {
      "id": 60,
      "title": "Role and Use of Race in AI/ML Models Related to Health",
      "link": "https://arxiv.org/abs/2504.00899",
      "description": "arXiv:2504.00899v1 Announce Type: cross \nAbstract: The role and use of race within health-related artificial intelligence and machine learning (AI/ML) models has sparked increasing attention and controversy. Despite the complexity and breadth of related issues, a robust and holistic framework to guide stakeholders in their examination and resolution remains lacking. This perspective provides a broad-based, systematic, and cross-cutting landscape analysis of race-related challenges, structured around the AI/ML lifecycle and framed through \"points to consider\" to support inquiry and decision-making.",
      "date": "2025-04-02",
      "authors": "Martin C. Were, Ang Li, Bradley A. Malin, Zhijun Yin, Joseph R. Coco, Benjamin X. Collins, Ellen Wright Clayton, Laurie L. Novak, Rachele Hendricks-Sturrup, Abiodun Oluyomi, Shilo Anders, Chao Yan",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article explores the complex role of race in health-related artificial intelligence and machine learning models, highlighting the controversies and challenges associated with its use. It presents a framework for stakeholders to analyze and address these issues throughout the AI/ML lifecycle.",
      "takeaways": [
        "- The use of race in AI/ML health models raises ethical and methodological concerns that need to be addressed.",
        "- A systematic framework is proposed to guide stakeholders in evaluating race-related challenges in AI/ML applications.",
        "- The article emphasizes the need for comprehensive frameworks in decision-making related to AI/ML and race in healthcare."
      ]
    },
    {
      "id": 61,
      "title": "Personalized Federated Training of Diffusion Models with Privacy Guarantees",
      "link": "https://arxiv.org/abs/2504.00952",
      "description": "arXiv:2504.00952v1 Announce Type: cross \nAbstract: The scarcity of accessible, compliant, and ethically sourced data presents a considerable challenge to the adoption of artificial intelligence (AI) in sensitive fields like healthcare, finance, and biomedical research. Furthermore, access to unrestricted public datasets is increasingly constrained due to rising concerns over privacy, copyright, and competition. Synthetic data has emerged as a promising alternative, and diffusion models -- a cutting-edge generative AI technology -- provide an effective solution for generating high-quality and diverse synthetic data. In this paper, we introduce a novel federated learning framework for training diffusion models on decentralized private datasets. Our framework leverages personalization and the inherent noise in the forward diffusion process to produce high-quality samples while ensuring robust differential privacy guarantees. Our experiments show that our framework outperforms non-collaborative training methods, particularly in settings with high data heterogeneity, and effectively reduces biases and imbalances in synthetic data, resulting in fairer downstream models.",
      "date": "2025-04-02",
      "authors": "Kumar Kshitij Patel, Weitong Zhang, Lingxiao Wang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a novel federated learning framework designed for training diffusion models using decentralized private datasets, addressing challenges in AI adoption within sensitive fields like healthcare due to data privacy concerns. The proposed method improves the generation of synthetic data, ensuring high quality while maintaining privacy through personalized training.",
      "takeaways": [
        "- The framework enhances the training of diffusion models by leveraging decentralized datasets, making it suitable for sensitive industries like healthcare.",
        "- It achieves high-quality synthetic data generation while ensuring robust differential privacy, addressing critical ethical concerns.",
        "- The approach demonstrates improved performance over traditional non-collaborative training, particularly in situations with diverse data characteristics, helping to reduce biases in AI models."
      ]
    },
    {
      "id": 62,
      "title": "Enabling Efficient Processing of Spiking Neural Networks with On-Chip Learning on Commodity Neuromorphic Processors for Edge AI Systems",
      "link": "https://arxiv.org/abs/2504.00957",
      "description": "arXiv:2504.00957v1 Announce Type: cross \nAbstract: The rising demand for energy-efficient edge AI systems (e.g., mobile agents/robots) has increased the interest in neuromorphic computing, since it offers ultra-low power/energy AI computation through spiking neural network (SNN) algorithms on neuromorphic processors. However, their efficient implementation strategy has not been comprehensively studied, hence limiting SNN deployments for edge AI systems. Toward this, we propose a design methodology to enable efficient SNN processing on commodity neuromorphic processors. To do this, we first study the key characteristics of targeted neuromorphic hardware (e.g., memory and compute budgets), and leverage this information to perform compatibility analysis for network selection. Afterward, we employ a mapping strategy for efficient SNN implementation on the targeted processor. Furthermore, we incorporate an efficient on-chip learning mechanism to update the systems' knowledge for adapting to new input classes and dynamic environments. The experimental results show that the proposed methodology leads the system to achieve low latency of inference (i.e., less than 50ms for image classification, less than 200ms for real-time object detection in video streaming, and less than 1ms in keyword recognition) and low latency of on-chip learning (i.e., less than 2ms for keyword recognition), while incurring less than 250mW of processing power and less than 15mJ of energy consumption across the respective different applications and scenarios. These results show the potential of the proposed methodology in enabling efficient edge AI systems for diverse application use-cases.",
      "date": "2025-04-02",
      "authors": "Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article discusses a design methodology for implementing efficient spiking neural networks (SNNs) on commodity neuromorphic processors aimed at enhancing edge AI systems' energy efficiency. The proposed approach includes compatibility analysis for hardware and an on-chip learning mechanism, resulting in low latencies and minimal energy consumption across various applications.",
      "takeaways": [
        "- The methodology allows for efficient processing of SNNs on neuromorphic hardware, which is crucial for energy-constrained edge AI applications.",
        "- Experiments demonstrated low inference and learning latencies, making SNNs viable for real-time applications such as image classification and object detection.",
        "- The approach achieves significant energy efficiency, utilizing less than 250mW of power and under 15mJ of energy across different scenarios."
      ]
    },
    {
      "id": 63,
      "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching",
      "link": "https://arxiv.org/abs/2504.00970",
      "description": "arXiv:2504.00970v1 Announce Type: cross \nAbstract: Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, and Needle-In-A-Haystack demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.",
      "date": "2025-04-02",
      "authors": "Yuxuan Zhu, Ali Falahati, David H. Yang, Mohammad Mohammadi Amiri",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents SentenceKV, a novel approach for improving inference efficiency in large language models by implementing sentence-level semantic key-value caching, which enhances memory management and computational efficiency during autoregressive generation. This method groups tokens based on semantic similarity, allowing for more contextually accurate predictions while minimizing memory usage and maintaining inference speed even for long inputs.",
      "takeaways": [
        "- SentenceKV improves computational efficiency and memory management in large language models by using sentence-level semantic similarity for caching.",
        "- The method reduces redundancy in data loading, resulting in lower memory overhead during inference.",
        "- Extensive evaluations demonstrate that SentenceKV outperforms existing state-of-the-art caching methods in both efficiency and accuracy, particularly in handling long contexts."
      ]
    },
    {
      "id": 64,
      "title": "Accelerating drug discovery with Artificial: a whole-lab orchestration and scheduling system for self-driving labs",
      "link": "https://arxiv.org/abs/2504.00986",
      "description": "arXiv:2504.00986v1 Announce Type: cross \nAbstract: Self-driving labs are transforming drug discovery by enabling automated, AI-guided experimentation, but they face challenges in orchestrating complex workflows, integrating diverse instruments and AI models, and managing data efficiently. Artificial addresses these issues with a comprehensive orchestration and scheduling system that unifies lab operations, automates workflows, and integrates AI-driven decision-making. By incorporating AI/ML models like NVIDIA BioNeMo - which facilitates molecular interaction prediction and biomolecular analysis - Artificial enhances drug discovery and accelerates data-driven research. Through real-time coordination of instruments, robots, and personnel, the platform streamlines experiments, enhances reproducibility, and advances drug discovery.",
      "date": "2025-04-02",
      "authors": "Yao Fehlis, Paul Mandel, Charles Crain, Betty Liu, David Fuller",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses \"Artificial,\" a whole-lab orchestration and scheduling system designed to optimize self-driving labs in drug discovery by automating workflows, integrating various AI models, and improving data management. It highlights how this platform enables real-time coordination among lab instruments and personnel, thereby accelerating the drug discovery process.",
      "takeaways": [
        "- The system addresses significant challenges in drug discovery by automating complex workflows and integrating diverse AI models.",
        "- Incorporation of AI/ML models, such as NVIDIA BioNeMo, enhances the platform's ability for molecular interaction prediction and biomolecular analysis.",
        "- Real-time coordination improves the efficiency and reproducibility of experiments in self-driving labs, ultimately advancing drug discovery efforts."
      ]
    },
    {
      "id": 65,
      "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs",
      "link": "https://arxiv.org/abs/2504.00993",
      "description": "arXiv:2504.00993v1 Announce Type: cross \nAbstract: Medical tasks such as diagnosis and treatment planning require precise and complex reasoning, particularly in life-critical domains. Unlike mathematical reasoning, medical reasoning demands meticulous, verifiable thought processes to ensure reliability and accuracy. However, there is a notable lack of datasets that provide transparent, step-by-step reasoning to validate and enhance the medical reasoning ability of AI models. To bridge this gap, we introduce MedReason, a large-scale high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in large language models (LLMs). We utilize a structured medical knowledge graph (KG) to convert clinical QA pairs into logical chains of reasoning, or ``thinking paths'', which trace connections from question elements to answers via relevant KG entities. Each path is validated for consistency with clinical logic and evidence-based medicine. Our pipeline generates detailed reasoning for various medical questions from 7 medical datasets, resulting in a dataset of 32,682 question-answer pairs, each with detailed, step-by-step explanations. Experiments demonstrate that fine-tuning with our dataset consistently boosts medical problem-solving capabilities, achieving significant gains of up to 7.7% for DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the Huatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the clinical benchmark MedBullets. We also engage medical professionals from diverse specialties to assess our dataset's quality, ensuring MedReason offers accurate and coherent medical reasoning. Our data, models, and code will be publicly available.",
      "date": "2025-04-02",
      "authors": "Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces MedReason, a novel large-scale dataset designed to enhance medical reasoning in large language models (LLMs) by employing a structured medical knowledge graph to link clinical question-answer pairs with logical reasoning paths. The study shows that fine-tuning models with this dataset significantly improves their medical problem-solving capabilities.",
      "takeaways": [
        "- MedReason provides a comprehensive dataset of 32,682 question-answer pairs with detailed, explainable reasoning for various medical tasks.",
        "- Fine-tuning with the MedReason dataset improves performance on medical reasoning tasks, achieving up to 7.7% gains for certain models.",
        "- The quality of the dataset was ensured through the involvement of medical professionals across specialties, making it a reliable resource for training AI in the medical field."
      ]
    },
    {
      "id": 66,
      "title": "Token embeddings violate the manifold hypothesis",
      "link": "https://arxiv.org/abs/2504.01002",
      "description": "arXiv:2504.01002v1 Announce Type: cross \nAbstract: To fully understand the behavior of a large language model (LLM) requires our understanding of its input space. If this input space differs from our assumption, our understanding of and conclusions about the LLM is likely flawed, regardless of its architecture. Here, we elucidate the structure of the token embeddings, the input domain for LLMs, both empirically and theoretically. We present a generalized and statistically testable model where the neighborhood of each token splits into well-defined signal and noise dimensions.\n  This model is based on a generalization of a manifold called a fiber bundle, so we denote our hypothesis test as the ``fiber bundle null.'' Failing to reject the null is uninformative, but rejecting it at a specific token indicates that token has a statistically significant local structure, and so is of interest to us. By running our test over several open-source LLMs, each with unique token embeddings, we find that the null is frequently rejected, and so the token subspace is provably not a fiber bundle and hence also not a manifold. As a consequence of our findings, when an LLM is presented with two semantically equivalent prompts, and if one prompt contains a token implicated by our test, that prompt will likely exhibit more output variability proportional to the local signal dimension of the token.",
      "date": "2025-04-02",
      "authors": "Michael Robinson, Sourya Dey, Tony Chiang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores the input space of large language models (LLMs) and tests the structure of token embeddings using a statistical model based on fiber bundles. It reveals that the local structure of tokens in LLMs does not conform to the manifold hypothesis, indicating that some tokens exhibit significant variability in output based on their local signal dimensions.",
      "takeaways": [
        "- The study introduces a novel statistical model that challenges the common assumption that token embeddings in LLMs adhere to the manifold hypothesis.",
        "- It demonstrates through empirical testing that the input space of LLMs contains structured variability, affecting how they respond to semantically equivalent prompts.",
        "- The findings could lead to improved understanding and methodologies for interacting with LLMs, especially in the context of prompt engineering within the AI field."
      ]
    },
    {
      "id": 67,
      "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
      "link": "https://arxiv.org/abs/2504.01005",
      "description": "arXiv:2504.01005v1 Announce Type: cross \nAbstract: Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
      "date": "2025-04-02",
      "authors": "Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article discusses the optimization of reasoning capabilities in large language models (LLMs) using two key methods: Self-Consistency (SC) and Generative Reward Models (GenRM). It evaluates their performance under fixed inference budgets, highlighting that SC tends to be more compute-efficient than GenRM for most scenarios.",
      "takeaways": [
        "- Self-Consistency (SC) generally outperforms Generative Reward Models (GenRM) in terms of compute efficiency for reasoning tasks in large language models.",
        "- The study provides insights into the trade-offs between solution generation and verification, suggesting that more compute should be allocated to scaling solution generation.",
        "- Practical guidance is offered for optimizing test-time scaling in AI systems, potentially influencing future developments in model design and deployment strategies."
      ]
    },
    {
      "id": 68,
      "title": "Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier",
      "link": "https://arxiv.org/abs/2405.17956",
      "description": "arXiv:2405.17956v3 Announce Type: replace \nAbstract: For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to easily tune language models to maximize auxiliary, non-preferential objectives according to the LLM designer's preferences (e.g., tuning lexical style or minimizing specific kinds of harmful content). Critically, these designer objectives may not be amply human-labeled or represented in available data, align with user preferences, or even be able to be captured tractably by binary preference pairs. To leverage the simplicity and performance of DPO with the generality of RL, we propose a unified approach. Based on a simple decomposition of preference and auxiliary objectives, we allow for tuning LLMs to optimize user and designer preferences without any additional specialized or preference data, computational cost, stability ``tweaks'', or training instability. The proposed method, Unified Preference Optimization, shows the ability to effectively generalize to user preferences and auxiliary objectives, while preserving or surpassing alignment performance on challenging benchmarks across a range of model sizes.",
      "date": "2025-04-02",
      "authors": "Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a new method called Unified Preference Optimization for aligning large language models (LLMs), which combines the benefits of reinforcement learning from human feedback and direct preference optimization, enabling tuning of both user and designer preferences without requiring specialized data or incurring additional computational costs.",
      "takeaways": [
        "- The proposed method simplifies the alignment of LLMs by optimizing both user preferences and auxiliary objectives using a unified approach.",
        "- Unified Preference Optimization improves upon existing techniques by maintaining performance while allowing for greater flexibility in tuning models to meet specific design objectives.",
        "- The method demonstrates effective generalization capabilities across various benchmarks and model sizes, indicating its robustness and potential for practical application in AI development."
      ]
    },
    {
      "id": 69,
      "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
      "link": "https://arxiv.org/abs/2411.06559",
      "description": "arXiv:2411.06559v2 Announce Type: replace \nAbstract: Language agents based on large language models (LLMs) have demonstrated great promise in automating web-based tasks. Recent work has shown that incorporating advanced planning algorithms, e.g., tree search, is advantageous over reactive planning for web agents. However, unlike simulated sandbox environments, real-world environments such as the web are rife with irreversible actions. This undermines the feasibility of backtracking, a cornerstone of (tree) search. Overly relying on test-time search also hurts efficiency. We advocate model-based planning for web agents that employs a world model to simulate and deliberate over the outcome of each candidate action before committing to one. We systematically explore this paradigm by (1) Proposing a model-based planning framework, WebDreamer, which employs LLMs to serve as both world models and value functions; (2) Training specialized LLMs as world models with a scalable data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves substantial performance improvements over reactive baselines. It is competitive, while being 4-5 times more efficient, with tree search in sandbox environments (VisualWebArena) and also works effectively on real-world websites (Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model, Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of specialized world models for efficient and effective planning in complex web environments.",
      "date": "2025-04-02",
      "authors": "Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, Yu Su",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses a model-based planning framework called WebDreamer, which uses large language models (LLMs) as world models to enhance the performance of web agents by simulating outcomes of actions before execution. This approach improves the efficiency and effectiveness of task automation in real-world web environments compared to existing reactive planning methods.",
      "takeaways": [
        "- The WebDreamer framework allows language agents to simulate and deliberate on actions, leading to better decision-making in complex web tasks.",
        "- Specialized LLMs used as world models can significantly enhance planning efficiency, achieving performance that is competitive with traditional tree search methods while being 4-5 times more efficient.",
        "- The introduced model, Dreamer-7B, shows comparable performance to well-known models like GPT-4o, indicating the potential of tailored models for specific applications in web environments."
      ]
    },
    {
      "id": 70,
      "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
      "link": "https://arxiv.org/abs/2411.13543",
      "description": "arXiv:2411.13543v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as several models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community. Code and Leaderboard at balrogai.com.",
      "date": "2025-04-02",
      "authors": "Davide Paglieri, Bart{\\l}omiej Cupia{\\l}, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, {\\L}ukasz Kuci\\'nski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rockt\\\"aschel",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces BALROG, a novel benchmark for evaluating the agentic capabilities of Large Language Models (LLMs) and Vision Language Models (VLMs) through a challenging set of games, highlighting their reasoning abilities and identifying deficiencies in complex environments. The benchmark aims to facilitate further research in assessing AI models' performance in dynamic and intricate tasks.",
      "takeaways": [
        "- BALROG provides a comprehensive benchmark to assess LLMs and VLMs in various game scenarios, ranging from easy to extremely difficult tasks.",
        "- Current models demonstrate limited success in simpler games but struggle significantly in more complex environments, particularly in vision-based decision-making.",
        "- The benchmark and its accompanying resources are open and accessible, promoting ongoing research and development in the field of agentic AI capabilities."
      ]
    },
    {
      "id": 71,
      "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps",
      "link": "https://arxiv.org/abs/2503.19326",
      "description": "arXiv:2503.19326v2 Announce Type: replace \nAbstract: Recent reasoning large language models (LLMs) have demonstrated remarkable improvements in mathematical reasoning capabilities through long Chain-of-Thought. The reasoning tokens of these models enable self-correction within reasoning chains, enhancing robustness. This motivates our exploration: how vulnerable are reasoning LLMs to subtle errors in their input reasoning chains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models presented with reasoning tokens containing manipulated calculation results tend to ignore correct reasoning steps and adopt incorrect results instead. Through systematic evaluation across multiple reasoning LLMs, we design three increasingly explicit prompting methods to measure CPT resistance, revealing that models struggle significantly to identify and correct these manipulations. Notably, contrary to existing research suggesting structural alterations affect model performance more than content modifications, we find that local ending token manipulations have greater impact on reasoning outcomes than structural changes. Moreover, we discover a security vulnerability in DeepSeek-R1 where tampered reasoning tokens can trigger complete reasoning cessation. Our work enhances understanding of reasoning robustness and highlights security considerations for reasoning-intensive applications.",
      "date": "2025-04-02",
      "authors": "Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article investigates the vulnerability of reasoning large language models (LLMs) to manipulated reasoning tokens, introducing the concept of \"Compromising Thought\" (CPT). The research reveals that these models can be misled by subtle input manipulations, leading to incorrect reasoning outcomes and highlighting significant security concerns for reasoning-dependent applications.",
      "takeaways": [
        "- Reasoning LLMs may ignore correct reasoning steps when presented with manipulated ending tokens, demonstrating a critical vulnerability in their reasoning processes.",
        "- The study finds that local manipulations of reasoning tokens have a more significant impact on outcomes than structural changes in the model.",
        "- Security implications are raised, particularly in the context of applications that rely heavily on accurate reasoning, necessitating further exploration of robustness and error correction in LLMs."
      ]
    },
    {
      "id": 72,
      "title": "MolGround: A Benchmark for Molecular Grounding",
      "link": "https://arxiv.org/abs/2503.23668",
      "description": "arXiv:2503.23668v2 Announce Type: replace \nAbstract: Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 79k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.",
      "date": "2025-04-02",
      "authors": "Jiaxin Wu, Ting Zhang, Rubing Chen, Wengyu Zhang, Chen Jason Zhang, Xiaoyong Wei, Li Qing",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a new benchmark for molecular grounding aimed at assessing a model's ability to link molecular concepts to specific structural components. It showcases the integration of NLP techniques to enhance molecular understanding and reports on the development of a multi-agent grounding prototype that surpasses existing models.",
      "takeaways": [
        "- The proposed benchmark includes the largest dataset for molecular grounding to date, consisting of 79,000 question-answer pairs.",
        "- The research highlights the potential of using natural language processing (NLP) techniques to improve molecular understanding in scientific contexts.",
        "- The developed multi-agent grounding prototype not only outperforms models like GPT-4o but also enhances traditional tasks such as molecular captioning and ATC classification."
      ]
    },
    {
      "id": 73,
      "title": "Holistic analysis on the sustainability of Federated Learning across AI product lifecycle",
      "link": "https://arxiv.org/abs/2312.14628",
      "description": "arXiv:2312.14628v2 Announce Type: replace-cross \nAbstract: In light of emerging legal requirements and policies focused on privacy protection, there is a growing trend of companies across various industries adopting Federated Learning (FL). This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data. Unlike traditional methods that necessitate data sharing and transmission, Cross-Silo FL allows clients to share model updates rather than raw data, thereby enhancing privacy. Despite its growing adoption, the carbon impact associated with Cross-Silo FL remains poorly understood due to the limited research in this area. This study seeks to bridge this gap by evaluating the sustainability of Cross-Silo FL throughout the entire AI product lifecycle, extending the analysis beyond the model training phase alone. We systematically compare this decentralized method with traditional centralized approaches and present a robust quantitative framework for assessing the costs and CO2 emissions in real-world Cross-Silo FL environments. Our findings indicate that the energy consumption and costs of model training are comparable between Cross-Silo Federated Learning and Centralized Learning. However, the additional data transfer and storage requirements inherent in Centralized Learning can result in significant, often overlooked CO2 emissions. Moreover, we introduce an innovative data and application management system that integrates Cross-Silo FL and analytics, aiming at improving the sustainability and economic efficiency of IT enterprises.",
      "date": "2025-04-02",
      "authors": "Hongliu Cao",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores the sustainability of Federated Learning (FL) across the entire AI product lifecycle, particularly in light of privacy concerns and emerging regulations. It evaluates the environmental impact and cost-effectiveness of Cross-Silo FL compared to traditional centralized learning methods, highlighting the carbon emissions associated with both approaches.",
      "takeaways": [
        "- Federated Learning, especially in a Cross-Silo context, enhances privacy by allowing model updates to be shared instead of raw data.",
        "- The research provides a quantitative framework for assessing the sustainability of Cross-Silo FL throughout the AI product lifecycle.",
        "- The findings suggest that while energy consumption is similar between Cross-Silo and Centralized Learning, Centralized Learning incurs additional CO2 emissions due to data transfer and storage requirements."
      ]
    },
    {
      "id": 74,
      "title": "Large Language Models are In-Context Molecule Learners",
      "link": "https://arxiv.org/abs/2403.04197",
      "description": "arXiv:2403.04197v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve similar informative context examples. Additionally, Post-retrieval Re-ranking is composed of Sequence Reversal and Random Walk selection to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context learning and reasoning capability of LLMs with the retrieved examples and adapts the parameters of LLMs for better alignment between molecules and texts. Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.",
      "date": "2025-04-02",
      "authors": "Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a new paradigm called In-Context Molecule Adaptation (ICMA) for Large Language Models (LLMs) that enables them to learn the alignment between molecular structures and natural language text from context examples, improving their performance in biochemical tasks without requiring extensive pre-training. The proposed method includes stages for retrieving relevant context examples and enhances the LLMs' capabilities for in-context learning and reasoning.",
      "takeaways": [
        "- ICMA allows LLMs to learn molecule-text alignment effectively without extra domain-specific pre-training.",
        "- The method employs innovative techniques for context retrieval and re-ranking to improve example selection.",
        "- Experimental results indicate that LLMs can achieve state-of-the-art performance in biochemical tasks through this new adaptation approach."
      ]
    },
    {
      "id": 75,
      "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
      "link": "https://arxiv.org/abs/2406.15877",
      "description": "arXiv:2406.15877v4 Announce Type: replace-cross \nAbstract: Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks ranging from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. Fulfilling both of these characteristics can pose a great challenge for LLMs.To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area.",
      "date": "2025-04-02",
      "authors": "Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, Binyuan Hui, Niklas Muennighoff, David Lo, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces BigCodeBench, a new benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in solving complex programming tasks that require calling diverse functions and understanding intricate instructions. The findings reveal that LLMs struggle with these challenges compared to human developers, highlighting the need for further advancements in their reasoning and program-calling abilities.",
      "takeaways": [
        "- BigCodeBench consists of 1,140 fine-grained tasks spanning 139 libraries and 7 domains, aimed at assessing LLMs' effectiveness in practical code generation.",
        "- LLMs scored an average of 60% in their ability to follow complex instructions and utilize function calls, significantly lower than the 97% performance of human developers.",
        "- The study emphasizes the importance of improving compositional reasoning in LLMs to enhance their capabilities in task automation and software development."
      ]
    },
    {
      "id": 76,
      "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals",
      "link": "https://arxiv.org/abs/2407.14561",
      "description": "arXiv:2407.14561v4 Announce Type: replace-cross \nAbstract: We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/.",
      "date": "2025-04-02",
      "authors": "Jaden Fiotto-Kaufman, Alexander R. Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla Brodley, Arjun Guha, Jonathan Bell, Byron C. Wallace, David Bau",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces NNsight and NDIF, tools designed to facilitate research into the internal workings of large neural networks by allowing for shared GPU resources and pretrained models through an open-source system that enhances PyTorch. The framework addresses the knowledge gap in studying large-scale AI internals and provides benchmarks for its effectiveness compared to previous methods.",
      "takeaways": [
        "- NNsight enables deferred remote execution for large neural networks, promoting collaborative research.",
        "- NDIF offers a scalable inference service for efficiently accessing and utilizing GPU resources and pretrained models.",
        "- The framework addresses the growing need for understanding the internals of large AI models, with supporting benchmarks demonstrating its performance advantages."
      ]
    },
    {
      "id": 77,
      "title": "Non-Determinism of \"Deterministic\" LLM Settings",
      "link": "https://arxiv.org/abs/2408.04667",
      "description": "arXiv:2408.04667v4 Announce Type: replace-cross \nAbstract: LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs under settings expected to be deterministic. Yet the questions of how pervasive this is, and with what impact on results, have not to our knowledge been systematically investigated. We investigate non-determinism in five LLMs configured to be deterministic when applied to eight common tasks in across 10 runs, in both zero-shot and few-shot settings. We see accuracy variations up to 15% across naturally occurring runs with a gap of best possible performance to worst possible performance up to 70%. In fact, none of the LLMs consistently delivers repeatable accuracy across all tasks, much less identical output strings. Sharing preliminary results with insiders has revealed that non-determinism perhaps essential to the efficient use of compute resources via co-mingled data in input buffers so this issue is not going away anytime soon. To better quantify our observations, we introduce metrics focused on quantifying determinism, TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement rate of parsed-out answers. Our code and data are publicly available at http://github.com/REDACTED.",
      "date": "2025-04-02",
      "authors": "Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, Breck Baldwin",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article investigates the non-determinism observed in large language models (LLMs) that are expected to operate deterministically. The study reveals substantial variations in output accuracy across multiple runs, emphasizing the challenges of achieving consistent results in AI applications.",
      "takeaways": [
        "- Non-determinism in LLMs can lead to accuracy variations of up to 15% across runs.",
        "- The gap between best and worst performance can reach 70%, highlighting significant inconsistencies in outputs.",
        "- The introduction of new metrics, TARr@N and TARa@N, aims to quantify and understand determinism in AI models."
      ]
    },
    {
      "id": 78,
      "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
      "link": "https://arxiv.org/abs/2410.09437",
      "description": "arXiv:2410.09437v3 Announce Type: replace-cross \nAbstract: Parameter-efficient fine-tuning (PEFT) has been widely employed for domain adaptation, with LoRA being one of the most prominent methods due to its simplicity and effectiveness. However, in multi-task learning (MTL) scenarios, LoRA tends to obscure the distinction between tasks by projecting sparse high-dimensional features from different tasks into the same dense low-dimensional intrinsic space. This leads to task interference and suboptimal performance for LoRA and its variants. To tackle this challenge, we propose MTL-LoRA, which retains the advantages of low-rank adaptation while significantly enhancing MTL capabilities. MTL-LoRA augments LoRA by incorporating additional task-adaptive parameters that differentiate task-specific information and capture shared knowledge across various tasks within low-dimensional spaces. This approach enables pre-trained models to jointly adapt to different target domains with a limited number of trainable parameters. Comprehensive experimental results, including evaluations on public academic benchmarks for natural language understanding, commonsense reasoning, and image-text understanding, as well as real-world industrial text Ads relevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants with comparable or even fewer learnable parameters in MTL setting.",
      "date": "2025-04-02",
      "authors": "Yaming Yang, Dilxat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Denvy Deng, Feng Sun, Qi Zhang, Weizhu Chen, Yunhai Tong",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces MTL-LoRA, an enhanced low-rank adaptation method for multi-task learning that addresses task interference by incorporating task-adaptive parameters, allowing for more effective model fine-tuning across different tasks. The proposed method demonstrates superior performance on various benchmarks while maintaining a low number of trainable parameters.",
      "takeaways": [
        "- MTL-LoRA retains the advantages of low-rank adaptation while improving capabilities in multi-task learning scenarios.",
        "- The method successfully differentiates task-specific information and captures shared knowledge across tasks, reducing task interference.",
        "- Experimental results show that MTL-LoRA outperforms traditional LoRA approaches on multiple academic and industrial datasets."
      ]
    },
    {
      "id": 79,
      "title": "MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba",
      "link": "https://arxiv.org/abs/2411.03855",
      "description": "arXiv:2411.03855v3 Announce Type: replace-cross \nAbstract: An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State Space Model (SSM)-based model, has attracted attention as a potential alternative to Transformers. While many large-scale Mamba-based models have been proposed, efficiently adapting pre-trained Mamba-based models to downstream tasks remains unexplored. In this paper, we conduct an exploratory analysis of PEFT methods for Mamba. We investigate the effectiveness of existing PEFT methods for Transformers when applied to Mamba. We also modify these methods to better align with the Mamba architecture. Additionally, we propose new Mamba-specific PEFT methods that leverage the distinctive structure of Mamba. Our experiments indicate that PEFT performs more effectively for Mamba than Transformers. Lastly, we demonstrate how to effectively combine multiple PEFT methods and provide a framework that outperforms previous works. To ensure reproducibility, we will release the code after publication.",
      "date": "2025-04-02",
      "authors": "Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses parameter-efficient fine-tuning (PEFT) methods for the Mamba model, a State Space Model-based alternative to Transformers. It explores the effectiveness of adapting existing PEFT strategies for Transformers to the Mamba architecture, proposing tailored methods that enhance performance in downstream tasks.",
      "takeaways": [
        "- The study reveals that PEFT can achieve better performance for the Mamba model compared to traditional Transformer models.",
        "- New Mamba-specific PEFT methods are introduced, leveraging its unique architecture to improve fine-tuning efficiency.",
        "- A comprehensive framework combining multiple PEFT strategies is proposed, which outperforms earlier approaches."
      ]
    },
    {
      "id": 80,
      "title": "TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG",
      "link": "https://arxiv.org/abs/2412.05447",
      "description": "arXiv:2412.05447v2 Announce Type: replace-cross \nAbstract: Retrieval-Augmented Generation (RAG) is one of the leading and most widely used techniques for enhancing LLM retrieval capabilities, but it still faces significant limitations in commercial use cases. RAG primarily relies on the query-chunk text-to-text similarity in the embedding space for retrieval and can fail to capture deeper semantic relationships across chunks, is highly sensitive to chunking strategies, and is prone to hallucinations. To address these challenges, we propose TOBUGraph, a graph-based retrieval framework that first constructs the knowledge graph from unstructured data dynamically and automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse relationships among data, going beyond RAG's text-to-text similarity. Retrieval is achieved through graph traversal, leveraging the extracted relationships and structures to enhance retrieval accuracy, eliminating the need for chunking configurations while reducing hallucination. We demonstrate TOBUGraph's effectiveness in TOBU, a real-world application in production for personal memory organization and retrieval. Our evaluation using real user data demonstrates that TOBUGraph outperforms multiple RAG implementations in both precision and recall, significantly improving user experience through improved retrieval accuracy.",
      "date": "2025-04-02",
      "authors": "Savini Kashmira, Jayanaka L. Dantanarayana, Joshua Brodsky, Ashish Mahendra, Yiping Kang, Krisztian Flautner, Lingjia Tang, Jason Mars",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces TOBUGraph, a novel graph-based retrieval framework designed to enhance the performance of retrieval-augmented generation (RAG) for large language models (LLMs). By constructing knowledge graphs from unstructured data and employing graph traversal for retrieval, TOBUGraph addresses limitations found in traditional RAG techniques, improving accuracy and user experience.",
      "takeaways": [
        "- TOBUGraph enhances retrieval capabilities of LLMs beyond traditional text-to-text similarity methods by using a graph-based approach.",
        "- The framework dynamically constructs knowledge graphs to capture deeper semantic relationships and reduce issues related to chunking strategies and hallucinations.",
        "- Evaluation results demonstrate that TOBUGraph significantly outperforms existing RAG implementations in retrieval precision and recall, leading to improved user experiences in applications such as personal memory organization."
      ]
    },
    {
      "id": 81,
      "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
      "link": "https://arxiv.org/abs/2412.09078",
      "description": "arXiv:2412.09078v5 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency. Code will be available at https://github.com/iamhankai/Forest-of-Thought.",
      "date": "2025-04-02",
      "authors": "Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces the Forest-of-Thought (FoT) framework, a novel approach that enhances the reasoning abilities of Large Language Models (LLMs) by employing multiple reasoning trees for collective decision-making, allowing for improved accuracy and efficiency when solving complex logical problems. It also presents a self-correction strategy and consensus-guided decision-making to optimize performance.",
      "takeaways": [
        "- The FoT framework integrates multiple reasoning paths, improving LLMs' ability to handle complex reasoning tasks through collective decision-making.",
        "- Sparse activation strategies are utilized to enhance efficiency by selecting the most relevant reasoning paths.",
        "- A dynamic self-correction strategy allows LLMs to correct mistakes in real-time, enhancing their overall accuracy and effectiveness in problem-solving."
      ]
    },
    {
      "id": 82,
      "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection",
      "link": "https://arxiv.org/abs/2412.11923",
      "description": "arXiv:2412.11923v2 Announce Type: replace-cross \nAbstract: In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations.",
      "date": "2025-04-02",
      "authors": "Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine Bosselut",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents PICLe, a novel framework for in-context learning with pseudo-annotated demonstrations that enhances Named Entity Detection (NED) in low-resource settings. By leveraging Large Language Models to generate noisy demonstrations, PICLe achieves significant performance improvements on biomedical NED tasks without requiring human annotations.",
      "takeaways": [
        "- PICLe demonstrates that partially correct annotations can be as effective as fully correct ones for in-context learning.",
        "- The framework allows for zero human annotation in Named Entity Detection, making it suitable for low-resource environments.",
        "- PICLe outperforms traditional in-context learning methods, showing the potential of utilizing synthetic demonstrations for task adaptation in biomedical applications."
      ]
    },
    {
      "id": 83,
      "title": "Provably-Safe Neural Network Training Using Hybrid Zonotope Reachability Analysis",
      "link": "https://arxiv.org/abs/2501.13023",
      "description": "arXiv:2501.13023v2 Announce Type: replace-cross \nAbstract: Even though neural networks are being increasingly deployed in safety-critical control applications, it remains difficult to enforce constraints on their output, meaning that it is hard to guarantee safety in such settings. While many existing methods seek to verify a neural network's satisfaction of safety constraints, few address how to correct an unsafe network. The handful of works that extract a training signal from verification cannot handle non-convex sets, and are either conservative or slow. To begin addressing these challenges, this work proposes a neural network training method that can encourage the exact image of a non-convex input set for a neural network with rectified linear unit (ReLU) nonlinearities to avoid a non-convex unsafe region. This is accomplished by reachability analysis with scaled hybrid zonotopes, a modification of the existing hybrid zonotope set representation that enables parameterized scaling of non-convex polytopic sets with a differentiable collision check via mixed-integer linear programs (MILPs). The proposed method was shown to be effective and fast for networks with up to 240 neurons, with the computational complexity dominated by inverse operations on matrices that scale linearly in size with the number of neurons and complexity of input and unsafe sets. We demonstrate the practicality of our method by training a forward-invariant neural network controller for a non-convex input set to an affine system, as well as generating safe reach-avoid plans for a black-box dynamical system.",
      "date": "2025-04-02",
      "authors": "Long Kiu Chung, Shreyas Kousik",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a novel method for training neural networks in safety-critical applications by using hybrid zonotope reachability analysis. This approach addresses the challenges of ensuring neural networks avoid unsafe regions during training while maintaining efficiency, even for complex, non-convex input sets.",
      "takeaways": [
        "- The proposed method integrates reachability analysis and parameterized scaling to effectively train neural networks with ReLU nonlinearities while ensuring safety constraints are met.",
        "- It offers a practical solution for training controllers in non-convex domains, showcasing rapid performance with neural networks that have a considerable number of neurons.",
        "- The application of the technique demonstrates its utility in generating safe plans for black-box dynamical systems, indicating potential relevance to AI applications in safety-critical environments, such as in the pharmaceutical industry."
      ]
    },
    {
      "id": 84,
      "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
      "link": "https://arxiv.org/abs/2501.14577",
      "description": "arXiv:2501.14577v3 Announce Type: replace-cross \nAbstract: Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing the existing top-$k$ attention method from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging \\textbf{Z}-Order Curves for \\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention, to enable parallel querying of past tokens for entire sequences. % in both space and time complexity of $\\mathcal{O}(N \\log N)$. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leverage $Z$-order curves to map low-dimensional keys and queries into \\emph{one}-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA matches the performance of standard attention on the synthetic \\textsc{Multi-Query Associative Recall} task and outperforms attention and its variants on \\textsc{Long Range Arena} and \\textsc{WikiText-103} language modeling.",
      "date": "2025-04-02",
      "authors": "Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces ZETA, a new approach utilizing Z-order curves to enhance the efficiency of top-$k$ attention in Transformer architectures, addressing the computational challenges posed by self-attention mechanisms during sequence modeling. This technique improves the performance and efficiency of querying past tokens for long sequences while achieving comparable results to standard attention mechanisms.",
      "takeaways": [
        "- ZETA allows for parallel querying of tokens in self-attention, reducing computational costs for long sequences.",
        "- The model achieves an efficiency of $\\mathcal{O}(N \\log N)$ in both space and time complexity for top-$k$ token selection.",
        "- Experimental results indicate that ZETA performs on par with traditional attention methods while surpassing them in specific language modeling tasks."
      ]
    },
    {
      "id": 85,
      "title": "BounTCHA: A CAPTCHA Utilizing Boundary Identification in Guided Generative AI-extended Videos",
      "link": "https://arxiv.org/abs/2501.18565",
      "description": "arXiv:2501.18565v3 Announce Type: replace-cross \nAbstract: In recent years, the rapid development of artificial intelligence (AI) especially multi-modal Large Language Models (MLLMs), has enabled it to understand text, images, videos, and other multimedia data, allowing AI systems to execute various tasks based on human-provided prompts. However, AI-powered bots have increasingly been able to bypass most existing CAPTCHA systems, posing significant security threats to web applications. This makes the design of new CAPTCHA mechanisms an urgent priority. We observe that humans are highly sensitive to shifts and abrupt changes in videos, while current AI systems still struggle to comprehend and respond to such situations effectively. Based on this observation, we design and implement BounTCHA, a CAPTCHA mechanism that leverages human perception of boundaries in video transitions and disruptions. By utilizing generative AI's capability to extend original videos with prompts, we introduce unexpected twists and changes to create a pipeline for generating guided short videos for CAPTCHA purposes. We develop a prototype and conduct experiments to collect data on humans' time biases in boundary identification. This data serves as a basis for distinguishing between human users and bots. Additionally, we perform a detailed security analysis of BounTCHA, demonstrating its resilience against various types of attacks. We hope that BounTCHA will act as a robust defense, safeguarding millions of web applications in the AI-driven era.",
      "date": "2025-04-02",
      "authors": "Lehao Lin, Ke Wang, Maha Abdallah, Wei Cai",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article introduces BounTCHA, a novel CAPTCHA system designed to utilize human perception of boundary identification in videos, aiming to prevent AI-powered bots from bypassing CAPTCHA defenses. It leverages generative AI to create dynamic video content that challenges users in ways current AI systems cannot easily handle, thus enhancing security for web applications.",
      "takeaways": [
        "- BounTCHA harnesses human sensitivity to video transitions to differentiate between human users and bots.",
        "- The system utilizes generative AI to create unexpected twists in short videos for CAPTCHA challenges.",
        "- A detailed security analysis demonstrates BounTCHA's robustness against various attack methods, potentially improving security across web applications."
      ]
    },
    {
      "id": 86,
      "title": "Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning",
      "link": "https://arxiv.org/abs/2502.03717",
      "description": "arXiv:2502.03717v2 Announce Type: replace-cross \nAbstract: Expressive robotic behavior is essential for the widespread acceptance of robots in social environments. Recent advancements in learned legged locomotion controllers have enabled more dynamic and versatile robot behaviors. However, determining the optimal behavior for interactions with different users across varied scenarios remains a challenge. Current methods either rely on natural language input, which is efficient but low-resolution, or learn from human preferences, which, although high-resolution, is sample inefficient. This paper introduces a novel approach that leverages priors generated by pre-trained LLMs alongside the precision of preference learning. Our method, termed Language-Guided Preference Learning (LGPL), uses LLMs to generate initial behavior samples, which are then refined through preference-based feedback to learn behaviors that closely align with human expectations. Our core insight is that LLMs can guide the sampling process for preference learning, leading to a substantial improvement in sample efficiency. We demonstrate that LGPL can quickly learn accurate and expressive behaviors with as few as four queries, outperforming both purely language-parameterized models and traditional preference learning approaches. Website with videos: https://lgpl-gaits.github.io/",
      "date": "2025-04-02",
      "authors": "Jaden Clark, Joey Hejna, Dorsa Sadigh",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents a novel approach called Language-Guided Preference Learning (LGPL), which combines insights from pre-trained large language models (LLMs) with preference learning to efficiently generate expressive robotic behaviors for quadrupeds. The method enhances sample efficiency and allows for rapid learning of behaviors that align with human expectations through minimal interaction.",
      "takeaways": [
        "- The proposed LGPL method improves sample efficiency by using LLMs to guide the sampling process for preference-based feedback.",
        "- The approach can learn accurate and expressive robotic behaviors with as few as four queries, outperforming traditional methods.",
        "- This research highlights the potential for integrating natural language processing techniques with robotics to enhance human-robot interactions."
      ]
    },
    {
      "id": 87,
      "title": "On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series",
      "link": "https://arxiv.org/abs/2502.12226",
      "description": "arXiv:2502.12226v2 Announce Type: replace-cross \nAbstract: Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. We evaluate our approach to the stock price prediction problem, a well-studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework effectively assess the robustness of FMTS and also offer actionable insights for model selection and deployment. Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and, (2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.",
      "date": "2025-04-02",
      "authors": "Kausik Lakkaraju, Rachneet Kaur, Parisa Zehtabi, Sunandita Patra, Siva Likitha Valluru, Zhen Zeng, Biplav Srivastava, Marco Valtorta",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article proposes a causally grounded rating framework to assess the robustness of Foundation Models for Time Series (FMTS) in forecasting stock prices. It evaluates the performance of various FMTS against input disturbances and offers insights into model selection for improved robustness and accuracy.",
      "takeaways": [
        "- A causally grounded rating framework is developed to evaluate the robustness of FMTS against input perturbations in time series forecasting.",
        "- Multi-modal FMTS show superior robustness and accuracy compared to uni-modal FMTS, indicating the benefits of incorporating diverse data types.",
        "- A user study confirms that the proposed ratings help reduce the difficulty users face in comparing the robustness of different forecasting models."
      ]
    },
    {
      "id": 88,
      "title": "Class-Dependent Perturbation Effects in Evaluating Time Series Attributions",
      "link": "https://arxiv.org/abs/2502.17022",
      "description": "arXiv:2502.17022v2 Announce Type: replace-cross \nAbstract: As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.",
      "date": "2025-04-02",
      "authors": "Gregor Baer, Isel Grau, Chao Zhang, Pieter Van Gorp",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article explores the effects of class-dependent metrics on perturbation-based evaluation of feature attributions in machine learning models for time series classification. It highlights the importance of understanding these effects for more accurate model interpretation, proposing a new evaluation framework that incorporates a class-aware penalty term.",
      "takeaways": [
        "- Class-dependent effects significantly impact the evaluation of feature attribution methods, revealing varied effectiveness across different classes.",
        "- The study shows that the best perturbation strategies can have pronounced differences depending on the classifier's learned biases.",
        "- A new evaluation framework is proposed, which could enhance the assessment of feature attributions by accounting for class imbalances in datasets."
      ]
    },
    {
      "id": 89,
      "title": "AI-Powered Bayesian Inference",
      "link": "https://arxiv.org/abs/2502.19231",
      "description": "arXiv:2502.19231v2 Announce Type: replace-cross \nAbstract: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.",
      "date": "2025-04-02",
      "authors": "Veronika Ro\\v{c}kov\\'a, Sean O'Hagan",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses a novel approach to Bayesian inference that integrates generative artificial intelligence (GAI) to construct prior distributions based on AI predictions, facilitating more effective decision-making and uncertainty quantification. It proposes a non-parametric Bayesian framework that utilizes both real and AI-generated data to enhance predictive analysis.",
      "takeaways": [
        "- The integration of GAI allows for leveraging varying AI responses to create informative prior distributions in Bayesian analysis.",
        "- The proposed method enhances predictive modeling through a coherent probabilistic framework that combines observed and AI-generated data.",
        "- This approach emphasizes the potential of AI in improving decision-making processes and uncertainty quantification in statistical analyses."
      ]
    },
    {
      "id": 90,
      "title": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach",
      "link": "https://arxiv.org/abs/2503.01141",
      "description": "arXiv:2503.01141v2 Announce Type: replace-cross \nAbstract: Chain-of-thought prompting has emerged as a powerful technique for enabling large language models (LLMs) to solve complex reasoning tasks. However, these reasoning chains can be verbose, raising concerns about efficiency. In response, recent works have sought to decrease response lengths through simple prompting strategies (e.g. 'be concise'). In this work, we conduct the first systematic study of the relationship between reasoning length and model performance across a diverse range of compression instructions (e.g. 'use 10 words or less' or 'remove all punctuation'). In doing so, we discover a universal tradeoff between reasoning length and accuracy that persists across even very distinct reasoning chains. We demonstrate that this tradeoff emerges from a sharp threshold behavior at the question level: each task has an intrinsic 'token complexity' - a minimal number of tokens required for successful problem-solving. We show how token complexity enables us to compute information-theoretic limits on the accuracy-compression tradeoff, and find that prompt-based compression strategies operate far from these theoretical limits. This suggests there may be significant room for improvement and our framework provides a benchmark to help researchers evaluate progress in reasoning efficiency. Our work also highlights the importance of adaptive compression -- giving shorter responses for easier questions -- and we show that token complexity is a useful tool for measuring this capability.",
      "date": "2025-04-02",
      "authors": "Ayeong Lee, Ethan Che, Tianyi Peng",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article explores the effectiveness of chain-of-thought prompting in large language models (LLMs) by investigating the balance between response length and reasoning accuracy. The authors introduce the concept of \"token complexity\" to analyze how different compression strategies impact model performance and identify room for improvement in reasoning efficiency.",
      "takeaways": [
        "- The research demonstrates a universal tradeoff between reasoning length and accuracy in LLMs, emphasizing the importance of concise yet effective responses.",
        "- Token complexity is introduced as a metric to determine the minimum number of tokens needed for problem-solving, providing insight into the efficiency of reasoning tasks.",
        "- The study shows that current prompting strategies may not fully optimize this tradeoff, indicating potential advancements for future model improvements in reasoning efficiency."
      ]
    },
    {
      "id": 91,
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "link": "https://arxiv.org/abs/2503.02130",
      "description": "arXiv:2503.02130v2 Announce Type: replace-cross \nAbstract: An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
      "date": "2025-04-02",
      "authors": "Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents the Forgetting Transformer (FoX), a new model that integrates a forget gate mechanism into the Transformer architecture, enhancing its performance on long-context language tasks while maintaining compatibility with existing algorithms. FoX demonstrates superior performance compared to standard Transformers in various language modeling scenarios.",
      "takeaways": [
        "- The Forgetting Attention mechanism allows for dynamic down-weighting of attention scores, improving the model's handling of long contexts.",
        "- FoX aligns well with the FlashAttention algorithm and eliminates the need for positional embeddings, simplifying the architecture.",
        "- Incorporating a \"Pro\" block design enhances the performance of both FoX and traditional Transformer models."
      ]
    },
    {
      "id": 92,
      "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
      "link": "https://arxiv.org/abs/2503.09445",
      "description": "arXiv:2503.09445v2 Announce Type: replace-cross \nAbstract: Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.",
      "date": "2025-04-02",
      "authors": "Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents Astrea, a novel Mixture-of-Experts (MoE) based Visual Language Model (VLM) designed to improve multimodal understanding by addressing the challenges of task heterogeneity and expert load imbalance. It introduces innovative mechanisms for expert coordination, dynamic knowledge fusion, and an enhanced optimization framework, showing significant performance improvements across various benchmark tasks.",
      "takeaways": [
        "- Astrea employs a heterogeneous expert coordination mechanism that integrates multiple specialized models to enhance visual comprehension.",
        "- The model features a dynamic knowledge fusion strategy with progressive pre-alignment, which helps harmonize the contributions of different experts.",
        "- Extensive evaluations indicate that Astrea outperforms state-of-the-art models, establishing new foundational methods for creating general-purpose multimodal AI agents, which could have implications for applications in the pharmaceutical industry."
      ]
    },
    {
      "id": 93,
      "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft prompt Optimization Approach",
      "link": "https://arxiv.org/abs/2503.13208",
      "description": "arXiv:2503.13208v2 Announce Type: replace-cross \nAbstract: Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs.",
      "date": "2025-04-02",
      "authors": "Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses a novel method called Dynamic Prompt Corruption (DPC) which aims to improve complex reasoning tasks in large language models (LLMs) by dynamically adjusting the influence of soft prompts. The method consists of two stages, leveraging experiments to show significant improvements in reasoning accuracy.",
      "takeaways": [
        "- DPC addresses the limitations of prompt-tuning in enhancing performance on complex reasoning tasks by adapting the use of soft prompts based on their impact.",
        "- The method involves a two-stage process: measuring the soft prompts' effectiveness and mitigating detrimental effects through selective masking.",
        "- Experimental results indicate that DPC can achieve 4%-8% accuracy gains over traditional prompt tuning, demonstrating its potential for improved reasoning in LLMs."
      ]
    },
    {
      "id": 94,
      "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data",
      "link": "https://arxiv.org/abs/2503.14538",
      "description": "arXiv:2503.14538v3 Announce Type: replace-cross \nAbstract: Background: This study introduces a Vision-Language Model (VLM) leveraging SIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB) screening. By integrating chest X-ray images and clinical notes, the model aims to enhance diagnostic accuracy and efficiency, particularly in resource-limited settings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context to generate detailed, context-aware diagnostic reports. The architecture employs SIGLIP for visual encoding and Gemma-3b for decoding, ensuring effective representation of acute TB-specific pathologies and clinical insights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and nodules, were detected with high precision (97percent) and recall (96percent). The model demonstrated strong spatial localization capabilities and robustness in distinguishing TB-positive cases, making it a reliable tool for acute TB diagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on radiologists, providing a scalable solution for acute TB screening. Future work will focus on improving the detection of subtle pathologies and addressing dataset biases to enhance its generalizability and application in diverse global healthcare settings.",
      "date": "2025-04-02",
      "authors": "Ananya Ganapthy, Praveen Shastry, Naveen Kumarasami, Anandakumar D, Keerthana R, Mounigasri M, Varshinipriya M, Kishore Prasath Venkatesh, Bargava Subramanian, Kalyan Sivasailam",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This study presents a Vision-Language Model that integrates chest X-ray images and clinical notes to enhance the accuracy and efficiency of acute tuberculosis diagnosis, particularly in resource-limited settings. The model demonstrates high precision in detecting key TB pathologies, aiming to reduce reliance on radiologists.",
      "takeaways": [
        "- The Vision-Language Model (VLM) combines imaging data and clinical information to generate context-aware diagnostic reports for tuberculosis.",
        "- The model achieved high precision (97%) and recall (96%) in detecting acute TB pathologies, showcasing strong diagnostic capabilities.",
        "- Future enhancements will focus on improving subtle pathology detection and addressing dataset biases for better global applicability."
      ]
    },
    {
      "id": 95,
      "title": "Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression",
      "link": "https://arxiv.org/abs/2503.18497",
      "description": "arXiv:2503.18497v2 Announce Type: replace-cross \nAbstract: Artificial intelligence models trained from data can only be as good as the underlying data is. Biases in training data propagating through to the output of a machine learning model are a well-documented and well-understood phenomenon, but the machinery to prevent these undesired effects is much less developed. Efforts to ensure data is clean during collection, such as using bias-aware sampling, are most effective when the entity controlling data collection also trains the AI. In cases where the data is already available, how do we find out if the data was already manipulated, i.e., ``poisoned'', so that an undesired behavior would be trained into a machine learning model? This is a challenge fundamentally different to (just) improving approximation accuracy or efficiency, and we provide a method to test training data for flaws, to establish a trustworthy ground-truth for a subsequent training of machine learning models (of any kind). Unlike the well-studied problem of approximating data using fuzzy rules that are generated from the data, our method hinges on a prior definition of rules to happen before seeing the data to be tested. Therefore, the proposed method can also discover hidden error patterns, which may also have substantial influence. Our approach extends the abilities of conventional statistical testing by letting the ``test-condition'' be any Boolean condition to describe a pattern in the data, whose presence we wish to determine. The method puts fuzzy inference into a regression model, to get the best of the two: explainability from fuzzy logic with statistical properties and diagnostics from the regression, and finally also being applicable to ``small data'', hence not requiring large datasets as deep learning methods do. We provide an open source implementation for demonstration and experiments.",
      "date": "2025-04-02",
      "authors": "Stefan Rass, Martin Dallinger",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a novel method for statistically testing training data for unwanted error patterns through rule-oriented regression, addressing the issue of data poisoning in machine learning. This approach aims to enhance the reliability of AI models by allowing the detection of hidden flaws in training data before model training begins.",
      "takeaways": [
        "- The proposed method improves the process of identifying and mitigating biases in training data that could negatively affect AI model performance.",
        "- It integrates fuzzy logic and statistical regression, providing both explainability and diagnostic capabilities, which can be particularly useful for smaller datasets.",
        "- An open-source implementation of the method is provided, enabling further experimentation and demonstration of its effectiveness."
      ]
    },
    {
      "id": 96,
      "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities",
      "link": "https://arxiv.org/abs/2503.22517",
      "description": "arXiv:2503.22517v2 Announce Type: replace-cross \nAbstract: In this work, we undertake the challenge of augmenting the existing generative capabilities of pre-trained text-only large language models (LLMs) with multi-modal generation capability while satisfying two core constraints: C1 preserving the preservation of original language generative capabilities with negligible performance degradation, and C2 adhering to a small parameter budget to learn the new modality, ensuring scalability and efficiency. In contrast to current approaches that add dedicated modules, thereby significantly increasing the parameter count, we propose a method that leverages the underutilized capacity inherent in deep models. Specifically, we exploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source of additional capacity for learning a new modality, enabling better parameter efficiency (C1). Moreover, we preserve the original language generation capabilities by applying low-rank adaptation exclusively to the tokens of the new modality (C2). Furthermore, we introduce a novel parameter initialization scheme based on the Gromov-Wasserstein distance to improve convergence and training stability. Through an extensive analysis of the routing mechanism, we uncover the emergence of modality-specific pathways and decreased redundancy within the experts that can efficiently unlock multi-modal generative capabilities. Overall, our method can be seamlessly applied to a wide range of contemporary LLMs, providing a new pathway for transitioning from uni-modal to multi-modal architectures.",
      "date": "2025-04-02",
      "authors": "Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, Steven McDonagh, Sarah Parisot",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "This article presents a method to enhance the generative capabilities of pre-trained text-only large language models (LLMs) by enabling multi-modal generation without significantly increasing the parameter count. The approach utilizes the unused parameter capacity in Mixture-of-Experts (MoEs) to learn new modalities efficiently while preserving the model's original language processing capabilities.",
      "takeaways": [
        "- The proposed method uses parameter redundancy in Mixture-of-Experts to facilitate multi-modal generation in LLMs, enhancing scalability and efficiency.",
        "- It ensures that the original generative capabilities of the language model are maintained during the transition to multi-modal capabilities.",
        "- A novel parameter initialization technique based on the Gromov-Wasserstein distance is introduced, enhancing convergence and training stability for integrating new modalities."
      ]
    },
    {
      "id": 97,
      "title": "DC-SGD: Differentially Private SGD with Dynamic Clipping through Gradient Norm Distribution Estimation",
      "link": "https://arxiv.org/abs/2503.22988",
      "description": "arXiv:2503.22988v2 Announce Type: replace-cross \nAbstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a widely adopted technique for privacy-preserving deep learning. A critical challenge in DP-SGD is selecting the optimal clipping threshold C, which involves balancing the trade-off between clipping bias and noise magnitude, incurring substantial privacy and computing overhead during hyperparameter tuning.\n  In this paper, we propose Dynamic Clipping DP-SGD (DC-SGD), a framework that leverages differentially private histograms to estimate gradient norm distributions and dynamically adjust the clipping threshold C. Our framework includes two novel mechanisms: DC-SGD-P and DC-SGD-E. DC-SGD-P adjusts the clipping threshold based on a percentile of gradient norms, while DC-SGD-E minimizes the expected squared error of gradients to optimize C. These dynamic adjustments significantly reduce the burden of hyperparameter tuning C. The extensive experiments on various deep learning tasks, including image classification and natural language processing, show that our proposed dynamic algorithms achieve up to 9 times acceleration on hyperparameter tuning than DP-SGD. And DC-SGD-E can achieve an accuracy improvement of 10.62% on CIFAR10 than DP-SGD under the same privacy budget of hyperparameter tuning. We conduct rigorous theoretical privacy and convergence analyses, showing that our methods seamlessly integrate with the Adam optimizer. Our results highlight the robust performance and efficiency of DC-SGD, offering a practical solution for differentially private deep learning with reduced computational overhead and enhanced privacy guarantees.",
      "date": "2025-04-02",
      "authors": "Chengkun Wei, Weixian Li, Chen Gong, Wenzhi Chen",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents Dynamic Clipping DP-SGD (DC-SGD), a novel framework for differential privacy in Stochastic Gradient Descent that dynamically adjusts the clipping threshold for gradients. This approach significantly reduces the computational burden of hyperparameter tuning while improving accuracy and maintaining privacy in deep learning tasks.",
      "takeaways": [
        "- DC-SGD utilizes differentially private histograms to estimate gradient norm distributions for dynamic clipping, enhancing efficiency in deep learning training.",
        "- The proposed methods show up to 9 times acceleration in hyperparameter tuning compared to conventional DP-SGD techniques.",
        "- Rigorous theoretical analyses confirm the framework's robustness in terms of privacy and integration with the Adam optimizer, making it a practical solution for privacy-preserving applications in AI."
      ]
    },
    {
      "id": 98,
      "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
      "link": "https://arxiv.org/abs/2503.23157",
      "description": "arXiv:2503.23157v2 Announce Type: replace-cross \nAbstract: Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.",
      "date": "2025-04-02",
      "authors": "Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan \"O. Arik",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents a novel reinforcement learning approach designed for the Text-to-SQL task, focusing on the use of tailored partial rewards to enhance reasoning capabilities in generating SQL queries. The proposed method has shown to outperform existing models in terms of accuracy and generalization.",
      "takeaways": [
        "- The introduction of tailored partial rewards in reinforcement learning effectively addresses reward sparsity in the Text-to-SQL task.",
        "- The group relative policy optimization (GRPO) technique encourages language models to develop intrinsic reasoning skills essential for accurate SQL generation.",
        "- The proposed RL-trained model achieved higher accuracy compared to larger proprietary models on the BIRD benchmark, demonstrating significant advancements in reasoning-enhanced capabilities."
      ]
    },
    {
      "id": 99,
      "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation",
      "link": "https://arxiv.org/abs/2503.23764",
      "description": "arXiv:2503.23764v2 Announce Type: replace-cross \nAbstract: Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient wavelet-based summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity.",
      "date": "2025-04-02",
      "authors": "Md Mahfuz Al Hasan, Mahdi Zaman, Abdul Jawad, Alberto Santamaria-Pang, Ho Hin Lee, Ivan Tarapov, Kyle See, Md Shah Imran, Antika Roy, Yaser Pourmohammadi Fallah, Navid Asadizanjani, Reza Forghani",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article discusses WaveFormer, a new 3D Transformer architecture designed for medical image segmentation, which addresses the challenges of high memory overhead and fine-grained feature capture in 3D settings by utilizing wavelet transformations for efficiency and performance. Evaluations show that it performs comparably to state-of-the-art models while significantly reducing computational complexity.",
      "takeaways": [
        "- WaveFormer incorporates discrete wavelet transformations to improve contextual representation and maintain high-frequency detail in medical images.",
        "- The model is designed to be efficient, reducing the number of parameters needed and making it more suitable for real-world applications.",
        "- It demonstrates competitive performance on benchmark datasets while being adaptable to various medical imaging tasks."
      ]
    },
    {
      "id": 100,
      "title": "Learned Image Compression and Restoration for Digital Pathology",
      "link": "https://arxiv.org/abs/2503.23862",
      "description": "arXiv:2503.23862v2 Announce Type: replace-cross \nAbstract: Digital pathology images play a crucial role in medical diagnostics, but their ultra-high resolution and large file sizes pose significant challenges for storage, transmission, and real-time visualization. To address these issues, we propose CLERIC, a novel deep learning-based image compression framework designed specifically for whole slide images (WSIs). CLERIC integrates a learnable lifting scheme and advanced convolutional techniques to enhance compression efficiency while preserving critical pathological details. Our framework employs a lifting-scheme transform in the analysis stage to decompose images into low- and high-frequency components, enabling more structured latent representations. These components are processed through parallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent Residual Blocks (R2B) to improve feature extraction and spatial adaptability. The synthesis stage applies an inverse lifting transform for effective image reconstruction, ensuring high-fidelity restoration of fine-grained tissue structures. We evaluate CLERIC on a digital pathology image dataset and compare its performance against state-of-the-art learned image compression (LIC) models. Experimental results demonstrate that CLERIC achieves superior rate-distortion (RD) performance, significantly reducing storage requirements while maintaining high diagnostic image quality. Our study highlights the potential of deep learning-based compression in digital pathology, facilitating efficient data management and long-term storage while ensuring seamless integration into clinical workflows and AI-assisted diagnostic systems. Code and models are available at: https://github.com/pnu-amilab/CLERIC.",
      "date": "2025-04-02",
      "authors": "SeonYeong Lee, EonSeung Seong, DongEon Lee, SiYeoul Lee, Yubin Cho, Chunsu Park, Seonho Kim, MinKyung Seo, YoungSin Ko, MinWoo Kim",
      "journal": "arxiv.org",
      "therapyArea": "AI Publications",
      "term": "arXiv: Computer Science - AI",
      "summary": "The article presents CLERIC, a deep learning-based image compression framework specifically designed for digital pathology images, which addresses the challenges of high-resolution image storage and transmission while preserving diagnostic quality. CLERIC improves compression efficiency through advanced techniques and achieves superior rate-distortion performance compared to existing models.",
      "takeaways": [
        "- CLERIC utilizes a learnable lifting scheme and advanced convolutional techniques to optimize the compression of whole slide images in digital pathology.",
        "- The framework enhances feature extraction and spatial adaptability through the use of Deformable Residual Blocks (DRB) and Recurrent Residual Blocks (R2B).",
        "- By effectively reducing storage needs while maintaining image quality, CLERIC shows promise for integration into AI-assisted diagnostic systems, facilitating better data management in clinical settings."
      ]
    }
  ],
  "keyHighlights": {
    "summary": "The recent AI developments reveal a significant focus on enhancing generative models, improving reasoning capabilities, and ensuring ethical compliance through governance frameworks. Noteworthy advancements include the integration of large language models (LLMs) with multimodal capabilities, the enhancement of decision-making through reasoning models, and the use of AI in medical applications to improve diagnostics and operational efficiency. Additionally, diverse approaches to model fine-tuning and robust training methodologies showcase ongoing innovation aimed at optimizing performance across various applications.",
    "trends": [
      "- The rise of ethical AI and compliance mechanisms, such as Constitutional AI and Automated Reasoning checks, emphasizes the importance of responsible content generation and model reliability.",
      "- Integration of multimodal capabilities, exemplified by models that combine text, image, and real-world data, highlights the shift towards more complex AI systems capable of nuanced tasks across different domains.",
      "- Advancements in model fine-tuning methods like Low-Rank Adaptation and Mixture of Experts seek to optimize resource efficiency and performance for large-scale AI applications.",
      "- Increased emphasis on intelligent systems that facilitate real-time decision-making and improved user interaction, demonstrated through innovations such as conversational AI agents and interactive content generation tools.",
      "- Growing use of federated learning and privacy-preserving techniques in AI reflects the need to address data privacy concerns across sectors, especially in healthcare and sensitive environments."
    ],
    "technologies": [
      "- Large Language Models (LLMs) with enhanced reasoning capabilities and multimodal integration.",
      "- Federated Learning frameworks that support decentralized training while ensuring data privacy.",
      "- Novel training methodologies such as Dynamic Clipping in Stochastic Gradient Descent and Mixture-of-Experts architectures targeting efficient AI model performance."
    ],
    "therapy_area_distribution": {
      "Technical AI Updates": 12,
      "AI Marketing and Advertising": 3,
      "AI Publications": 86
    },
    "total_articles": 101
  }
}
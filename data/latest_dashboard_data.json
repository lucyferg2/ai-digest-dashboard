{
  "metadata": {
    "generatedAt": "2025-07-31 07:47:08",
    "year": 2025,
    "month": 7,
    "monthName": "July",
    "articleCount": 99,
    "newArticlesThisWeek": 47,
    "deduplicationApplied": true,
    "duplicatesRemoved": 22
  },
  "articles": [
    {
      "id": 0,
      "title": "AlphaEarth Foundations helps map our planet in unprecedented detail",
      "link": "https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/",
      "description": "New AI model integrates petabytes of Earth observation data to generate a unified data representation that revolutionizes global mapping and monitoring",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "deepmind.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google Deepmind​",
      "terms": [
        "Google Deepmind​"
      ],
      "summary": "The article discusses AlphaEarth Foundations' new AI model that integrates vast amounts of Earth observation data to create a comprehensive and detailed global map, representing a significant advancement in AI-driven environmental monitoring.",
      "takeaways": [
        "- Introduction of a novel AI model handling petabytes of data for unified Earth mapping.",
        "- Potential to revolutionize how global environmental changes and monitoring are conducted.",
        "- Demonstrates the expanding role of AI in processing and analyzing large-scale geospatial datasets."
      ]
    },
    {
      "id": 1,
      "title": "Introducing Stargate Norway",
      "link": "https://openai.com/index/introducing-stargate-norway",
      "description": "We’re launching Stargate Norway—OpenAI’s first AI data center initiative in Europe under our OpenAI for Countries program. Stargate is OpenAI’s overarching infrastructure platform and is a critical part of our long-term vision to deliver the benefits of AI to everyone.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "openai.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "OpenAI",
      "terms": [
        "OpenAI"
      ],
      "summary": "The article announces the launch of Stargate Norway, OpenAI’s first AI data center in Europe, as part of the OpenAI for Countries program. This infrastructure platform represents a significant step in OpenAI's strategy to expand AI accessibility and capabilities globally.",
      "takeaways": [
        "- Stargate Norway is OpenAI’s inaugural AI data center initiative in Europe.",
        "- The platform is a key component of OpenAI's long-term vision to democratize AI benefits.",
        "- This marks an important infrastructure advancement in AI deployment within Europe."
      ]
    },
    {
      "id": 2,
      "title": "Three lessons for creating a sustainable AI advantage",
      "link": "https://openai.com/index/intercom",
      "description": "Discover how Intercom built a scalable AI platform with 3 key lessons—from evaluations to architecture—to lead the future of customer support.",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "openai.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "OpenAI",
      "terms": [
        "OpenAI"
      ],
      "summary": "This article outlines three key lessons from Intercom’s experience in building a scalable AI platform, focusing on evaluations, architecture, and strategies to maintain a sustainable competitive advantage in AI for customer support. It provides insights into practical approaches for developing and leveraging AI technology effectively.",
      "takeaways": [
        "- Importance of thorough AI evaluations to ensure effectiveness and reliability.",
        "- Architectural considerations crucial for building scalable and maintainable AI platforms.",
        "- Strategic lessons on sustaining a competitive edge through continuous AI innovation."
      ]
    },
    {
      "id": 3,
      "title": "Automate the creation of handout notes using Amazon Bedrock Data Automation",
      "link": "https://aws.amazon.com/blogs/machine-learning/automate-the-creation-of-handout-notes-using-amazon-bedrock-data-automation/",
      "description": "Organizations across various sectors face significant challenges when converting meeting recordings or recorded presentations into structured documentation. The process of creating handouts from presentations requires lots of manual effort, such as reviewing recordings to identify slide transitions, transcribing spoken content, capturing and organizing screenshots, synchronizing visual elements with speaker notes, and formatting content. These challenges impact productivity and scalability, especially when dealing with multiple presentation recordings, conference sessions, training materials, and educational content.\nIn this post, we show how you can build an automated, serverless solution to transform webinar recordings into comprehensive handouts using Amazon Bedrock Data Automation for video analysis. We walk you through the implementation of Amazon Bedrock Data Automation to transcribe and detect slide changes, as well as the use of Amazon Bedrock foundation models (FMs) for transcription refinement, combined with custom AWS Lambda functions orchestrated by AWS Step Functions. Through detailed implementation details, architectural patterns, and code, you will learn how to build a workflow that automates the handout creation process.\nAmazon Bedrock Data Automation\nAmazon Bedrock Data Automation uses generative AI to automate the transformation of multimodal data (such as images, videos and more) into a customizable structured format. Examples of structured formats include summaries of scenes in a video, unsafe or explicit content in text and images, or organized content based on advertisements or brands. The solution presented in this post uses Amazon Bedrock Data Automation to extract audio segments and different shots in videos.\nSolution overview\nOur solution uses a serverless architecture orchestrated by Step Functions to process presentation recordings into comprehensive handouts. The workflow consists of the following steps:\n\nThe workflow begins when a video is uploaded to Amazon Simple Storage Service (Amazon S3), which triggers an event notification through Amazon EventBridge rules that initiates our video processing workflow in Step Functions.\nAfter the workflow is triggered, Amazon Bedrock Data Automation initiates a video transformation job to identify different shots in the video. In our case, this is represented by a change of slides. The workflow moves into a waiting state, and checks for the transformation job progress. If the job is in progress, the workflow returns to the waiting state. When the job is complete, the workflow continues, and we now have extracted both visual shots and spoken content.\nThese visual shots and spoken content feed into a synchronization step. In this Lambda function, we use the output of the Amazon Bedrock Data Automation job to match the spoken content to the correlating shots based on the matching of timestamps.\nAfter function has matched the spoken content to the visual shots, the workflow moves into a parallel state. One of the steps of this state is the generation of screenshots. We use a FFmpeg-enabled Lambda function to create images for each identified video shot.\nThe other step of the parallel state is the refinement of our transformations. Amazon Bedrock processes and improves each raw transcription section through a Map state. This helps us remove speech disfluencies and improve the sentence structure.\nLastly, after the screenshots and refined transcript are created, the workflow uses a Lambda function to create handouts. We use the Python-PPTX library, which generates the final presentation with synchronized content. These final handouts are stored in Amazon S3 for distribution.\n\nThe following diagram illustrates this workflow.\n\nIf you want to try out this solution, we have created an AWS Cloud Development Kit (AWS CDK) stack available in the accompanying GitHub repo that you can deploy in your account. It deploys the Step Functions state machine to orchestrate the creation of handout notes from the presentation video recording. It also provides you with a sample video to test out the results.\nTo deploy and test the solution in your own account, follow the instructions in the GitHub repository’s README file. The following sections describe in more detail the technical implementation details of this solution.\nVideo upload and initial processing\nThe workflow begins with Amazon S3, which serves as the entry point for our video processing pipeline. When a video is uploaded to a dedicated S3 bucket, it triggers an event notification that, through EventBridge rules, initiates our Step Functions workflow.\nShot detection and transcription using Amazon Bedrock Data Automation\nThis step uses Amazon Bedrock Data Automation to detect slide transitions and create video transcriptions. To integrate this as part of the workflow, you must create an Amazon Bedrock Data Automation project. A project is a grouping of output configurations. Each project can contain standard output configurations as well as custom output blueprints for documents, images, video, and audio. The project has already been created as part of the AWS CDK stack. After you set up your project, you can process content using the InvokeDataAutomationAsync API. In our solution, we use the Step Functions service integration to execute this API call and start the asynchronous processing job. A job ID is returned for tracking the process.\nThe workflow must now check the status of the processing job before continuing with the handout creation process. This is done by polling Amazon Bedrock Data Automation for the job status using the GetDataAutomationStatus API on a regular basis. Using a combination of the Step Functions Wait and Choice states, we can ask the workflow to poll the API on a fixed interval. This not only gives you the ability to customize the interval depending on your needs, but it also helps you control the workflow costs, because every state transition is billed in Standard workflows, which this solution uses.\nWhen the GetDataAutomationStatus API output shows as SUCCESS, the loop exits and the workflow continues to the next step, which will match transcripts to the visual shots.\nMatching audio segments with corresponding shots\nTo create comprehensive handouts, you must establish a mapping between the visual shots and their corresponding audio segments. This mapping is crucial to make sure the final handouts accurately represent both the visual content and the spoken narrative of the presentation.\nA shot represents a series of interrelated consecutive frames captured during the presentation, typically indicating a distinct visual state. In our presentation context, a shot corresponds to either a new slide or a significant slide animation that adds or modifies content.\nAn audio segment is a specific portion of an audio recording that contains uninterrupted spoken language, with minimal pauses or breaks. This segment captures a natural flow of speech. The Amazon Bedrock Data Automation output provides an audio_segments array, with each segment containing precise timing information such as the start and end time of each segment. This allows for accurate synchronization with the visual shots.\nThe synchronization between shots and audio segments is critical for creating accurate handouts that preserve the presentation’s narrative flow. To achieve this, we implement a Lambda function that manages the matching process in three steps:\n\nThe function retrieves the processing results from Amazon S3, which contains both the visual shots and audio segments.\nIt creates structured JSON arrays from these components, preparing them for the matching algorithm.\nIt executes a matching algorithm that analyzes the different timestamps of the audio segments and the shots, and matches them based on these timestamps. This algorithm also considers timestamp overlaps between shots and audio segments.\n\nFor each shot, the function examines audio segments and identifies those whose timestamps overlap with the shot’s duration, making sure the relevant spoken content is associated with its corresponding slide in the final handouts. The function returns the matched results directly to the Step Functions workflow, where it will serve as input for the next step, where Amazon Bedrock will refine the transcribed content and where we will create screenshots in parallel.\nScreenshot generation\nAfter you get the timestamps of each shot and associated audio segment, you can capture the slides of the presentation to create comprehensive handouts. Each detected shot from Amazon Bedrock Data Automation represents a distinct visual state in the presentation—typically a new slide or significant content change. By generating screenshots at these precise moments, we make sure our handouts accurately represent the visual flow of the original presentation.\nThis is done with a Lambda function using the ffmpeg-python library. This library acts as a Python binding for the FFmpeg media framework, so you can run FFmpeg terminal commands using Python methods. In our case, we can extract frames from the video at specific timestamps identified by Amazon Bedrock Data Automation. The screenshots are stored in an S3 bucket to be used in creating the handouts, as described in the following code. To use ffmpeg-python in Lambda, we created a Lambda ZIP deployment containing the required dependencies to run the code. Instructions on how to create the ZIP file can be found in our GitHub repository.\nThe following code shows how a screenshot is taken using ffmpeg-python. You can view the full Lambda code on GitHub.\n\n## Taking a screenshot at a specific timestamp \nffmpeg.input(video_path, ss=timestamp).output(screenshot_path, vframes=1).run()\n\nTranscript refinement with Amazon Bedrock\nIn parallel with the screenshot generation, we refine the transcript using a large language model (LLM). We do this to improve the quality of the transcript and filter out errors and speech disfluencies. This process uses an Amazon Bedrock model to enhance the quality of the matched transcription segments while maintaining content accuracy. We use a Lambda function that integrates with Amazon Bedrock through the Python Boto3 client, using a prompt to guide the model’s refinement process. The function can then process each transcript segment, instructing the model to do the following:\n\nFix typos and grammatical errors\nRemove speech disfluencies (such as “uh” and “um”)\nMaintain the original meaning and technical accuracy\nPreserve the context of the presentation\n\nIn our solution, we used the following prompt with three example inputs and outputs:\n\nprompt = '''This is the result of a transcription. \nI want you to look at this audio segment and fix the typos and mistakes present. \nFeel free to use the context of the rest of the transcript to refine (but don't leave out any info). \nLeave out parts where the speaker misspoke. \nMake sure to also remove works like \"uh\" or \"um\". \nOnly make change to the info or sentence structure when there are mistakes. \nOnly give back the refined transcript as output, don't add anything else or any context or title. \nIf there are no typos or mistakes, return the original object input. \nDo not explain why you have or have not made any changes; I just want the JSON object. \n\nThese are examples: \nInput: <an example-input> \nOutput: <an example-output>\n\nInput: <an example-input> \nOutput: <an example-output>\n\nInput: <an example-input> \nOutput: <an example-output>\n\nHere is the object: ''' + text\n\nThe following is an example input and output:\n\nInput: Yeah. Um, so let's talk a little bit about recovering from a ransomware attack, right?\n\nOutput: Yes, let's talk a little bit about recovering from a ransomware attack.\n\nTo optimize processing speed while adhering to the maximum token limits of the Amazon Bedrock InvokeModel API, we use the Step Functions Map state. This enables parallel processing of multiple transcriptions, each corresponding to a separate video segment. Because these transcriptions must be handled individually, the Map state efficiently distributes the workload. Additionally, it reduces operational overhead by managing integration—taking an array as input, passing each element to the Lambda function, and automatically reconstructing the array upon completion.The Map state returns the refined transcript directly to the Step Functions workflow, maintaining the structure of the matched segments while providing cleaner, more professional text content for the final handout generation.\nHandout generation\nThe final step in our workflow involves creating the handouts using the python-pptx library. This step combines the refined transcripts with the generated screenshots to create a comprehensive presentation document.\nThe Lambda function processes the matched segments sequentially, creating a new slide for each screenshot while adding the corresponding refined transcript as speaker notes. The implementation uses a custom Lambda layer containing the python-pptx package. To enable this functionality in Lambda, we created a custom layer using Docker. By using Docker to create our layer, we make sure the dependencies are compiled in an environment that matches the Lambda runtime. You can find the instructions to create this layer and the layer itself in our GitHub repository.\nThe Lambda function implementation uses python-pptx to create structured presentations:\n\nimport boto3\nfrom pptx import Presentation\nfrom pptx.util import Inches\nimport os\nimport json\n\ndef lambda_handler(event, context):\n    # Create new presentation with specific dimensions\n    prs = Presentation()\n    prs.slide_width = int(12192000)  # Standard presentation width\n    prs.slide_height = int(6858000)  # Standard presentation height\n    \n    # Process each segment\n    for i in range(num_images):\n        # Add new slide\n        slide = prs.slides.add_slide(prs.slide_layouts[5])\n        \n        # Add screenshot as full-slide image\n        slide.shapes.add_picture(image_path, 0, 0, width=slide_width)\n        \n        # Add transcript as speaker notes\n        notes_slide = slide.notes_slide\n        transcription_text = transcription_segments[i].get('transcript', '')\n        notes_slide.notes_text_frame.text = transcription_text\n    \n    # Save presentation\n    pptx_path = os.path.join(tmp_dir, \"lecture_notes.pptx\")\n    prs.save(pptx_path)\n\nThe function processes segments sequentially, creating a presentation that combines visual shots with their corresponding audio segments, resulting in handouts ready for distribution.\nThe following screenshot shows an example of a generated slide with notes. The full deck has been added as a file in the GitHub repository.\n\nConclusion\nIn this post, we demonstrated how to build a serverless solution that automates the creation of handout notes from recorded slide presentations. By combining Amazon Bedrock Data Automation with custom Lambda functions, we’ve created a scalable pipeline that significantly reduces the manual effort required in creating handout materials. Our solution addresses several key challenges in content creation:\n\nAutomated detection of slide transitions, content changes, and accurate transcription of spoken content using the video modality capabilities of Amazon Bedrock Data Automation\nIntelligent refinement of transcribed text using Amazon Bedrock\nSynchronized visual and textual content with a custom matching algorithm\nHandout generation using the ffmpeg-python and python-pptx libraries in Lambda\n\nThe serverless architecture, orchestrated by Step Functions, provides reliable execution while maintaining cost-efficiency. By using Python packages for FFmpeg and a Lambda layer for python-pptx, we’ve overcome technical limitations and created a robust solution that can handle various presentation formats and lengths. This solution can be extended and customized for different use cases, from educational institutions to corporate training programs. Certain steps such as the transcript refinement can also be improved, for instance by adding translation capabilities to account for diverse audiences.\nTo learn more about Amazon Bedrock Data Automation, refer to the following resources:\n\nTransform unstructured data into meaningful insights using Amazon Bedrock Data Automation\nNew Amazon Bedrock capabilities enhance data processing and retrieval\nSimplify multimodal generative AI with Amazon Bedrock Data Automation\nGuidance for Multimodal Data Processing Using Amazon Bedrock Data Automation\n\n\n\nAbout the authors\nLaura Verghote is the GenAI Lead for PSI Europe at Amazon Web Services (AWS), driving Generative AI adoption across public sector organizations. She partners with customers throughout Europe to accelerate their GenAI initiatives through technical expertise and strategic planning, bridging complex requirements with innovative AI solutions.\nElie Elmalem is a solutions architect at Amazon Web Services (AWS) and supports Education customers across the UK and EMEA. He works with customers to effectively use AWS services, providing architectural best practices, advice, and guidance. Outside of work, he enjoys spending time with family and friends and loves watching his favorite football team play.\n",
      "date": "2025-07-30",
      "authors": "Laura Verghote",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article presents a detailed guide on automating the creation of handout notes from recorded presentations using Amazon Bedrock Data Automation, generative AI, and serverless AWS services. It highlights how AI-driven video analysis and large language models refine transcriptions and synchronize content for scalable, efficient documentation in educational and corporate settings.",
      "takeaways": [
        "- Amazon Bedrock Data Automation uses generative AI to detect slide transitions, transcribe spoken content, and extract multimodal data from videos.",
        "- The workflow orchestrated by AWS Step Functions integrates AI models with custom Lambda functions to refine transcripts, generate screenshots, and create synchronized presentation handouts automatically.",
        "- The serverless architecture provides a scalable, cost-efficient solution that significantly reduces manual effort in transforming recorded presentations into structured, professional handouts."
      ]
    },
    {
      "id": 4,
      "title": "Streamline GitHub workflows with generative AI using Amazon Bedrock and MCP",
      "link": "https://aws.amazon.com/blogs/machine-learning/streamline-github-workflows-with-generative-ai-using-amazon-bedrock-and-mcp/",
      "description": "Customers are increasingly looking to use the power of large language models (LLMs) to solve real-world problems. However, bridging the gap between these LLMs and practical applications has been a challenge. AI agents have appeared as an innovative technology that bridges this gap.\nThe foundation models (FMs) available through Amazon Bedrock serve as the cognitive engine for AI agents, providing the reasoning and natural language understanding capabilities essential for interpreting user requests and generating appropriate responses. You can integrate these models with various agent frameworks and orchestration layers to create AI applications that can understand context, make decisions, and take actions. You can build with Amazon Bedrock Agents or other frameworks like LangGraph and the recently launched Strands Agent SDK.\nThis blog post explores how to create powerful agentic applications using the Amazon Bedrock FMs, LangGraph, and the Model Context Protocol (MCP), with a practical scenario of handling a GitHub workflow of issue analysis, code fixes, and pull request generation.\nFor teams seeking a managed solution to streamline GitHub workflows, Amazon Q Developer in GitHub offers native integration with GitHub repositories. It provides built-in capabilities for code generation, review, and code transformation without requiring custom agent development. While Amazon Q Developer provides out-of-the-box functionality for common development workflows, organizations with specific requirements or unique use cases may benefit from building custom solutions using Amazon Bedrock and agent frameworks. This flexibility allows teams to choose between a ready-to-use solution with Amazon Q Developer or a customized approach using Amazon Bedrock, depending on their specific needs, technical requirements, and desired level of control over the implementation.\nChallenges with the current state of AI agents\nDespite the remarkable advancements in AI agent technology, the current state of agent development and deployment faces significant challenges that limit their effectiveness, reliability, and broader adoption. These challenges span technical, operational, and conceptual domains, creating barriers that developers and organizations must navigate when implementing agentic solutions.\nOne of the significant challenges is tool integration. Although frameworks like Amazon Bedrock Agents, LangGraph, and the Strands Agent SDK provide mechanisms for agents to interact with external tools and services, the current approaches often lack standardization and flexibility. Developers must create custom integrations for each tool, define precise schemas, and handle a multitude of edge cases in tool invocation and response processing. Furthermore, the rigid nature of many tool integration frameworks means that agents struggle to adapt to changes in tool interfaces or to discover and use new capabilities dynamically.\nHow MCP helps in creating agents\nAppearing as a response to the limitations and challenges of current agent architectures, MCP provides a standardized framework that fundamentally redefines the relationship between FMs, context management, and tool integration. This protocol addresses many of the core challenges that have hindered the broader adoption and effectiveness of AI agents, particularly in enterprise environments and complex use cases.\nThe following diagram illustrates an example architecture.\n\nTool integration is dramatically simplified through MCP’s Tool Registry and standardized invocation patterns. Developers can register tools with the registry using a consistent format, and the protocol manages the complexities of tool selection, parameter preparation, and response processing. This not only reduces the development effort required to integrate new tools but also enables more sophisticated tool usage patterns, such as tool chaining and parallel tool invocation, that are challenging to implement in current frameworks.\nThis combination takes advantage of the strengths of each technology—high-quality FMs in Amazon Bedrock, MCP’s context management capabilities, and LangGraph’s orchestration framework—to create agents that can tackle increasingly complex tasks with greater reliability and effectiveness.\nImagine your development team wakes up to find yesterday’s GitHub issues already analyzed, fixed, and waiting as pull requests — all handled autonomously overnight.\nRecent advances in AI, particularly LLMs with code generation capabilities, have resulted in an impactful approach to development workflows. By using agents, development teams can automate simple changes—such as dependency updates or straightforward bug fixes.\nSolution Overview\nAmazon Bedrock is a fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI.\nLangGraph orchestrates agentic workflows through a graph-based architecture that handles complex processes and maintains context across agent interactions. It uses supervisory control patterns and memory systems for coordination. For more details, refer to Build multi-agent systems with LangGraph and Amazon Bedrock.\nThe Model Context Protocol (MCP) is an open standard that empowers developers to build secure, two-way connections between their data sources and AI-powered tools. The GitHub MCP Server is an MCP server that provides seamless integration with GitHub APIs. It offers a standard way for AI tools to work with GitHub’s repositories. Developers can use it to automate tasks, analyze code, and improve workflows without handling complex API calls.\nThis post uses these three technologies in a complementary fashion. Amazon Bedrock offers the AI capabilities for understanding issues and generating code fixes. LangGraph orchestrates the end-to-end workflow, managing the state and decision-making throughout the process. The GitHub MCP Server interfaces with GitHub repositories, providing context to the FM and implementing the generated changes. Together, these technologies enable an automation system that can understand and analyze GitHub issues, extract relevant code context, generate code fixes, create well-documented pull requests, and integrate seamlessly with existing GitHub workflows.\nThe figure below shows high-level view of how LangGraph integrates with GitHub through MCP while leveraging LLMs from Amazon Bedrock.\n\nIn the following sections, we explore the technical approach for building an AI-powered automation system, using Amazon Bedrock, LangGraph, and the GitHub MCP Server. We discuss the core concepts of building the solution; we don’t focus on deploying the agent or running the MCP server in the AWS environment. For a detailed explanation, refer to the GitHub repository.\nPrerequisites\nYou must have the following prerequisites before you can deploy this solution. For this post, we use the us-west-2 AWS Region. For details on available Regions, see Amazon Bedrock endpoints and quotas.\n\nA valid AWS account.\nAn AWS Identity and Access Management (IAM) role in the account that has sufficient permissions to invoke Amazon Bedrock models. If you’re planning to run your code on a Amazon SageMaker Jupyter notebook instance (rather than locally), you will also need permissions to set up and manage SageMaker resources. If you have administrator access, no action is needed for this step.\nAccess to Anthropic’s Claude 3.5 Haiku on Amazon Bedrock. For instructions, see Access Amazon Bedrock foundation models.\nDocker or Finch to run GitHub MCP server as a container.\nA fine-grained personal access token. The GitHub MCP server can use supported GitHub APIs, so enable the least permission needed for this post. Assign repository permissions for contents, issues, and pull requests.\n\nEnvironment configuration and setup\nThe MCP server acts as a bridge between our LangGraph agent and GitHub’s API. Instead of directly calling GitHub APIs, we use the containerized the GitHub MCP Server, which provides standardized tool interfaces.\nYou need to define the MCP configuration using the personal access token that you defined in the prerequisites. This configuration will start the GitHub MCP Server using Docker or Finch.\n\n\n\nmcp_config = {\n    \"mcp\": {\n        \"inputs\": [\n            {\n                \"type\": \"promptString\",\n                \"id\": \"github_token\",\n                \"description\": \"GitHub Personal Access Token\",\n                \"password\": \"true\",\n            }\n        ],\n        \"servers\": {\n            \"github\": {\n                \"command\": \"/usr/local/bin/docker\",\n                \"args\": [\n                    \"run\",\n                    \"-i\",\n                    \"--rm\",\n                    \"-e\",\n                    \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n                    \"ghcr.io/github/github-mcp-server\",\n                ],\n                \"env\": {\n                    \"GITHUB_PERSONAL_ACCESS_TOKEN\": os.environ.get(\"GITHUB_TOKEN\")\n                },\n            }\n        },\n    }\n}\n\n\n\nAgent state\nLangGraph needs a shared state object that flows between the nodes in the workflow. This state acts as memory, allowing each step to access data from earlier steps and pass results to later ones.\n\nclass AgentState(TypedDict):\n    issues: List[Dict[str, Any]] \n    current_issue_index: int \n    analysis_result: Optional[Dict[str, Any]] \n    action_required: Optional[str]\n\nStructured output\nInstead of parsing free-form LLM responses, we use Pydantic models to enforce consistent, machine-readable outputs. This reduces parsing errors and make sure downstream nodes receive data in expected formats. The Field descriptions guide the LLM to provide exactly what we need.\n\n\nclass IssueAnalysis(BaseModel):\n    \"\"\"Analysis of the GitHub issue.\"\"\"\n    analysis: str = Field(\n        description=\"Brief summary of the issue's core problem or request.\"\n    )\n    action_required: str = Field(\n        description=\"Decision on next step. Must be one of: 'code_change_required', 'no_change_needed', 'needs_clarification'.\"\n    )\n\n\nMCP tools integration\nThe load_mcp_tools function from the LangChain’s MCP adapter automatically converts the MCP server capabilities into LangChain-compatible tools. This abstraction makes it possible to use the GitHub operations (list issues, create branches, update files) as if they were built-in LangChain tools.\n\nasync def get_mcp_tools(session: ClientSession) -> List[Any]:\n     \"\"\"Loads tools from the connected MCP session.\"\"\"\n     tools = await load_mcp_tools(session)\n     return tools \n\nWorkflow structure\nEach node is stateless — it takes the current state, performs one specific task, and returns state updates. This makes the workflow predictable, testable, and straightforward to debug. These nodes are connected using edges or conditional edges. Not every GitHub issue requires code changes. Some might be documentation requests, duplicates, or need clarification. The routing functions use the structured LLM output to dynamically decide the next step, making the workflow adaptive rather than rigid.\nFinally, we start the agent by invoking the compiled graph with an initial state. The agent then follows the steps and decisions defined in the graph. The following diagram illustrates the workflow.\n\nAgent Execution and Result\nWe can invoke the compiled graph with the initial_state and recursion_limit. It will fetch open issues from the given GitHub repository, analyze them one at a time, make the code changes if needed and then create the pull request in GitHub.\nConsiderations\nTo enable automated workflows, Amazon EventBridge offers an integration with GitHub through its SaaS partner event sources. After it’s configured, EventBridge receives these GitHub events in near real-time. You can create rules that match specific issue patterns and route them to various AWS services like AWS Lambda functions, AWS Step Functions state machines, or Amazon Simple Notification Service (Amazon SNS) topics for further processing. This integration enables automated workflows that can trigger your analysis pipelines or code generation processes when relevant GitHub issue activities occur.\nWhen deploying the system, consider a phased rollout strategy. Start with a pilot phase in two or three non-critical repositories to confirm effectiveness and find issues. During this pilot phase, it’s crucial to thoroughly evaluate the solution across a diverse set of code files. This test should cover different programming languages, frameworks, formats (such as – Jupyter notebook), and varying levels of complexity in number and size of code files. Gradually expand to more repositories, prioritizing those with high maintenance burdens or standardized code patterns.\nInfrastructure best practices include containerization, designing for scalability, providing high availability, and implementing comprehensive monitoring for application, system, and business metrics. Security considerations are paramount, including operating with least privilege access, proper secrets management, input validation, and vulnerability management through regular updates and security scanning.\nIt is crucial to align with your company’s generative AI operations and governance frameworks. Prior to deployment, verify alignment with your organization’s AI safety protocols, data handling policies, and model deployment guidelines. Although this architectural pattern offers significant benefits, you should adapt implementation to fit within your organization’s specific AI governance structure and risk management frameworks.\nClean up\nClean up your environment by completing the following steps:\n\nDelete IAM roles and policies created specifically for this post.\nDelete the local copy of this post’s code.\nIf you no longer need access to an Amazon Bedrock FM, you can remove access to it. For instructions, see Add or remove access to Amazon Bedrock foundation models.\nDelete the personal access token. For instructions, see Deleting a personal access token.\n\nConclusion\nThe integration of Amazon Bedrock FMs with the MCP and LangGraph is a significant advancement in the field of AI agents. By addressing the fundamental challenges of context management and tool integration, this combination enables the development of more sophisticated, reliable, and powerful agentic applications.\nThe GitHub issues workflow scenario demonstrates benefits that include productivity enhancement, consistency improvement, faster response times, scalable maintenance, and knowledge amplification. Important insights include the role of FMs as development partners, the necessity of workflow orchestration, the importance of repository context, the need for confidence assessment, and the value of feedback loops for continuous improvement.\nThe future of AI-powered development automation will see trends like multi-agent collaboration systems, proactive code maintenance, context-aware code generation, enhanced developer collaboration, and ethical AI development. Challenges include skill evolution, governance complexity, quality assurance, and integration complexity, whereas opportunities include developer experience transformation, accelerated innovation, knowledge democratization, and accessibility improvements. Organizations can prepare by starting small, investing in knowledge capture, building feedback loops, developing AI literacy, and experimenting with new capabilities. The goal is to enhance developer capabilities, not replace them, fostering a collaborative future where AI and human developers work together to build better software.\nFor the example code and demonstration discussed in this post, refer to the accompanying GitHub repository.\nRefer to the following resources for additional guidance to get started:\n\nModel Context Protocol documentation\nAmazon Bedrock Documentation\n\n\nAbout the authors\nJagdeep Singh Soni is a Senior Partner Solutions Architect at AWS based in the Netherlands. He uses his passion for generative AI to help customers and partners build generative AI applications using AWS services. Jagdeep has 15 years of experience in innovation, experience engineering, digital transformation, cloud architecture, and ML applications.\nAjeet Tewari is a Senior Solutions Architect for Amazon Web Services. He works with enterprise customers to help them navigate their journey to AWS. His specialties include architecting and implementing scalable OLTP systems and leading strategic AWS initiatives.\nMani Khanuja is a Tech Lead – Generative AI Specialists, author of the book Applied Machine Learning and High-Performance Computing on AWS, and a member of the Board of Directors for Women in Manufacturing Education Foundation Board. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.",
      "date": "2025-07-30",
      "authors": "Ajeet Tewari",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article discusses the integration of Amazon Bedrock foundation models, the Model Context Protocol (MCP), and LangGraph to build advanced AI agents that automate GitHub workflows, such as issue analysis, code fixes, and pull request generation. It highlights new AI tools, techniques for enhanced context management, and standardized tool integration to improve the development and deployment of generative AI applications.",
      "takeaways": [
        "- Introduction of MCP as a standardized protocol addressing key challenges in AI agent tool integration and context management.",
        "- Demonstration of practical use of large language models (LLMs) for automating software development tasks with generative AI.",
        "- Exploration of a flexible, scalable AI workflow orchestration combining Amazon Bedrock’s models, LangGraph, and GitHub MCP Server to enhance developer productivity and enable sophisticated agentic applications."
      ]
    },
    {
      "id": 5,
      "title": "Mistral-Small-3.2-24B-Instruct-2506 is now available on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart",
      "link": "https://aws.amazon.com/blogs/machine-learning/mistral-small-3-2-24b-instruct-2506-is-now-available-on-amazon-bedrock-marketplace-and-amazon-sagemaker-jumpstart/",
      "description": "Today, we’re excited to announce that Mistral-Small-3.2-24B-Instruct-2506—a 24-billion-parameter large language model (LLM) from Mistral AI that’s optimized for enhanced instruction following and reduced repetition errors—is available for customers through Amazon SageMaker JumpStart and Amazon Bedrock Marketplace. Amazon Bedrock Marketplace is a capability in Amazon Bedrock that developers can use to discover, test, and use over 100 popular, emerging, and specialized foundation models (FMs) alongside the current selection of industry-leading models in Amazon Bedrock.\nIn this post, we walk through how to discover, deploy, and use Mistral-Small-3.2-24B-Instruct-2506 through Amazon Bedrock Marketplace and with SageMaker JumpStart.\nOverview of Mistral Small 3.2 (2506)\nMistral Small 3.2 (2506) is an update of Mistral-Small-3.1-24B-Instruct-2503, maintaining the same 24-billion-parameter architecture while delivering improvements in key areas. Released under Apache 2.0 license, this model maintains a balance between performance and computational efficiency. Mistral offers both the pretrained (Mistral-Small-3.1-24B-Base-2503) and instruction-tuned (Mistral-Small-3.2-24B-Instruct-2506) checkpoints of the model under Apache 2.0.\nKey improvements in Mistral Small 3.2 (2506) include:\n\nImproves in following precise instructions with 84.78% accuracy compared to 82.75% in version 3.1 from Mistral’s benchmarks\nProduces twice as fewer infinite generations or repetitive answers, reducing from 2.11% to 1.29% according to Mistral\nOffers a more robust and reliable function calling template for structured API interactions\nNow includes image-text-to-text capabilities, allowing the model to process and reason over both textual and visual inputs. This makes it ideal for tasks such as document understanding, visual Q&A, and image-grounded content generation.\n\nThese improvements make the model particularly well-suited for enterprise applications on AWS where reliability and precision are critical. With a 128,000-token context window, the model can process extensive documents and maintain context throughout longer conversation.\nSageMaker JumpStart overview\nSageMaker JumpStart is a fully managed service that offers state-of-the-art FMs for various use cases such as content writing, code generation, question answering, copywriting, summarization, classification, and information retrieval. It provides a collection of pre-trained models that you can deploy quickly, accelerating the development and deployment of machine learning (ML) applications. One of the key components of SageMaker JumpStart is model hubs, which offer a vast catalog of pre-trained models, such as Mistral, for a variety of tasks.\nYou can now discover and deploy Mistral models in Amazon SageMaker Studio or programmatically through the Amazon SageMaker Python SDK, deriving model performance and MLOps controls with SageMaker features such as Amazon SageMaker Pipelines, Amazon SageMaker Debugger, or container logs. The model is deployed in a secure AWS environment and under your virtual private cloud (VPC) controls, helping to support data security for enterprise security needs.\nPrerequisites\nTo deploy Mistral-Small-3.2-24B-Instruct-2506, you must have the following prerequisites:\n\nAn AWS account that will contain all your AWS resources.\nAn AWS Identity and Access Management (IAM) role to access SageMaker. To learn more about how IAM works with SageMaker, see Identity and Access Management for Amazon SageMaker.\nAccess to SageMaker Studio, a SageMaker notebook instance, or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference.\nAccess to accelerated instances (GPUs) for hosting the model.\n\nIf needed, request a quota increase and contact your AWS account team for support. This model requires a GPU-based instance type (approximately 55 GB of GPU RAM in bf16 or fp16) such as ml.g6.12xlarge.\nDeploy Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace\nTo access Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace, complete the following steps:\n\nOn the Amazon Bedrock console, in the navigation pane under Discover, choose Model catalog.\nFilter for Mistral as a provider and choose the Mistral-Small-3.2-24B-Instruct-2506 model.\n\n\nThe model detail page provides essential information about the model’s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.The page also includes deployment options and licensing information to help you get started with Mistral-Small-3.2-24B-Instruct-2506 in your applications.\n\nTo begin using Mistral-Small-3.2-24B-Instruct-2506, choose Deploy.\nYou will be prompted to configure the deployment details for Mistral-Small-3.2-24B-Instruct-2506. The model ID will be pre-populated. \n  \nFor Endpoint name, enter an endpoint name (up to 50 alphanumeric characters).\nFor Number of instances, enter a number between 1–100.\nFor Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended.\nOptionally, configure advanced security and infrastructure settings, including VPC networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, review these settings to align with your organization’s security and compliance requirements.\n \nChoose Deploy to begin using the model.\n\n\nWhen the deployment is complete, you can test Mistral-Small-3.2-24B-Instruct-2506 capabilities directly in the Amazon Bedrock playground, a tool on the Amazon Bedrock console to provide a visual interface to experiment with running different models.\n\nChoose Open in playground to access an interactive interface where you can experiment with different prompts and adjust model parameters such as temperature and maximum length.\n\n\nThe playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results.\nTo invoke the deployed model programmatically with Amazon Bedrock APIs, you need to get the endpoint Amazon Resource Name (ARN). You can use the Converse API for multimodal use cases. For tool use and function calling, use the Invoke Model API.\nReasoning of complex figures\nVLMs excel at interpreting and reasoning about complex figures, charts, and diagrams. In this particular use case, we use Mistral-Small-3.2-24B-Instruct-2506 to analyze an intricate image containing GDP data. Its advanced capabilities in document understanding and complex figure analysis make it well-suited for extracting insights from visual representations of economic data. By processing both the visual elements and accompanying text, Mistral Small 2506 can provide detailed interpretations and reasoned analysis of the GDP figures presented in the image.\nWe use the following input image.\n\nWe have defined helper functions to invoke the model using the Amazon Bedrock Converse API:\n\ndef get_image_format(image_path):\n    with Image.open(image_path) as img:\n        # Normalize the format to a known valid one\n        fmt = img.format.lower() if img.format else 'jpeg'\n        # Convert 'jpg' to 'jpeg'\n        if fmt == 'jpg':\n            fmt = 'jpeg'\n    return fmt\n\ndef call_bedrock_model(model_id=None, prompt=\"\", image_paths=None, system_prompt=\"\", temperature=0.6, top_p=0.9, max_tokens=3000):\n    \n    if isinstance(image_paths, str):\n        image_paths = [image_paths]\n    if image_paths is None:\n        image_paths = []\n    \n    # Start building the content array for the user message\n    content_blocks = []\n\n    # Include a text block if prompt is provided\n    if prompt.strip():\n        content_blocks.append({\"text\": prompt})\n\n    # Add images as raw bytes\n    for img_path in image_paths:\n        fmt = get_image_format(img_path)\n        # Read the raw bytes of the image (no base64 encoding!)\n        with open(img_path, 'rb') as f:\n            image_raw_bytes = f.read()\n\n        content_blocks.append({\n            \"image\": {\n                \"format\": fmt,\n                \"source\": {\n                    \"bytes\": image_raw_bytes\n                }\n            }\n        })\n\n    # Construct the messages structure\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": content_blocks\n        }\n    ]\n\n    # Prepare additional kwargs if system prompts are provided\n    kwargs = {}\n    \n    kwargs[\"system\"] = [{\"text\": system_prompt}]\n\n    # Build the arguments for the `converse` call\n    converse_kwargs = {\n        \"messages\": messages,\n        \"inferenceConfig\": {\n            \"maxTokens\": 4000,\n            \"temperature\": temperature,\n            \"topP\": top_p\n        },\n        **kwargs\n    }\n\n    \n    converse_kwargs[\"modelId\"] = model_id\n\n    # Call the converse API\n    try:\n        response = client.converse(**converse_kwargs)\n    \n        # Parse the assistant response\n        assistant_message = response.get('output', {}).get('message', {})\n        assistant_content = assistant_message.get('content', [])\n        result_text = \"\".join(block.get('text', '') for block in assistant_content)\n    except Exception as e:\n        result_text = f\"Error message: {e}\"\n    return result_text\n\nOur prompt and input payload are as follows:\n\nimport boto3\nimport base64\nimport json\nfrom PIL import Image\nfrom botocore.exceptions import ClientError\n\n# Create a Bedrock Runtime client in the AWS Region you want to use.\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n\nsystem_prompt='You are a Global Economist.'\ntask = 'List the top 5 countries in Europe with the highest GDP'\nimage_path = './image_data/gdp.png'\n\nprint('Input Image:\\n\\n')\nImage.open(image_path).show()\n\nresponse = call_bedrock_model(model_id=endpoint_arn, \n                   prompt=task, \n                   system_prompt=system_prompt,\n                   image_paths = image_path)\n\nprint(f'\\nResponse from the model:\\n\\n{response}')\n\nThe following is a response using the Converse API:\n\nBased on the image provided, the top 5 countries in Europe with the highest GDP are:\n\n1. **Germany**: $3.99T (4.65%)\n2. **United Kingdom**: $2.82T (3.29%)\n3. **France**: $2.78T (3.24%)\n4. **Italy**: $2.07T (2.42%)\n5. **Spain**: $1.43T (1.66%)\n\nThese countries are highlighted in green, indicating their location in the Europe region.\n\nDeploy Mistral-Small-3.2-24B-Instruct-2506 in SageMaker JumpStart\nYou can access Mistral-Small-3.2-24B-Instruct-2506 through SageMaker JumpStart in the SageMaker JumpStart UI and the SageMaker Python SDK. SageMaker JumpStart is an ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. With SageMaker JumpStart, you can customize pre-trained models to your use case, with your data, and deploy them into production using either the UI or SDK.\nDeploy Mistral-Small-3.2-24B-Instruct-2506 through the SageMaker JumpStart UI\nComplete the following steps to deploy the model using the SageMaker JumpStart UI:\n\nOn the SageMaker console, choose Studio in the navigation pane.\nFirst-time users will be prompted to create a domain. If not, choose Open Studio.\nOn the SageMaker Studio console, access SageMaker JumpStart by choosing JumpStart in the navigation pane.\n\n\n\nSearch for and choose Mistral-Small-3.2-24B-Instruct-2506 to view the model card.\n\n\n\nClick the model card to view the model details page. Before you deploy the model, review the configuration and model details from this model card. The model details page includes the following information:\n\n\nThe model name and provider information.\nA Deploy button to deploy the model.\nAbout and Notebooks tabs with detailed information.\nThe Bedrock Ready badge (if applicable) indicates that this model can be registered with Amazon Bedrock, so you can use Amazon Bedrock APIs to invoke the model.\n\n\n\nChoose Deploy to proceed with deployment. \n  \nFor Endpoint name, enter an endpoint name (up to 50 alphanumeric characters).\nFor Number of instances, enter a number between 1–100 (default: 1).\nFor Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended.\n \n\n\n\nChoose Deploy to deploy the model and create an endpoint.\n\nWhen deployment is complete, your endpoint status will change to InService. At this point, the model is ready to accept inference requests through the endpoint. You can invoke the model using a SageMaker runtime client and integrate it with your applications.\nDeploy Mistral-Small-3.2-24B-Instruct-2506 with the SageMaker Python SDK\nDeployment starts when you choose Deploy. After deployment finishes, you will see that an endpoint is created. Test the endpoint by passing a sample inference request payload or by selecting the testing option using the SDK. When you select the option to use the SDK, you will see example code that you can use in the notebook editor of your choice in SageMaker Studio.\nTo deploy using the SDK, start by selecting the Mistral-Small-3.2-24B-Instruct-2506 model, specified by the model_id with the value mistral-small-3.2-24B-instruct-2506. You can deploy your choice of the selected models on SageMaker using the following code. Similarly, you can deploy Mistral-Small-3.2-24B-Instruct-2506 using its model ID.\n\nfrom sagemaker.jumpstart.model import JumpStartModel \naccept_eula = True \nmodel = JumpStartModel(model_id=\"huggingface-vlm-mistral-small-3-2-24b-instruct-2506\") \npredictor = model.deploy(accept_eula=accept_eula)\nThis deploys the model on SageMaker with default configurations, including the default instance type and default VPC configurations. You can change these configurations by specifying non-default values in JumpStartModel. The EULA value must be explicitly defined as True to accept the end-user license agreement (EULA).\n\nAfter the model is deployed, you can run inference against the deployed endpoint through the SageMaker predictor:\n\nprompt = \"Hello!\"\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": prompt\n        }\n    ],\n    \"max_tokens\": 4000,\n    \"temperature\": 0.15,\n    \"top_p\": 0.9,\n}\n    \nresponse = predictor.predict(payload)\nprint(response['choices'][0]['message']['content'])\nWe get following response:\n\nHello! 😊 How can I assist you today?\n\nVision reasoning example\nUsing the multimodal capabilities of Mistral-Small-3.2-24B-Instruct-2506, you can process both text and images for comprehensive analysis. The following example highlights how the model can simultaneously analyze a tuition ROI chart to extract visual patterns and data points. The following image is the input chart.png.\n\nOur prompt and input payload are as follows:\n\n# Read and encode the image\nimage_path = \"chart.png\"\nwith open(image_path, \"rb\") as image_file:\n base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n\n\n# Create a prompt focused on visual analysis of the box plot chart\nvisual_prompt = \"\"\"Please analyze this box plot chart showing the relationship between Annual Tuition (x-axis) and \n40-Year Net Present Value (y-axis) in US$. \nDescribe the key trend between tuition and net present value shown in this chart. What's one notable insight?\"\"\"\n\n# Create payload with image input\npayload = {\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": visual_prompt},\n{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n]\n}\n],\n\"max_tokens\": 800,\n\"temperature\": 0.15\n}\n\n# Make a prediction\nresponse = predictor.predict(payload)\n\n# Display the visual analysis\nmessage = response['choices'][0]['message']\nif message.get('content'):\nprint(\"Vision Analysis:\")\nprint(message['content'])\n\nWe get following response:\n\nVision Analysis:\nThis box plot chart illustrates the relationship between annual tuition costs (x-axis) and the 40-year net present value (NPV) in US dollars (y-axis). Each box plot represents a range of annual tuition costs, showing the distribution of NPV values within that range.\n\n### Key Trend:\n1. **General Distribution**: Across all tuition ranges, the median 40-year NPV (indicated by the line inside each box) appears to be relatively consistent, hovering around the $1,000,000 mark.\n2. **Variability**: The spread of NPV values (indicated by the height of the boxes and whiskers) is wider for higher tuition ranges, suggesting greater variability in outcomes for more expensive schools.\n3. **Outliers**: There are several outliers, particularly in the higher tuition ranges (e.g., 35-40k, 40-45k, and >50k), indicating that some individuals experience significantly higher or lower NPVs.\n\n### Notable Insight:\nOne notable insight from this chart is that higher tuition costs do not necessarily translate into a higher 40-year net present value. For example, the median NPV for the highest tuition range (>50k) is not significantly higher than that for the lowest tuition range (<5k). This suggests that the return on investment for higher tuition costs may not be proportionally greater, and other factors beyond tuition cost may play a significant role in determining long-term financial outcomes.\n\nThis insight highlights the importance of considering factors beyond just tuition costs when evaluating the potential return on investment of higher education.\n\nFunction calling example\nThis following example shows Mistral Small 3.2’s function calling by demonstrating how the model identifies when a user question needs external data and calls the correct function with proper parameters.Our prompt and input payload are as follows:\n\n# Define a simple weather function\nweather_function = {\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get weather for a location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"City name\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n\n# User question\nuser_question = \"What's the weather like in Seattle?\"\n\n# Create payload\npayload = {\n\"messages\": [{\"role\": \"user\", \"content\": user_question}],\n\"tools\": [weather_function],\n\"tool_choice\": \"auto\",\n\"max_tokens\": 200,\n\"temperature\": 0.15\n}\n\n# Make prediction\nresponse = predictor.predict(payload)\n\n# Display raw response to see exactly what we get\nprint(json.dumps(response['choices'][0]['message'], indent=2))\n\n# Extract function call information from the response content\nmessage = response['choices'][0]['message']\ncontent = message.get('content', '')\n\nif '[TOOL_CALLS]' in content:\nprint(\"Function call details:\", content.replace('[TOOL_CALLS]', ''))\n\nWe get following response:\n\n{\n\"role\": \"assistant\",\n\"reasoning_content\": null,\n\"content\": \"[TOOL_CALLS]get_weather{\\\"location\\\": \\\"Seattle\\\"}\",\n\"tool_calls\": []\n}\nFunction call details: get_weather{\"location\": \"Seattle\"}\n\nClean up\nTo avoid unwanted charges, complete the following steps in this section to clean up your resources.\nDelete the Amazon Bedrock Marketplace deployment\nIf you deployed the model using Amazon Bedrock Marketplace, complete the following steps:\n\nOn the Amazon Bedrock console, under Tune in the navigation pane, select Marketplace model deployment.\nIn the Managed deployments section, locate the endpoint you want to delete.\nSelect the endpoint, and on the Actions menu, choose Delete.\nVerify the endpoint details to make sure you’re deleting the correct deployment: \n  \nEndpoint name\nModel name\nEndpoint status\n \nChoose Delete to delete the endpoint.\nIn the deletion confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint.\n\nDelete the SageMaker JumpStart predictor\nAfter you’re done running the notebook, make sure to delete the resources that you created in the process to avoid additional billing. For more details, see Delete Endpoints and Resources. You can use the following code:\n\npredictor.delete_model()\npredictor.delete_endpoint()\n\nConclusion\nIn this post, we showed you how to get started with Mistral-Small-3.2-24B-Instruct-2506 and deploy the model using Amazon Bedrock Marketplace and SageMaker JumpStart for inference. This latest version of the model brings improvements in instruction following, reduced repetition errors, and enhanced function calling capabilities while maintaining performance across text and vision tasks. The model’s multimodal capabilities, combined with its improved reliability and precision, support enterprise applications requiring robust language understanding and generation.\nVisit SageMaker JumpStart in Amazon SageMaker Studio or Amazon Bedrock Marketplace now to get started with Mistral-Small-3.2-24B-Instruct-2506.\nFor more Mistral resources on AWS, check out the Mistral-on-AWS GitHub repo.\n\nAbout the authors\nNiithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelor’s degree in Computer Science and Bioinformatics.\nBreanne Warner is an Enterprise Solutions Architect at Amazon Web Services supporting healthcare and life science (HCLS) customers. She is passionate about supporting customers to use generative AI on AWS and evangelizing model adoption for first- and third-party models. Breanne is also Vice President of the Women at Amazon board with the goal of fostering inclusive and diverse culture at Amazon. Breanne holds a Bachelor’s of Science in Computer Engineering from the University of Illinois Urbana-Champaign.\nKoushik Mani is an Associate Solutions Architect at AWS. He previously worked as a Software Engineer for 2 years focusing on machine learning and cloud computing use cases at Telstra. He completed his Master’s in Computer Science from the University of Southern California. He is passionate about machine learning and generative AI use cases and building solutions.",
      "date": "2025-07-29",
      "authors": "Breanne Warner",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article announces the availability of the Mistral-Small-3.2-24B-Instruct-2506 large language model on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart, detailing its advancements in instruction following, reduced repetition errors, and multimodal capabilities for processing both text and images. It provides comprehensive guidance on deploying and using the model for various AI applications, highlighting its enterprise-ready features and improved function calling.",
      "takeaways": [
        "- Mistral-Small-3.2-24B-Instruct-2506 offers significant improvements in instruction accuracy, reduced repetitive outputs, and supports image-text multimodal reasoning, enhancing AI application capabilities.",
        "- The model is accessible through AWS services—Amazon Bedrock Marketplace and SageMaker JumpStart—enabling streamlined deployment, integration, and enterprise-grade security.",
        "- Practical examples demonstrate the model’s versatility in vision reasoning, function calling, and economic data interpretation, showcasing advances in AI model research and tools relevant for pharmaceutical and healthcare sectors."
      ]
    },
    {
      "id": 6,
      "title": "Generate suspicious transaction report drafts for financial compliance using generative AI",
      "link": "https://aws.amazon.com/blogs/machine-learning/generate-suspicious-transaction-report-drafts-for-financial-compliance-using-generative-ai/",
      "description": "Financial regulations and compliance are constantly changing, and automation of compliance reporting has emerged as a game changer in the financial industry. Amazon Web Services (AWS) generative AI solutions offer a seamless and efficient approach to automate this reporting process. The integration of AWS generative AI into the compliance framework not only enhances efficiency but also instills a greater sense of confidence and trust in the financial sector by promoting precision and timely delivery of compliance reports. These solutions help financial institutions avoid the costly and reputational consequences of noncompliance. This, in turn, contributes to the overall stability and integrity of the financial ecosystem, benefiting both the industry and the consumers it serves.\nAmazon Bedrock is a managed generative AI service that provides access to a wide array of advanced foundation models (FMs). It includes features that facilitate the efficient creation of generative AI applications with a strong focus on privacy and security. Getting a good response from an FM relies heavily on using efficient techniques for providing prompts to the FM. Retrieval Augmented Generation (RAG) is a pivotal approach to augmenting FM prompts with contextually relevant information from external sources. It uses vector databases such as Amazon OpenSearch Service to enable semantic searching of the contextual information.\nAmazon Bedrock Knowledge Bases, powered by vector databases such as Amazon OpenSearch Serverless, helps in implementing RAG to supplement model inputs with relevant information from factual resources, thereby reducing potential hallucinations and increasing response accuracy.\nAmazon Bedrock Agents enables generative AI applications to execute multistep tasks using action groups and enable interaction with APIs, knowledge bases, and FMs. Using agents, you can design intuitive and adaptable generative AI applications capable of understanding natural language queries and creating engaging dialogues to gather details required for using the FMs effectively.\nA suspicious transaction report (STR) or suspicious activity report (SAR) is a type of report that a financial organization must submit to a financial regulator if they have reasonable grounds to suspect any financial transaction that has occurred or was attempted during their activities. There are stipulated timelines for filing these reports and it typically takes several hours of manual effort to create one report for one customer account.\nIn this post, we explore a solution that uses FMs available in Amazon Bedrock to create a draft STR. We cover how generative AI can be used to automate the manual process of draft generation using account information, transaction details, and correspondence summaries as well as creating a knowledge base of information about fraudulent entities involved in such transactions.\nSolution overview\nThe solution uses Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, AWS Lambda, Amazon Simple Storage Service (Amazon S3), and OpenSearch Service. The workflow is as follows:\n\nThe user requests for creation of a draft STR report through the business application.\nThe application calls Amazon Bedrock Agents, which has been preconfigured with detailed instructions to engage in a conversational flow with the user. The agent follows these instructions to gather the required information from the user, completes the missing information by using actions groups to invoke the Lambda function, and generates the report in the specified format.\nFollowing its instructions, the agent invokes Amazon Bedrock Knowledge Bases to find details about fraudulent entities involved in the suspicious transactions.\nAmazon Bedrock Knowledge Bases queries OpenSearch Service to perform semantic search for the entities required for the report. If the information about fraudulent entities is available in Amazon Bedrock Knowledge Bases, the agent follows its instructions to generate a report for the user.\nIf the information isn’t found in the knowledge base, the agent uses the chat interface to prompt the user to provide the website URL that contains the relevant information. Alternatively, the user can provide a description about the fraudulent entity in the chat interface.\nIf the user provides a URL for a publicly accessible website, the agent follows its instructions to call the action group to invoke a Lambda function to crawl the website URL. The Lambda function scrapes the information from the website and returns it to the agent for use in the report.\nThe Lambda function also stores the scraped content in an S3 bucket for future use by the search index.\nAmazon Bedrock Knowledge Bases can be programmed to periodically scan the S3 bucket to index the new content in OpenSearch Service.\n\nThe following diagram illustrates the solution architecture and workflow.\n\nYou can use the full code available in GitHub to deploy the solution using the AWS Cloud Development Kit (AWS CDK). Alternatively, you can follow a step-by-step process for manual deployment. We walk through both approaches in this post.\nPrerequisites\nTo implement the solution provided in this post, you must enable model access in Amazon Bedrock for Amazon Titan Text Embeddings V2 and Anthropic Claude 3.5 Haiku.\nDeploy the solution with the AWS CDK\nTo set up the solution using the AWS CDK, follow these steps:\n\nVerify that the AWS CDK has been installed in your environment. For installation instructions, refer to the AWS CDK Immersion Day Workshop.\nUpdate the AWS CDK to version 36.0.0 or higher:\n\n\nnpm install -g aws-cdk\n\n\nInitialize the AWS CDK environment in the AWS account:\n\n\ncdk bootstrap\n\n\nClone the GitHub repository containing the solution files:\n\n\ngit clone https://github.com/aws-samples/suspicious-financial-transactions-reporting\n\n\nNavigate to the solution directory:\n\n\ncd financial-transaction-report-drafting-for-compliance\n\n\nCreate and activate the virtual environment:\n\n\npython3 -m venv .venv\nsource .venv/bin/activate\n\nActivating the virtual environment differs based on the operating system. Refer to the AWS CDK workshop for information about activating in other environments.\n\nAfter the virtual environment is activated, install the required dependencies:\n\n\npip install -r requirements.txt\n\n\nDeploy the backend and frontend stacks:\n\n\ncdk deploy -a ./app.py --all\n\n\nWhen the deployment is complete, check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following two screenshots.\n\n\n\nManual deployment\nTo implement the solution without using the AWS CDK, complete the following steps:\n\nSet up an S3 bucket.\nCreate a Lambda function.\nSet up Amazon Bedrock Knowledge Bases.\nSet up Amazon Bedrock Agents.\n\nVisual layouts in some screenshots in this post might look different than those on your AWS Management Console.\nSet up an S3 bucket\nCreate an S3 bucket with a unique bucket name for the document repository, as shown in the following screenshot. This will be a data source for Amazon Bedrock Knowledge Bases.\n\nCreate the website scraper Lambda function\nCreate a new Lambda function called Url-Scraper using the Python 3.13 runtime to crawl and scrape the website URL provided by Amazon Bedrock Agents. The function will scrape the content, send the information to the agent, and store the contents in the S3 bucket for future references.\n\nError handling has been skipped in this code snippet for brevity. The full code is available in GitHub.\nCreate a new file called search_suspicious_party.py with the following code snippet:\n\nimport boto3\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport urllib.request\nBUCKET_NAME = os.getenv('S3_BUCKET')\ns3 = boto3.client('s3')\ndef get_receiving_entity_from_url(start_url):\n    response = urllib.request.urlopen(\n        urllib.request.Request(url=start_url, method='GET'),\n        timeout=5)\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    # Extract page title\n    title = soup.title.string if soup.title else 'Untitled'\n    # Extract page content for specific HTML elements\n    content = ' '.join(p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']))\n    content = re.sub(r'\\s+', ' ', content).strip()\n    s3.put_object(Body=content, Bucket=BUCKET_NAME, Key=f\"docs/{title}.txt\")\nreturn content\n\nReplace the default generated code in lambda_function.py with the following code:\n\nimport json\nfrom search-suspicious-party import *\ndef lambda_handler(event, context):\n    # apiPath should match the path specified in action group schema\n    if event['apiPath'] == '/get-receiving-entity-details':\n        # Extract the property from request data\n        start_url = get_named_property(event, 'start_url')\n        scraped_text = get_receiving_entity_from_url(start_url)\n        action_response = {\n            'actionGroup': event['actionGroup'],\n            'apiPath': event['apiPath'],\n            'httpMethod': event['httpMethod'],\n            'httpStatusCode': 200,\n            'responseBody': {\n                'application/json': {\n                    'body': json.dumps({'scraped_text': scraped_text})\n                }\n            }\n        }\n        return {'response': action_response}\n    # Return an error if apiPath is not recognized\n    return {\n        'statusCode': 400,\n        'body': json.dumps({'error': 'Invalid API path'})\n    }\ndef get_named_property(event, name):\n    return next(\n        item for item in\n        event['requestBody']['content']['application/json']['properties']\n        if item['name'] == name\n    )['value']\n\nConfigure the Lambda function\nSet up a Lambda environment variable S3_BUCKET, as shown in the following screenshot. For Value, use the S3 bucket you created previously.\n\nIncrease the timeout duration for Lambda function to 30 seconds. You can adjust this value based on the time it takes for the crawler to complete its work.\n\nSet up Amazon Bedrock Knowledge Bases\nComplete the following steps to create a new knowledge base in Amazon Bedrock. This knowledge base will use OpenSearch Serverless to index the fraudulent entity data stored in Amazon S3. For more information, refer to Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases.\n\nOn the Amazon Bedrock console, choose Knowledge bases in the navigation pane and choose Create knowledge base.\nFor Knowledge base name, enter a name (for example, str-knowledge-base).\nFor Service role name, keep the default system generated value.\n\n\n\nSelect Amazon S3 as the data source.\n\n\n\nConfigure the Amazon S3 data source: \n  \nFor Data source name, enter a name (for example, knowledge-base-data-source-s3).\nFor S3 URI, choose Browse S3 and choose the bucket where information scraped by web crawler about fraudulent entities is available for the knowledge base to use.\nKeep all other default values.\n \n\n\n\nFor Embeddings model, choose Titan Text Embeddings V2.\n\n\n\nFor Vector database, select Quick create a new vector store to create a default vector store with OpenSearch Serverless.\n\n\n\nReview the configurations and choose Create knowledge base.\n\nAfter the knowledge base is successfully created, you can see the knowledge base ID, which you will need when creating the agent in Amazon Bedrock.\n\nSelect knowledge-base-data-source-s3 from the list of data sources and choose Sync to index the documents.\n\n\nSet up Amazon Bedrock Agents\nTo create a new agent in Amazon Bedrock, complete the following steps. For more information, refer to Create and configure agent manually.\n\nOn the Amazon Bedrock console, choose Agents in the navigation pane and choose Create Agent.\nFor Name, enter a name (for example, agent-str).\nChoose Create.\n\n\n\nFor Agent resource role, keep the default setting (Create and use a new service role).\nFor Select model, choose a model provider and model name (for example, Anthropic’s Claude 3.5 Haiku)\nFor Instructions for the Agent, provide the instructions that allow the agent to invoke the large language model (LLM).\n\nYou can download the instructions from the agent-instructions.txt file in the GitHub repo. Refer to next section in this post to understand how to write the instructions.\n\nKeep all other default values.\nChoose Save.\n\n\n\nUnder Action groups, choose Add to create a new action group.\n\n\nAn action is a task the agent can perform by making API calls. A set of actions comprises an action group.\n\nProvide an API schema that defines all the APIs in the action group.\nFor Action group details, enter an action group name (for example, agent-group-str-url-scraper).\nFor Action group type, select Define with API schemas.\nFor Action group invocation, select Select an existing Lambda function, which is the Lambda function that you created previously.\n\n\n\nFor Action group schema, choose Define via in-line schema editor.\nReplace the default sample code with the following example to define the schema to specify the input parameters with default and mandatory values:\n\n\nopenapi: 3.0.0\ninfo:\n  title: Gather suspicious receiving entity details from website\n  version: 1.0.0\npaths:\n  /:\n    post:\n      description: Get details about suspicious receiving entity from the URL\n      operationId: getReceivingEntityDetails\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: \"#/components/schemas/ScrapeRequest\"\n      responses:\n        \"200\":\n          description: Receiving entity details gathered successfully\ncomponents:\n  schemas:\n    ScrapeRequest:\n      type: object\n      properties:\n        :\n          type: string\n          description: The URL to start scraping from\n      required:\n        - start_url\n\n\nChoose Create.\nUnder Knowledge bases, choose Add.\n\n\n\nFor Select knowledge base, choose knowledge-base-str, which you created previously, and add the following instructions:\n\nUse the information in the knowledge-base-str knowledge base to select transaction reports.\n\n\nChoose Save to save all changes.\nFinally, choose Prepare to prepare this agent to get it ready for testing.\n\nYou can also create a Streamlit application to create a UI for this application. The source code is available in GitHub.\nAgent instructions\nAgent instructions for Amazon Bedrock Agents provide the mechanism for a multistep user interaction to gather the inputs an agent needs to invoke the LLM with a rich prompt to generate the response in the required format. Provide logical instructions in plain English. There are no predefined formats for these instructions.\n\nProvide an overview of the task including the role:\n\n\nYou are a financial user creating Suspicious Transaction Report (STR) draft for a financial compliance use case.\n\n\nProvide the message that the agent can use for initiating the user interaction:\n\n\nGreet the user with the message “Hi <name>. Welcome to STR report drafting. How can I help?”\nAsk the user to provide the transactions details. From the transaction details, capture the response in the <answer> tag and include the <thinking> tag to understand the rationale behind the response.\n\n\nSpecify the processing that needs to be done on the output received from the LLM:\n\n\nFor the transaction input provided by user, create a narrative description for financial risk reporting of the provided bank account and transaction details.\n1. Add a summary of correspondence logs that includes title, summary, correspondence history, and analysis in the narrative description.\n2. Add the details about the receiving entity in the narrative description. You can get details about receiving entities from the agent action group.\n\n\nProvide the optional messages that the agent can use for a multistep interaction to gather the missing inputs if required:\n\n\nIf you don't have knowledge about Receiving entity, you should ask the Human for more details about it with a message “Unfortunately I do not have enough context or details about the receiving entity <entity name> to provide an accurate risk assessment or summary. Can you please provide some additional background information about <entity name>? What is the URL of the <entity name> or the description?”\n\n\nSpecify the actions that the agent can take to process the user input using action groups:\n\n\nIf user provides the URL of <entity name>, call the action group <add action group name> to get the details. If user provides the description of <entity name>, then summarize and add it to the narrative description as a receiving entity.\n\n\nSpecify how the agent should provide the response, including the format details:\n\n\nOnce you have all the necessary input (financial transaction details and receiving entity details), create a detailed well-formatted draft report for financial risk reporting of the provided bank account and transaction details containing the following sections:\n1. Title\n2. Summary of transactions\n3. Correspondence History & Analysis\n4. Receiving entity summary\n\nTest the solution\nTo test the solution, follow these steps:\n\nChoose Test to start testing the agent.\nInitiate the chat and observe how the agent uses the instructions you provided in the configuration step to ask for required details for generating the report.\nTry different prompts, such as “Generate an STR for an account.”\n\nThe following screenshot shows an example chat.\n\nThe following screenshot shows an example chat with the prompt, “Generate an STR for account number 49179-180-2092803.”\n\nAnother option is to provide all the details at the same time, for example, “Generate an STR for account number 12345-999-7654321 with the following transactions.”\n\nCopy and paste the sample transactions from the sample-transactions.txt file in GitHub.\n\nThe agent keeps asking for missing information, such as account number, transaction details, and correspondence history. After it has all the details, it will generate a draft STR document.\nThe code in GitHub also contains a sample StreamLit application that you can use to test the application.\nClean up\nTo avoid incurring unnecessary future charges, clean up the resources you created as part of this solution. If you created the solution using the GitHub code sample and the AWS CDK, empty the S3 bucket and delete the CloudFormation stack. If you created the solution manually, complete the following steps:\n\nDelete the Amazon Bedrock agent.\nDelete the Amazon Bedrock knowledge base.\nEmpty and delete the S3 bucket if you created one specifically for this solution.\nDelete the Lambda function.\n\nConclusion\nIn this post, we showed how Amazon Bedrock offers a robust environment for building generative AI applications, featuring a range of advanced FMs. This fully managed service prioritizes privacy and security while helping developers create AI-driven applications efficiently. A standout feature, RAG, uses external knowledge bases to enrich AI-generated content with relevant information, backed by OpenSearch Service as its vector database. Additionally, you can include metadata fields in the knowledge base and agent session context with Amazon Verified Permissions to pass fine-grained access context for authorization.\nWith careful prompt engineering, Amazon Bedrock minimizes inaccuracies and makes sure that AI responses are grounded in factual documentation. This combination of advanced technology and data integrity makes Amazon Bedrock an ideal choice for anyone looking to develop reliable generative AI solutions. You can now explore extending this sample code to use Amazon Bedrock and RAG for reliably generating draft documents for compliance reporting.\n\nAbout the Authors\nDivyajeet (DJ) Singh is a Senior Solutions Architect at AWS Canada. He loves working with customers to help them solve their unique business challenges using the cloud. Outside of work, he enjoys spending time with family and friends and exploring new places.\nParag Srivastava is a Senior Solutions Architect at AWS, where he has been helping customers successfully apply generative AI to real-life business scenarios. During his professional career, he has been extensively involved in complex digital transformation projects. He is also passionate about building innovative solutions around geospatial aspects of addresses.\nSangeetha Kamatkar is a Senior Solutions Architect at AWS who helps customers with successful cloud adoption and migration. She works with customers to craft highly scalable, flexible, and resilient cloud architectures that address customer business problems. In her spare time, she listens to music, watches movies, and enjoys gardening during summertime.\nVineet Kachhawaha is a Senior Solutions Architect at AWS focusing on AI/ML and generative AI. He co-leads the AWS for Legal Tech team within AWS. He is passionate about working with enterprise customers and partners to design, deploy, and scale AI/ML applications to derive business value.",
      "date": "2025-07-29",
      "authors": "Divyajeet Singh",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article discusses the use of Amazon Bedrock's generative AI capabilities to automate the drafting of suspicious transaction reports (STR) in the financial compliance sector. It highlights how advanced foundation models, Retrieval Augmented Generation (RAG), and integrations with AWS services enable efficient AI-powered workflows to enhance accuracy and reduce manual effort in compliance reporting.",
      "takeaways": [
        "- Amazon Bedrock provides managed generative AI services with advanced foundation models and features like RAG to improve the quality and factual grounding of AI outputs.",
        "- The solution leverages AI agents, knowledge bases, and Lambda functions to gather details, interactively prompt users, and generate detailed STR drafts, demonstrating practical AI adoption in compliance.",
        "- This implementation showcases how generative AI can streamline regulated financial processes while emphasizing privacy, security, and extensibility of AI tools."
      ]
    },
    {
      "id": 7,
      "title": "Fine-tune and deploy Meta Llama 3.2 Vision for generative AI-powered web automation using AWS DLCs, Amazon EKS, and Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/fine-tune-and-deploy-meta-llama-3-2-vision-for-generative-ai-powered-web-automation-using-aws-dlcs-amazon-eks-and-amazon-bedrock/",
      "description": "Fine-tuning of large language models (LLMs) has emerged as a crucial technique for organizations seeking to adapt powerful foundation models (FMs) to their specific needs. Rather than training models from scratch—a process that can cost millions of dollars and require extensive computational resources—companies can customize existing models with domain-specific data at a fraction of the cost. This approach has become particularly valuable as organizations across healthcare, finance, and technology sectors look to use AI for specialized tasks while maintaining cost-efficiency. However, implementing a production-grade fine-tuning solution presents several significant challenges. Organizations must navigate complex infrastructure setup requirements, enforce robust security measures, optimize performance, and establish reliable model hosting solutions.\nIn this post, we present a complete solution for fine-tuning and deploying the Llama-3.2-11B-Vision-Instruct model for web automation tasks. We demonstrate how to build a secure, scalable, and efficient infrastructure using AWS Deep Learning Containers (DLCs) on Amazon Elastic Kubernetes Service (Amazon EKS). By using AWS DLCs, you can gain access to well-tested environments that come with enhanced security features and pre-installed software packages, significantly simplifying the optimization of your fine-tuning process. This approach not only accelerates development, but also provides robust security and performance in production environments.\nSolution overview\nIn this section, we explore the key components of our architecture for fine-tuning a Meta Llama model and using it for web task automation. We explore the benefits of different components and how they interact with each other, and how we can use them to build a production-grade fine-tuning pipeline.\nAWS DLCs for training and hosting AI/ML workloads\nAt the core of our solution are AWS DLCs, which provide optimized environments for machine learning (ML) workloads. These containers come preconfigured with essential dependencies, including NVIDIA drivers, CUDA toolkit, and Elastic Fabric Adapter (EFA) support, along with preinstalled frameworks like PyTorch for model training and hosting. AWS DLCs tackle the complex challenge of packaging various software components to work harmoniously with training scripts, so you can use optimized hardware capabilities out of the box. Additionally, AWS DLCs implement unique patching algorithms and processes that continuously monitor, identify, and address security vulnerabilities, making sure the containers remain secure and up-to-date. Their pre-validated configurations significantly reduce setup time and reduce compatibility issues that often occur in ML infrastructure setup.\nAWS DLCs, Amazon EKS, and Amazon EC2 for seamless infrastructure management\nWe deploy these DLCs on Amazon EKS, creating a robust and scalable infrastructure for model fine-tuning. Organizations can use this combination to build and manage their training infrastructure with unprecedented flexibility. Amazon EKS handles the complex container orchestration, so you can launch training jobs that run within DLCs on your desired Amazon Elastic Compute Cloud (Amazon EC2) instance, producing a production-grade environment that can scale based on training demands while maintaining consistent performance.\nAWS DLCs and EFA support for high-performance networking\nAWS DLCs come with pre-configured support for EFA, enabling high-throughput, low-latency communication between EC2 nodes. An EFA is a network device that you can attach to your EC2 instance to accelerate AI, ML, and high performance computing applications. DLCs are pre-installed with EFA software that is tested and compatible with the underlying EC2 instances, so you don’t have to go through the hassle of setting up the underlying components yourself. For this post, we use setup scripts to create EKS clusters and EC2 instances that will support EFA out of the box.\nAWS DLCs with FSDP for enhanced memory efficiency\nOur solution uses PyTorch’s built-in support for Fully Sharded Data Parallel (FSDP) training, a cutting-edge technique that dramatically reduces memory requirements during training. Unlike traditional distributed training approaches where each GPU must hold a complete model copy, FSDP shards model parameters, optimizer states, and gradients across workers. The optimized implementation of FSDP within AWS DLCs makes it possible to train larger models with limited GPU resources while maintaining training efficiency.\nFor more information, see Scale LLMs with PyTorch 2.0 FSDP on Amazon EKS – Part 2.\nModel deployment on Amazon Bedrock\nFor model deployment, we use Amazon Bedrock, a fully managed service for FMs. Although we can use AWS DLCs for model hosting, we use Amazon Bedrock for this post to demonstrate diversity in service utilization.\nWeb automation integration\nFinally, we implement the SeeAct agent, a sophisticated web automation tool, and demonstrate its integration with our hosted model on Amazon Bedrock. This combination creates a powerful system capable of understanding visual inputs and executing complex web tasks autonomously, showcasing the practical applications of our fine-tuned model.In the following sections, we demonstrate how to:\n\nSet up an EKS cluster for AI workloads.\nUse AWS DLCs to fine-tune Meta Llama 3.2 Vision using PyTorch FSDP.\nDeploy the fine-tuned model on Amazon Bedrock.\nUse the model with SeeAct for web task automation.\n\nPrerequisites\nYou must have the following prerequisites:\n\nAn AWS account.\nAn AWS Identity and Access Management (IAM) role with appropriate policies. Because this post deals with creating clusters, nodes, and infrastructure, administrator-level permissions would work well. However, if you must have restricted permissions, you should at least have the following permissions: AmazonEC2FullAccess, AmazonSageMakerFullAccess, AmazonBedrockFullAccess, AmazonS3FullAccess, AWSCloudFormationFullAccess, AmazonEC2ContainerRegistryFullAccess. For more information about other IAM policies needed, see Minimum IAM policies.\nThe necessary dependencies installed for Amazon EKS. For instructions, see Set up to use Amazon EKS.\nFor this post, we use P5 instances. To request a quota increase, see Requesting a quota increase.\nAn EC2 key pair. For instructions, see Create a key pair for your Amazon EC2 instance.\n\nRun export AWS_REGION=<region_name> in your bash script from where you are running the commands.\nSet up the EKS cluster\nIn this section, we walk through the steps to create your EKS cluster and install the necessary plugins, operators, and other dependencies.\nCreate an EKS cluster\nThe simplest way to create an EKS cluster is to use the cluster configuration YAML file. You can use the following sample configuration file as a base and customize it as needed. Provide the EC2 key pair created as a prerequisite. For more configuration options, see Using Config Files.\n\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: MyCluster\n  region: us-west-2\n\nmanagedNodeGroups: \n  - name: p5\n    instanceType: p5.48xlarge\n    minSize: 0\n    maxSize: 2\n    desiredCapacity: 2\n    availabilityZones: [\"us-west-2a\"]\n    volumeSize: 1024\n    ssh:\n      publicKeyName: <your-ec2-key-pair>\n    efaEnabled: true\n    privateNetworking: true\n    ## In case you have an On Demand Capacity Reservation (ODCR) and want to use it, uncomment the lines below.\n    # capacityReservation:\n    #   capacityReservationTarget:\n    #     capacityReservationResourceGroupARN: arn:aws:resource-groups:us-west-2:897880167187:group/eks_blog_post_capacity_reservation_resource_group_p5\n\nRun the following command to create the EKS cluster:\neksctl create cluster --config-file cluster.yamlThe following is an example output:\n\nYYYY-MM-DD HH:mm:SS [ℹ] eksctl version x.yyy.z\nYYYY-MM-DD HH:mm:SS [ℹ] using region <region_name>\n...\nYYYY-MM-DD HH:mm:SS [✔] EKS cluster \"<cluster_name>\" in \"<region_name>\" region is ready\n\nCluster creation might take between 15–30 minutes. After it’s created, your local ~/.kube/config file gets updated with connection information to your cluster.\nRun the following command line to verify that the cluster is accessible:\nkubectl get nodes\nInstall plugins, operators, and other dependencies\nIn this step, you install the necessary plugins, operators and other dependencies on your EKS cluster. This is necessary to run the fine-tuning on the correct node and save the model.\n\nInstall the NVIDIA Kubernetes device plugin:\n\n\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.0/deployments/static/nvidia-device-plugin.yml\n\n\nInstall the AWS EFA Kubernetes device plugin:\n\n\nhelm repo add eks https://aws.github.io/eks-charts\ngit clone -b v0.0.190 https://github.com/aws/eks-charts.git\ncd  eks-charts/stable\nhelm install efa ./aws-efa-k8s-device-plugin -n kube-system\ncd ../..\n\n\nDelete aws-efa-k8s-device-plugin-daemonset by running the following command:\n\n\nkubectl delete daemonset aws-efa-k8s-device-plugin-daemonset -n kube-system\n\n\nClone the code locally that with help with setup and fine-tuning:\n\n\ngit clone https://github.com/aws-samples/aws-do-eks.git\ncd aws-do-eks\ngit checkout f59007ee50117b547305f3b8475c8e1b4db5a1d5\ncurl -L -o patch-aws-do-eks.tar.gz https://github.com/aws/deep-learning-containers/raw/refs/heads/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-aws-do-eks.tar.gz\nftar -xzf patch-aws-do-eks.tar.gz\ncd patch-aws-do-eks/\ngit am *.patch\ncd ../..\n\n\nInstall etcd for running distributed training with PyTorch:\n\n\nkubectl apply -f aws-do-eks/Container-Root/eks/deployment/etcd/etcd-deployment.yaml\n\n\nDeploy the FSx CSI driver for saving the model after fine-tuning: \n  \nEnter into the fsx folder: \n    \ncd aws-do-eks/Container-Root/eks/deployment/csi/fsx/\n \nEdit the fsx.conf file to modify the CLUSTER_NAME, CLUSTER_REGION, and CLUSTER_ZONE values to your cluster specific data: \n    \nvi fsx.conf\n \nDeploy the FSX CSI driver: \n    \n./deploy.sh\n \n \nDeploy the Kubeflow Training Operator that will be used to run the fine-tuning job: \n  \nChange the location to the following: \n    \ncd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/\n \nDeploy the Kubeflow Training Operator: \n    \n./deploy.sh\n \n \nDeploy the Kubeflow MPI Operator for running NCCL tests: \n  \nRun deploy.sh from the following GitHub repo.\nChange the location to the following: \n    \ncd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/\n \nDeploy the Kubeflow MPI Operator: \n    \n./deploy.sh\n \n \n\nFine-tune Meta Llama 3.2 Vision using DLCs on Amazon EKS\nThis section outlines the process for fine-tuning the Meta Llama 3.2 Vision model using PyTorch FSDP on Amazon EKS. We use the DLCs as the base image to run our training jobs.\nConfigure the setup needed for fine-tuning\nComplete the following steps to configure the setup for fine-tuning:\n\nCreate a Hugging Face account and get a Hugging Face security token.\nEnter into the fsdp folder:\n\n\ncd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp\n\n\nCreate a Persistent Volume Claim (PVC) that will use the underlying FSx CSI driver that you installed earlier:\n\n\nkubectl apply -f pvc.yaml\n\nMonitor kubectl get pvc fsx-claim and make sure it reached BOUND status. You can then go to the Amazon EKS console to see an unnamed volume created without a name. You can let this happen in the background, but before you run the ./run.sh command to run the fine-tuning job in a later step, make sure the BOUND status is achieved.\n\nTo configure the environment, open the .env file and modify the following variables: \n  \nHF_TOKEN: Add the Hugging Face token that you generated earlier.\nS3_LOCATION: Add the Amazon Simple Storage Service (Amazon S3) location where you want to store the fine-tuned model after the training is complete.\n \nCreate the required resource YAMLs:\n\n\n./deploy.sh\n\nThis line uses the values in the .env file to generate new YAML files that will eventually be used for model deployment.\n\nBuild and push the container image:\n\n\n./login-dlc.sh\n./build.sh\n./push.sh\n\nRun the fine-tuning job\nIn this step, we use the upstream DLCs and add the training scripts within the image for running the training.\nMake sure that you have requested access to the Meta Llama 3.2 Vision model on Hugging Face. Continue to the next step after permission has been granted.\nExecute the fine-tuning job:\n\n./run.sh\n\nFor our use case, the job took 1.5 hours to complete. The script uses the following PyTorch command that’s defined in the .env file within the fsdp folder:\n\n```\nbash\ntorchrun --nnodes 1 --nproc_per_node 8  \\\nrecipes/quickstart/finetuning/finetuning.py \\\n--enable_fsdp --lr 1e-5  --num_epochs 5 \\\n--batch_size_training 2 \\\n--model_name meta-llama/Llama-3.2-11B-Vision-Instruct \\\n--dist_checkpoint_root_folder ./finetuned_model \\\n--dist_checkpoint_folder fine-tuned  \\\n--use_fast_kernels \\\n--dataset \"custom_dataset\" --custom_dataset.test_split \"test\" \\\n--custom_dataset.file \"recipes/quickstart/finetuning/datasets/mind2web_dataset.py\"  \\\n--run_validation False --batching_strategy padding\n```\n\nYou can use the ./logs.sh command to see the training logs in both FSDP workers.\nAfter a successful run, logs from fsdp-worker will look as follows:\n\nSharded state checkpoint saved to /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct\nCheckpoint Time = 85.3276\n\nEpoch 5: train_perplexity=1.0214, train_epoch_loss=0.0211, epoch time 706.1626197730075s\ntraining params are saved in /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/train_params.yaml\nKey: avg_train_prep, Value: 1.0532150745391846\nKey: avg_train_loss, Value: 0.05118955448269844\nKey: avg_epoch_time, Value: 716.0386156642023\nKey: avg_checkpoint_time, Value: 85.34336999000224\nfsdp-worker-1:78:5593 [0] NCCL INFO [Service thread] Connection closed by localRank 1\nfsdp-worker-1:81:5587 [0] NCCL INFO [Service thread] Connection closed by localRank 4\nfsdp-worker-1:85:5590 [0] NCCL INFO [Service thread] Connection closed by localRank 0\nI0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\nI0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\nI0305 19:37:56.177000 140632318404416 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.0037238597869873047 seconds\n\nAdditionally:\n\n[rank8]:W0305 19:37:46.754000 139970058049344 torch/distributed/distributed_c10d.py:2429] _tensor_to_object size: 2817680 hash value: 9260685783781206407\nfsdp-worker-0:84:5591 [0] NCCL INFO [Service thread] Connection closed by localRank 7\nI0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\nI0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\nI0305 19:37:56.177000 139944709084992 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.05295562744140625 seconds\n\n\n\nRun the processing model and store output in Amazon S3\nAfter the jobs are complete, the fine-tuned model will exist in the FSx file system. The next step is to convert the model into Hugging Face format and save it in Amazon S3 so you can access and deploy the model in the upcoming steps:kubectl apply -f model-processor.yaml\nThe preceding command deploys a pod on your instance that will read the model from FSx, convert it to Hugging Face type, and push it to Amazon S3. It takes approximately 8–10 minutes for this pod to run. You can monitor the logs for this using ./logs.sh or kubectl logs -l app=model-processor.\nGet the location where your model has been stored in Amazon S3. This is the same Amazon S3 location that was mentioned the .env file in an earlier step. Run the following command (provide the Amazon S3 location):aws s3 cp tokenizer_config.json <S3_LOCATION>://tokenizer_config.json\nThis is the tokenizer config that is needed by Amazon Bedrock to import Meta Llama models so they work with the Amazon Bedrock Converse API. For more details, see Converse API code samples for custom model import.\nFor this post, we use the Mind2Web dataset. We have implemented code that has been adapted from the Mind2Web code for fine-tuning. The adapted code is as follows:\n\ngit clone https://github.com/meta-llama/llama-cookbook && \\\ncd llama-cookbook && \\\ngit checkout a346e19df9dd1a9cddde416167732a3edd899d09 && \\\ncurl -L -o patch-llama-cookbook.tar.gz https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-llama-cookbook.tar.gz && \\\ntar -xzf patch-llama-cookbook.tar.gz && \\\ncd patch-llama-cookbook && \\\ngit config --global user.email \"you@example.com\" && \\\ngit am *.patch && \\\ncd .. && \\\ncat recipes/quickstart/finetuning/datasets/mind2web_dataset.py\n\nDeploy the fine-tuned model on Amazon Bedrock\nAfter you fine-tune your Meta Llama 3.2 Vision model, you have several options for deployment. This section covers one deployment method using Amazon Bedrock. With Amazon Bedrock, you can import and use your custom trained models seamlessly. Make sure your fine-tuned model is uploaded to an S3 bucket, and it’s converted to Hugging Face format. Complete the following steps to import your fine-tuned Meta Llama 3.2 Vision model:\n\nOn the Amazon Bedrock console, under Foundation models in the navigation pane, choose Imported models.\nChoose Import model.\nFor Model name, enter a name for the model.\n\n\n\nFor Model import source, select Amazon S3 bucket.\nFor S3 location, enter the location of the S3 bucket containing your fine-tuned model.\n\n\n\nConfigure additional model settings as needed, then import your model.\n\nThe process might take 10–15 minutes depending on the model size to complete.\nAfter you import your custom model, you can invoke it using the same Amazon Bedrock API as the default Meta Llama 3.2 Vision model. Just replace the model name with your imported model’s Amazon Resource Name (ARN). For detailed instructions, refer to Amazon Bedrock Custom Model Import.\nYou can follow the prompt formats mentioned in the following GitHub repo. For example:\nWhat are the steps to build a docker image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nRun the agent workload using the hosted Amazon Bedrock model\nRunning the agent workload involves using the SeeAct framework and browser automation to start an interactive session with the AI agent and perform the browser operations. We recommend completing the steps in this section on a local machine for browser access.\nClone the SeeAct repository\nClone the customized SeeAct repository, which contains example code that can work with Amazon Bedrock, as well as a couple of test scripts:\n\ngit clone https://github.com/OSU-NLP-Group/SeeAct.git\n\nSet up SeeAct in a local runtime environment\nComplete the following steps to set up SeeAct in a local runtime environment:\n\nCreate a Python virtual environment for this demo. We use Python 3.11 in the example, but you can change to other Python versions.\n\n\npython3.11 -m venv seacct-python-3-11\nsource seacct-python-3-11/bin/activate\n\n\nApply a patch to add the code change needed for this demo:\n\n\ncd SeeAct\ncurl -O https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-seeact.patch\ngit checkout 2fdbf373f58a1aa5f626f7c5931fe251afc69c0a\ngit apply patch-seeact.patch\n\n\nRun the following commands to install the SeeAct package and dependencies:\n\n\ncd SeeAct/seeact_package\npip install .\npip install -r requirements.txt\npip install -U boto3\nplaywright install\n\nMake sure you’re using the latest version of Boto3 for these steps.\nValidate the browser automation tool used by SeeAct\nWe added a small Python script to verify the functionality of Playwright, the browser automation tool used by SeeAct:\n\ncd SeeAct/src\npython test_playwright.py\n\nYou should see a browser launched and closed after a few seconds. You should also see a screenshot being captured in SeeAct/src/example.png showing google.com.\n\nTest Amazon Bedrock model availability\nModify the content of test_bedrock.py. Update the MODEL_ID to be your hosted Amazon Bedrock model ARN and set up the AWS connection.\n\nexport AWS_ACCESS_KEY_ID=\"replace with your aws credential\"\nexport AWS_SECRET_ACCESS_KEY=\"replace with your aws credential\"\nexport AWS_SESSION_TOKEN=\"replace with your aws credential\"\n\nRun the test:\n\ncd SeeAct\npython test_bedrock.py\n\nAfter a successful invocation, you should see a log similar to the following in your terminal:\n\nThe image shows a dog lying down inside a black pet carrier, with a leash attached to the dog's collar.\n\nIf the botocore.errorfactory.ModelNotReadyException error occurs, retry the command in a few minutes.\nRun the agent workflow\nThe branch has already added support for BedrockEngine and SGLang for running inference with the fine-tuned Meta Llama 3.2 Vision model. The default option uses Amazon Bedrock inference.\nTo run the agent workflow, update self.model from src/demo_utils/inference_engine.py at line 229 to your Amazon Bedrock model ARN. Then run the following code:\n\ncd SeeAct/src\npython seeact.py -c config/demo_mode.toml \n\nThis will launch a terminal prompt like the following code, so you can input the task you want the agent to do:\n\nPlease input a task, and press Enter. \nOr directly press Enter to use the default task: Find pdf of paper \"GPT-4V(ision) is a Generalist Web Agent, if Grounded\" from arXiv\nTask: \n\nIn the following screenshot, we asked the agent to search for the website for DLCs.\n\nClean up\nUse the following code to clean the resources you created as part of this post:\n\ncd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp\nkubectl delete -f ./fsdp.yaml ## Deletes the training fsdp job\nkubectl delete -f ./etcd.yaml ## Deletes etcd\nkubectl delete -f ./model-processor.yaml ## Deletes model processing YAML\n\ncd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/\n./remove.sh\n\ncd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/\n./remove.sh\n\n## [VOLUME GETS DELETED] - If you want to delete the FSX volume\nkubectl delete -f ./pvc.yaml ## Deletes persistent volume claim, persistent volume and actual volume\n\nTo stop the P5 nodes and release them, complete the following steps:\n\nOn the Amazon EKS console, choose Clusters in the navigation pane.\nChoose the cluster that contains your node group.\nOn the cluster details page choose the Compute tab.\nIn the Node groups section, select your node group, then choose Edit.\nSet the desired size to 0.\n\nConclusion\nIn this post, we presented an end-to-end workflow for fine-tuning and deploying the Meta Llama 3.2 Vision model using the production-grade infrastructure of AWS. By using AWS DLCs on Amazon EKS, you can create a robust, secure, and scalable environment for model fine-tuning. The integration of advanced technologies like EFA support and FSDP training enables efficient handling of LLMs while optimizing resource usage. The deployment through Amazon Bedrock provides a streamlined path to production, and the integration with SeeAct demonstrates practical applications in web automation tasks. This solution serves as a comprehensive reference point for engineers to develop their own specialized AI applications, adapt the demonstrated approaches, and implement similar solutions for web automation, content analysis, or other domain-specific tasks requiring vision-language capabilities.\nTo get started with your own implementation, refer to our GitHub repo. To learn more about AWS DLCs, see the AWS Deep Learning Containers Developer Guide. For more details about Amazon Bedrock, see Getting started with Amazon Bedrock.\nFor deeper insights into related topics, refer to the following resources:\n\nScale LLMs with PyTorch 2.0 FSDP on Amazon EKS – Part 2\nBuild high-performance ML models using PyTorch 2.0 on AWS – Part 1\nMind2Web dataset\n\nNeed help or have questions? Join our AWS Machine Learning community on Discord or reach out to AWS Support. You can also stay updated with the latest developments by following the AWS Machine Learning Blog.\n\nAbout the Authors\nShantanu Tripathi is a Software Development Engineer at AWS with over 4 years of experience in building and optimizing large-scale AI/ML solutions. His experience spans developing distributed AI training libraries, creating and launching DLCs and Deep Learning AMIs, designing scalable infrastructure for high-performance AI workloads, and working on generative AI solutions. He has contributed to AWS services like Amazon SageMaker HyperPod, AWS DLCs, and DLAMIs, along with driving innovations in AI security. Outside of work, he enjoys theater and swimming.\nJunpu Fan is a Senior Software Development Engineer at Amazon Web Services, specializing in AI/ML Infrastructure. With over 5 years of experience in the field, Junpu has developed extensive expertise across the full cycle of AI/ML workflows. His work focuses on building robust systems that power ML applications at scale, helping organizations transform their data into actionable insights.\nHarish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He helps customers harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices.\nArindam Paul is a Sr. Product Manager in SageMaker AI team at AWS responsible for Deep Learning workloads on SageMaker, EC2, EKS, and ECS. He is passionate about using AI to solve customer problems. In his spare time, he enjoys working out and gardening.",
      "date": "2025-07-29",
      "authors": "Shantanu Tripathi",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article presents a comprehensive workflow for fine-tuning and deploying the Meta Llama 3.2 Vision large language model using AWS Deep Learning Containers on Amazon Elastic Kubernetes Service (EKS) and Amazon Bedrock. It details the use of advanced AI techniques such as PyTorch Fully Sharded Data Parallel training, high-performance networking, and integration with the SeeAct web automation agent, showcasing cutting-edge generative AI tools and infrastructure for scalable, production-grade AI model development and deployment.",
      "takeaways": [
        "- Introduces a production-grade infrastructure for fine-tuning and deploying generative AI models using AWS services.",
        "- Describes the application of advanced AI training techniques like FSDP for memory-efficient distributed training of large language models.",
        "- Demonstrates practical usage of the fine-tuned model for autonomous web automation tasks through integration with the SeeAct agent."
      ]
    },
    {
      "id": 8,
      "title": "How Nippon India Mutual Fund improved the accuracy of AI assistant responses using advanced RAG methods on Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-nippon-india-mutual-fund-improved-the-accuracy-of-ai-assistant-responses-using-advanced-rag-methods-on-amazon-bedrock/",
      "description": "This post is co-written with Abhinav Pandey from Nippon Life India Asset Management Ltd.\nAccurate information retrieval through generative AI-powered assistants is a popular use case for enterprises. To reduce hallucination and improve overall accuracy, Retrieval Augmented Generation (RAG) remains the most commonly used method to retrieve reliable and accurate responses that use enterprise data when responding to user queries. RAG is used for use cases such as AI assistants, search, real-time insights, and improving overall content quality by using the relevant data to generate the response, thereby reducing hallucinations.\nAmazon Bedrock Knowledge Bases provides a managed RAG experience that can be used for many use cases. Amazon Bedrock Knowledge Bases is a fully managed service that does the heavy lifting of implementing a RAG pattern—including data ingestion, data chunking, data embedding, and query matching. Amazon Bedrock offers a choice of high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock Knowledge Bases, you can create a RAG solution quickly and seamlessly.\nHowever, in a large enterprise scenario with a large number of relevant documents, the final response is generated based on only the top 5 or top 10 results provided by the database. Because RAG uses a similarity match and not an exact match, there is a possibility that the most relevant result won’t be in the top results returned by the database. In such scenarios, the regular RAG pattern might not be highly accurate.\nIn this post, we examine a solution adopted by Nippon Life India Asset Management Limited that improves the accuracy of the response over a regular (naive) RAG approach by rewriting the user queries and aggregating and reranking the responses. The proposed solution uses enhanced RAG methods such as reranking to improve the overall accuracy.\nLimitations and workarounds in naive RAG for a large volume of documents\nThe following are the primary limitations with naive RAG when using it with a large volume of documents:\n\nAccuracy – As the documents grow in number or size, the final list of extracted chunks might miss some relevant sections or documents because of the limited number of documents returned\nParsing complex structures – Entities such as nested tables, images, and graphs are not parsed accurately\nLimited use of relevant data – As the dataset grows, only the configured set of top results are used for context, impacting accuracy\nResponding to complex questions – Compound questions (such as a question with multiple sub-questions) pose challenges to the RAG solution\nRetrieving the correct context – Documents such as legal documents or technical manuals have semantically related sections on different pages, impacting the overall accuracy\nAvoiding hallucinations – Models must generate complete, correct, and grounded responses without hallucinations\n\nTo address these challenges, developers usually adopt a combination of the following workarounds:\n\nProgrammatic parsing – Use another service (such as Amazon Textract) to extract the table content into a markdown (.MD) file.\nProgrammatic split of compound questions – Programmatically split the question or reformulate the question and get the responses and then programmatically aggregate the response.\nProgrammatic chunking – Programmatically create custom chunks of the documents and manage them in a vector store.\n\nSolution overview\nIn this section, we review the basics of RAG, Amazon Bedrock Knowledge Bases, and advanced RAG methods to address the preceding challenges. The following table details the specific solution components adopted by Nippon to overcome the challenges we discussed in the previous section.\n\n\n\nNaive RAG Challenges\nHow Nippon Addressed These Challenges\n\n\nLower accuracy due to high volume of documents\nUse Amazon Bedrock Knowledge Bases with advanced RAG methods, including semantic chunking, multi-query RAG, and results reranking\n\n\nParsing complex document structure such as nested tables and graphs within documents\nUse Amazon Textract to parse the documents into markdown files\n\n\nHandling compound questions\nUse query reformulation and results reranking to get the relevant results\n\n\nRetrieving current context from the documents\nUse semantic chunking in Amazon Bedrock Knowledge Bases\n\n\nGenerating grounded responses without any hallucination\nUse Amazon Bedrock Knowledge Bases RAG evaluation\n\n\n\nIn the next few sections, we detail each of the solution components. We start with an overview of RAG and then discuss Amazon Bedrock Knowledge Bases. We then discuss advanced RAG methods adopted by Nippon, including advanced parsing, query reformulation, multi-query RAG, and results reranking.\nRAG overview\nThe RAG pattern for this solution has two primary workflows: a data ingestion workflow and a text generation phase, as depicted in the following figure. During the ingestion phase, the solution chunks the content of the source documents, creates embeddings for the created chunks, and stores them in a vector database. In the text generation workflow, the user query is converted to a vector embedding and the query embedding is compared with embeddings stored in the vector database. The database provides the top results that are close to the user query. The solution then sends the user query along with the retrieved top results as the context to the FM, which then provides the final response. Because the responses are based on the relevant contextual enterprise data, this solution reduces hallucinations.\n\n\nIngestion & Text generation workflows\n\nYou can implement this solution using AWS services as shown in the following figure.\n\n\nRAG implementation using AWS services\n\nThe following is a typical RAG-based AI assistant flow.The first series of steps, as numbered in the preceding diagram, augment the user query using a knowledge base:\n\nUser queries are served by Amazon Lex hosted on Amazon CloudFront.\nThe business logic for the AI assistant can run on Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), or AWS Lambda.\nYou can manage the conversation history or state in Amazon DynamoDB, Amazon DocumentDB (with MongoDB compatibility), or Amazon MemoryDB.\nThe user query and the earlier conversation history are sent to the Amazon Bedrock embedding model.\nUser prompts are compared to earlier prompts cached in MemoryDB. If the user prompt matches a stored prompt, the stored prompt result is retrieved and sent to the user.\nIf there’s no match for the user prompt in the cache, the vector database (such as Amazon OpenSearch, Amazon Aurora, Amazon Kendra, MemoryDB, DocumentDB, or Amazon Neptune) is searched for the query embedding. The relevant knowledge or Amazon Simple Storage Service (Amazon S3) path to the relevant image or video is retrieved.\nThe relevant knowledge, image, or video is retrieved from the Amazon S3 path.\nThe multimodal large language model (LLM) in Amazon Bedrock uses the query results to create a final response.\nThe new answer is stored in the database cache.\nThe final response is sent to the user.\n\nThe workflow to create and maintain the knowledge base is shown in the previous diagram:\n\nThe data is ingested from structured and unstructured sources such as a file system, emails, or multi-media content.\nThe data is ingested into Amazon S3 in batches or in real time using services such as AWS Database Migration Service (AWS DMS), Amazon Kinesis, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon AppFlow, or AWS Glue.\nThe data is processed through AWS Glue, Amazon SageMaker processing, or Amazon EMR.\nAn embeddings LLM from Amazon Bedrock generates the embeddings for the content chunks.\nThe embeddings are stored in the vector database.\n\nAmazon Bedrock Knowledge Bases\nAmazon Bedrock Knowledge Bases provides managed RAG, which does the heavy lifting of the core activities in the data ingestion and text generation workflows. Nippon uses Amazon Bedrock Knowledge Bases for implementing the RAG pattern. The following figure shows the fully managed data ingestion workflow provided by Amazon Bedrock Knowledge Bases. You can choose the data source from which data of various formats can be incrementally updated. For content chunking, you can choose from the following strategies: fixed, semantic, or hierarchical. You can also choose the embedding models and vector store.\n\n\nData ingestion flow\n\nThe following table compares various chunking strategies. You can also author a custom chunking strategy using Lambda.\n\n\n\nChunking strategy\nDetails\nAdvantages\n\n\nFixed chunking\n\n\nFixed character count division\nRecursive character text splitting\n \nQuick and efficient\n\n\nSemantic chunking (used by Nippon)\n\n\nMeaningful and complete chunks based on semantic similarity\n \nBetter retrieval quality\n\n\nHierarchical chunking\n\n\nOrganize chunks in a structured manner\nNodes with summaries and parent-child relationships\n \nImproved retrieval efficiency and reliability\n\n\n\nNippon uses semantic chunking because the documents have sections with semantic similarity. During the text generation workflow, Amazon Bedrock Knowledge Bases creates embeddings from the user input and performs semantic search in the Amazon Bedrock knowledge base. The retrieved results from the context are augmented with a prompt for the LLM to generate the final response as shown in the following figure.\n\n\nRAG Flow\n\nAdvanced RAG methods adopted by Nippon\nIn this section, we review the key advanced RAG methods used in the Nippon AI assistant solution.\nAdvanced parsing and chunking\nA complex document with tables, images, and graphs poses challenges in RAG because the default fixed chunking often loses the context. For these scenarios, Nippon uses the following approaches:\n\nParse the document with Amazon Textract – Nippon uses Amazon Textract—which excels at understanding complex structures like tables—to extract the details from the document into a markdown file (.MD).\nParse the document with an FM with specific instructions – In this approach, Nippon uses an FM that has specific parsing instructions to extract the details from the document. Amazon Bedrock Knowledge Bases provides LLMs along with instructions for parsing the documents.\nParse the document using third-party parsers – This approach uses third-party parsers such as open source document parsers and then indexed the parsed content.\n\nAfter the documents are parsed, Amazon Bedrock Knowledge Bases chunks the data. Amazon Bedrock Knowledge Bases provides fixed chunking, semantic chunking, and hierarchical chunking strategies. You can also add custom chunking using Lambda functions for specific documents.\nQuery reformulation\nIf the user queries are complex, the overall accuracy of the RAG solution decreases because it can be difficult to understand all the nuances of the complex question. In such cases, you can split the complex question into multiple sub-questions for improved accuracy. Nippon used an LLM to split the compound query into multiple sub-queries. The search results for each of the sub-queries are returned simultaneously and are then ranked and combined. The LLM uses the combined data to provide the final response, as shown in the following figure.\n\n\nQuery reformulation\n\nMulti-query RAG\nIn the multi-query RAG approach, you can reformulate the question into different variants (Amazon Bedrock Knowledge Bases provides automatic query reformulation) and execute the queries in parallel. You then summarize the results of those queries and rerank them to get the final response as shown in the following figure.\n\n\nMulti-query RAG\n\nNippon uses the initial question and break it into n sub-questions using our choice of LLM (Anthropic’s Claude3 Sonnet on Amazon Bedrock). With this implementation, the quality of our responses has improved, and the query is answered in greater depth. Because this requires additional processing, the response time is slightly longer, which is justified by the quality of the response.\nResults reranking\nIn a typical enterprise scenario where you have a large volume of documents, the ranking of the results might not be correct and accuracy could be reduced. For example, the default results returned from a vector database will be based on vector similarity criteria whose rank might not be accurate or relevant based on the user’s conversation history or the query context. In such scenarios, you can use another FM to look closely at the results and reorder them based on analysis of the context, relevance, and other criteria. After the results are reranked, you can be certain that the top results are highly relevant to the user context. Each document or search result is assigned a score based on criteria such as semantic similarity, contextual alignment, or domain-specific features, thereby increasing the overall accuracy of the final response, reducing hallucination, and improving user satisfaction. The key reranker strategies are:\n\nCross-encoder reranking – Uses models specifically trained to assess the relevance between query-document pairs\nHybrid reranking – Combines multiple signals such as vector similarity, keyword matching, recency, and so on\nMulti-stage retrieval – Retrieves a larger set using faster methods, then applies more computationally intensive reranking on a subset\n\nAmazon Bedrock provides reranker models that calculate the relevance of the chunks for a given query, assigns a score for the chunk, and reorders the results based on the score. By using a reranker model in Amazon Bedrock, you can get highly relevant chunks in the search results, improving the overall response accuracy. Nippon has combined query reformulation with results reranking to improve the overall accuracy of the RAG solution using the Amazon Bedrock reranker models.\nMetrics for RAG Evaluation\nBecause RAG solutions have multiple moving parts, you need to evaluate them against key success metrics to make sure that you get relevant and contextual data from the knowledge base. RAG evaluation should validate the generation of complete, correct, and grounded answers without hallucinations. You also need to evaluate the bias, safety, and trust of RAG solutions.\nAmazon Bedrock Knowledge Bases provides built-in support for RAG evaluation, including quality metrics such as correctness, completeness, and faithfulness (hallucination detection); responsible AI metrics such as harmfulness, answer refusal, and stereotyping; and compatibility with Amazon Bedrock Guardrails. Nippon used RAG evaluation to compare against multiple evaluation jobs using custom datasets.\nNippon is currently evaluating additional techniques, including GraphRAG, metadata filtering, and agentic AI. We have briefly summarized the capabilities being evaluated at Nippon in the following sections.\nGraphRAG\nFor applications that need to establish relationships with data that are hierarchically related (such as knowledge management, enterprise search, recommendation systems, and so on), Nippon uses a graph database instead of a vector database. GraphRAG applications use rich and interconnected entity relationships to identify the dependencies and excel in querying multi-dimensional relationships that further boost the context for FMs. In addition, with graph databases, Nippon can efficiently query and traverse the data, and their schema enables them to accommodate dynamic content. The graph databases help the FMs better understand the semantic relationship, uncover hidden patterns, adapt to dynamic knowledge bases, and provide better reasonability.\nAmazon Bedrock Knowledge Bases supports fully managed GraphRAG, which uses the Amazon Neptune graph database.\nMetadata filtering\nIn a few use cases, you might need to filter the documents in a knowledge base based on specific metadata values. For instance, government regulatory bodies release regulatory guidelines frequently, often with the same document names and only minor variations in the regulatory clauses. When your query is about a specific regulation, you want the most recent document to be returned. In such cases, you can rank the documents based on their modified date. Amazon Bedrock Knowledge Bases provide custom metadata as filters (such as modified date) to improve the quality of search results.\nAmazon Bedrock Agents\nWith Amazon Bedrock Agents, you can orchestrate multi-step business processes using tools and information sources. Nippon is currently evaluating the latest FMs in Amazon Bedrock Agents for their AI assistant use case.\nSolution flow\nThe following diagram shows the end-to-end flow of the Nippon AI assistant solution.\n\n\nEnd-to-end flow of the Nippon AI assistant solution\n\nThe data ingestion workflow consists of the following steps:\n\nThe documents from the data source are prepared for ingestion.\nNippon uses custom parsing to extract the relevant details from complex elements like tables, graphs, and images. They use custom chunking to create the chunks.\nThey use an embedding model from Amazon Bedrock to convert the content chunks to vector embeddings and store them in the vector database.\n\nThe content generation workflow is as follows:\n\nThe user’s query is converted into embeddings by the Amazon Bedrock embedding model.\nNippon uses multi-query RAG by creating multiple variants of the query and executing them in parallel. The obtained results are reranked using a reranker model for higher accuracy.\nThe prompt is augmented with the references from the source documents to create citations\nThe augmented prompt is sent to Amazon Bedrock.\nAmazon Bedrock creates and sends the final response to the user.\n\nNippon plans to use agentic AI implementation in the future for automating the data retrieval, indexing, and ingestion.\nResults\nNippon saw the following improvements after implementing RAG:\n\nAccuracy was increased by more than 95%\nHallucination was reduced by 90–95%\nThey were able to add source chunks and file links (through file metadata), which improves the user confidence in the response\nThe time needed to generate a report was reduced from 2 days to approximately 10 minutes\n\nSummary\nIn this post, we discussed RAG and some of the challenges associated with processing large volumes of documents. We explained the advanced RAG methods used in the Nippon AI assistant, including enhanced parsing using Amazon Bedrock Knowledge Bases and third-party models. In addition, we explained query reformulation and multi-query RAG techniques—such as generating multiple queries, reranking results, using GraphRAG, and applying metadata filtering. Finally, we described the end-to-end solution implemented for the Nippon AI assistant. These methods are generally available and are not built by or belong only to Nippon.\nExplore Amazon Bedrock Knowledge Bases for RAG use cases by using advanced RAG features such as FM as a parser, query reformulation, reranker models, GraphRAG, and others to implement highly accurate RAG solutions. You can also use Amazon Bedrock Guardrails to build responsible AI solutions by enforcing content and image safeguards and enabling automated reasoning checks.\nPlease note that RAG methods mentioned in the blog are generally available to all and are not built by or belong only to Nippon. \n\nAbout the authors\nShailesh Shivakumar is an FSI Sr. Solutions Architect with AWS India. He works with financial enterprises such as banks, NBFCs, and trading enterprises to help them design secure cloud systems and accelerate their cloud journey. He builds demos and proofs of concept to demonstrate the art of the possible on the AWS Cloud. He leads other initiatives such as customer enablement workshops, AWS demos, cost optimization, and solution assessments to make sure AWS customers succeed in their cloud journey. Shailesh is part of Machine Learning TFC at AWS, handling generative AI and machine learning-focused customer scenarios. Security, serverless, containers, and machine learning in the cloud are his key areas of interest.\nAbhinav Pandey is a seasoned Data Scientist on the Technology team at Nippon Life India Asset Management Ltd, with over 18 years of industry experience, primarily in BFSI, who is passionate about using generative AI and agentic AI to transform business operations. With a proven track record of applying data for strategic decision-making and business growth, he excels at extracting actionable insights from complex datasets using cutting-edge analytical techniques. A strategic thinker and innovative problem solver, he has developed data-driven strategies that enhance operational efficiency and profitability while working effectively with cross-functional teams to align data initiatives with business objectives. In his leadership role, he has driven data science initiatives, fostered innovation, maintained a results-driven approach, and continuously adapted to evolving technologies to stay ahead of industry trends.",
      "date": "2025-07-29",
      "authors": "Shailesh Shivakumar",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article details how Nippon India Mutual Fund enhanced the accuracy and reliability of AI assistant responses by using advanced Retrieval Augmented Generation (RAG) methods on Amazon Bedrock, including techniques like semantic chunking, query reformulation, multi-query RAG, and results reranking. It highlights the implementation of these AI advancements in enterprise knowledge management and the significant improvements in reducing hallucinations and improving response accuracy.",
      "takeaways": [
        "- Nippon implemented advanced RAG techniques on Amazon Bedrock to overcome challenges of processing large volumes of complex enterprise documents.",
        "- Techniques such as semantic chunking, multi-query reformulation, and results reranking significantly increased accuracy and reduced hallucinations in AI-generated responses.",
        "- The article also covers use of AI tools like Amazon Textract for parsing complex documents and evaluation metrics for responsible and reliable generative AI deployments."
      ]
    },
    {
      "id": 9,
      "title": "Build a drug discovery research assistant using Strands Agents and Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-a-drug-discovery-research-assistant-using-strands-agents-and-amazon-bedrock/",
      "description": "Drug discovery is a complex, time-intensive process that requires researchers to navigate vast amounts of scientific literature, clinical trial data, and molecular databases. Life science customers like Genentech and AstraZeneca are using AI agents and other generative AI tools to increase the speed of scientific discovery. Builders at these organizations are already using the fully managed features of Amazon Bedrock to quickly deploy domain-specific workflows for a variety of use cases, from early drug target identification to healthcare provider engagement.\nHowever, more complex use cases might benefit from using the open source Strands Agents SDK. Strands Agents takes a model-driven approach to develop and run AI agents. It works with most model providers, including custom and internal large language model (LLM) gateways, and agents can be deployed where you would host a Python application.\nIn this post, we demonstrate how to create a powerful research assistant for drug discovery using Strands Agents and Amazon Bedrock. This AI assistant can search multiple scientific databases simultaneously using the Model Context Protocol (MCP), synthesize its findings, and generate comprehensive reports on drug targets, disease mechanisms, and therapeutic areas. This assistant is available as an example in the open-source healthcare and life sciences agent toolkit for you to use and adapt.\nSolution overview\nThis solution uses Strands Agents to connect high-performing foundation models (FMs) with common life science data sources like arXiv, PubMed, and ChEMBL. It demonstrates how to quickly create MCP servers to query data and view the results in a conversational interface.\nSmall, focused AI agents that work together can often produce better results than a single, monolithic agent. This solution uses a team of sub-agents, each with their own FM, instructions, and tools. The following flowchart shows how the orchestrator agent (shown in orange) handles user queries and routes them to sub-agents for either information retrieval (green) or planning, synthesis, and report generation (purple).\n\nThis post focuses on building with Strands Agents in your local development environment. Refer to the Strands Agents documentation to deploy production agents on AWS Lambda, AWS Fargate, Amazon Elastic Kubernetes Service (Amazon EKS), or Amazon Elastic Compute Cloud (Amazon EC2).\nIn the following sections, we show how to create the research assistant in Strands Agents by defining an FM, MCP tools, and sub-agents.\nPrerequisites\nThis solution requires Python 3.10+, strands-agents, and several additional Python packages. We strongly recommend using a virtual environment like venv or uv to manage these dependencies.\nComplete the following steps to deploy the solution to your local environment:\n\nClone the code repository from GitHub.\nInstall the required Python dependencies with pip install -r requirements.txt.\nConfigure your AWS credentials by setting them as environment variables, adding them to a credentials file, or following another supported process.\nSave your Tavily API key to a .env file in the following format: TAVILY_API_KEY=\"YOUR_API_KEY\".\n\nYou also need access to the following Amazon Bedrock FMs in your AWS account:\n\nAnthropic’s Claude 3.7 Sonnet\nAnthropic’s Claude 3.5 Sonnet\nAnthropic’s Claude 3.5 Haiku\n\nDefine the foundation model\nWe start by defining a connection to an FM in Amazon Bedrock using the Strands Agents BedrockModel class. We use Anthropic’s Claude 3.7 Sonnet as the default model. See the following code:\n\nfrom strands import Agent, tool\nfrom strands.models import BedrockModel\nfrom strands.agent.conversation_manager import SlidingWindowConversationManager\nfrom strands.tools.mcp import MCPClient\n# Model configuration with Strands using Amazon Bedrock's foundation models\ndef get_model():\n    model = BedrockModel(\n        boto_client_config=Config(\n            read_timeout=900,\n            connect_timeout=900,\n            retries=dict(max_attempts=3, mode=\"adaptive\"),\n        ),\n        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        max_tokens=64000,\n        temperature=0.1,\n        top_p=0.9,\n        additional_request_fields={\n            \"thinking\": {\n                \"type\": \"disabled\"  # Can be enabled for reasoning mode\n            }\n        }\n    )\n    return model\n\nDefine MCP tools\nMCP provides a standard for how AI applications interact with their external environments. Thousands of MCP servers already exist, including those for life science tools and datasets. This solution provides example MCP servers for:\n\narXiv – Open-access repository of scholarly articles\nPubMed – Peer-reviewed citations for biomedical literature\nChEMBL – Curated database of bioactive molecules with drug-like properties\nClinicalTrials.gov – US government database of clinical research studies\nTavily Web Search – API to find recent news and other content from the public internet\n\nStrands Agents streamlines the definition of MCP clients for our agent. In this example, you connect to each tool using standard I/O. However, Strands Agents also supports remote MCP servers with Streamable-HTTP Events transport. See the following code:\n\n# MCP Clients for various scientific databases\ntavily_mcp_client = MCPClient(lambda: stdio_client(\n    StdioServerParameters(command=\"python\", args=[\"application/mcp_server_tavily.py\"])\n))\narxiv_mcp_client = MCPClient(lambda: stdio_client(\n    StdioServerParameters(command=\"python\", args=[\"application/mcp_server_arxiv.py\"])\n))\npubmed_mcp_client = MCPClient(lambda: stdio_client(\n    StdioServerParameters(command=\"python\", args=[\"application/mcp_server_pubmed.py\"])\n))\nchembl_mcp_client = MCPClient(lambda: stdio_client(\n    StdioServerParameters(command=\"python\", args=[\"application/mcp_server_chembl.py\"])\n))\nclinicaltrials_mcp_client = MCPClient(lambda: stdio_client(\n    StdioServerParameters(command=\"python\", args=[\"application/mcp_server_clinicaltrial.py\"])\n))\n\nDefine specialized sub-agents\nThe planning agent looks at user questions and creates a plan for which sub-agents and tools to use:\n\n@tool\ndef planning_agent(query: str) -> str:\n    \"\"\"\n    A specialized planning agent that analyzes the research query and determines\n    which tools and databases should be used for the investigation.\n    \"\"\"\n    planning_system = \"\"\"\n    You are a specialized planning agent for drug discovery research. Your role is to:\n    \n    1. Analyze research questions to identify target proteins, compounds, or biological mechanisms\n    2. Determine which databases would be most relevant (Arxiv, PubMed, ChEMBL, ClinicalTrials.gov)\n    3. Generate specific search queries for each relevant database\n    4. Create a structured research plan\n    \"\"\"\n    model = get_model()\n    planner = Agent(\n        model=model,\n        system_prompt=planning_system,\n    )\n    response = planner(planning_prompt)\n    return str(response)\n\nSimilarly, the synthesis agent integrates findings from multiple sources into a single, comprehensive report:\n\n@tool\ndef synthesis_agent(research_results: str) -> str:\n    \"\"\"\n    Specialized agent for synthesizing research findings into a comprehensive report.\n    \"\"\"\n    system_prompt = \"\"\"\n    You are a specialized synthesis agent for drug discovery research. Your role is to:\n    \n    1. Integrate findings from multiple research databases\n    2. Create a comprehensive, coherent scientific report\n    3. Highlight key insights, connections, and opportunities\n    4. Organize information in a structured format:\n       - Executive Summary (300 words)\n       - Target Overview\n       - Research Landscape\n       - Drug Development Status\n       - References\n    \"\"\"\n    model = get_model()\n    synthesis = Agent(\n        model=model,\n        system_prompt=system_prompt,\n    )\n    response = synthesis(synthesis_prompt)\n    return str(response)\n\nDefine the orchestration agent\nWe also define an orchestration agent to coordinate the entire research workflow. This agent uses the SlidingWindowConversationManager class from Strands Agents to store the last 10 messages in the conversation. See the following code:\n\ndef create_orchestrator_agent(\n    history_mode,\n    tavily_client=None,\n    arxiv_client=None,\n    pubmed_client=None,\n    chembl_client=None,\n    clinicaltrials_client=None,\n):\n    system = \"\"\"\n    You are an orchestrator agent for drug discovery research. Your role is to coordinate a multi-agent workflow:\n    \n    1. COORDINATION PHASE:\n       - For simple queries: Answer directly WITHOUT using specialized tools\n       - For complex research requests: Initiate the multi-agent research workflow\n    \n    2. PLANNING PHASE:\n       - Use the planning_agent to determine which databases to search and with what queries\n    \n    3. EXECUTION PHASE:\n       - Route specialized search tasks to the appropriate research agents\n    \n    4. SYNTHESIS PHASE:\n       - Use the synthesis_agent to integrate findings into a comprehensive report\n       - Generate a PDF report when appropriate\n    \"\"\"\n    # Aggregate all tools from specialized agents and MCP clients\n    tools = [planning_agent, synthesis_agent, generate_pdf_report, file_write]\n    # Dynamically load tools from each MCP client\n    if tavily_client:\n        tools.extend(tavily_client.list_tools_sync())\n    # ... (similar for other clients)\n    conversation_manager = SlidingWindowConversationManager(\n        window_size=10,  # Maintains context for the last 10 exchanges\n    )\n    orchestrator = Agent(\n        model=model,\n        system_prompt=system,\n        tools=tools,\n        conversation_manager=conversation_manager\n    )\n    return orchestrator\n\nExample use case: Explore recent breast cancer research\nTo test out the new assistant, launch the chat interface by running streamlit run application/app.py and opening the local URL (typically http://localhost:8501) in your web browser. The following screenshot shows a typical conversation with the research agent. In this example, we ask the assistant, “Please generate a report for HER2 including recent news, recent research, related compounds, and ongoing clinical trials.” The assistant first develops a comprehensive research plan using the various tools at its disposal. It decides to start with a web search for recent news about HER2, as well as scientific articles on PubMed and arXiv. It also looks at HER2-related compounds in ChEMBL and ongoing clinical trials. It synthesizes these results into a single report and generates an output file of its findings, including citations.\n\nThe following is an excerpt of a generated report:\n\nComprehensive Scientific Report: HER2 in Breast Cancer Research and Treatment\n1. Executive Summary\nHuman epidermal growth factor receptor 2 (HER2) continues to be a critical target in breast cancer research and treatment development. This report synthesizes recent findings across the HER2 landscape highlighting significant advances in understanding HER2 biology and therapeutic approaches. The emergence of antibody-drug conjugates (ADCs) represents a paradigm shift in HER2-targeted therapy, with trastuzumab deruxtecan (T-DXd, Enhertu) demonstrating remarkable efficacy in both early and advanced disease settings. The DESTINY-Breast11 trial has shown clinically meaningful improvements in pathologic complete response rates when T-DXd is followed by standard therapy in high-risk, early-stage HER2+ breast cancer, potentially establishing a new treatment paradigm.\n\nNotably, you don’t have to define a step-by-step process to accomplish this task. By providing the assistant with a well-documented list of tools, it can decide which to use and in what order.\nClean up\nIf you followed this example on your local computer, you will not create new resources in your AWS account that you need to clean up. If you deployed the research assistant using one of those services, refer to the relevant service documentation for cleanup instructions.\nConclusion\nIn this post, we showed how Strands Agents streamlines the creation of powerful, domain-specific AI assistants. We encourage you to try this solution with your own research questions and extend it with new scientific tools. The combination of Strands Agents’s orchestration capabilities, streaming responses, and flexible configuration with the powerful language models of Amazon Bedrock creates a new paradigm for AI-assisted research. As the volume of scientific information continues to grow exponentially, frameworks like Strands Agents will become essential tools for drug discovery.\nTo learn more about building intelligent agents with Strands Agents, refer to Introducing Strands Agents, an Open Source AI Agents SDK, Strands Agents SDK, and the GitHub repository. You can also find more sample agents for healthcare and life sciences built on Amazon Bedrock.\nFor more information about implementing AI-powered solutions for drug discovery on AWS, visit us at AWS for Life Sciences.\n\nAbout the authors\nHasun Yu is an AI/ML Specialist Solutions Architect with extensive expertise in designing, developing, and deploying AI/ML solutions for healthcare and life sciences. He supports the adoption of advanced AWS AI/ML services, including generative and agentic AI.\nBrian Loyal is a Principal AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 20 years’ experience in biotechnology and machine learning and is passionate about using AI to improve human health and well-being.",
      "date": "2025-07-28",
      "authors": "Brian Loyal",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article demonstrates how to build a drug discovery research assistant using Strands Agents, an open-source AI agents SDK, combined with Amazon Bedrock's foundation models to orchestrate multiple AI sub-agents for searching scientific databases, synthesizing research findings, and generating comprehensive reports. It highlights advances in AI-driven workflows and tools tailored for accelerating drug discovery and scientific research in life sciences.",
      "takeaways": [
        "- Introduces Strands Agents as a flexible framework for building domain-specific AI research assistants using foundation models such as Anthropic’s Claude on Amazon Bedrock.",
        "- Showcases orchestration of specialized sub-agents and MCP tools that query multiple scientific databases (e.g., PubMed, arXiv, ChEMBL) to gather and synthesize drug discovery relevant information.",
        "- Demonstrates an AI-powered multi-agent workflow for navigating complex research questions, generating structured reports, and accelerating drug discovery processes."
      ]
    },
    {
      "id": 10,
      "title": "Amazon Nova Act SDK (preview): Path to production for browser automation agents",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-nova-act-sdk-preview-path-to-production-for-browser-automation-agents/",
      "description": "In early 2025, we introduced the Amazon Nova Act SDK as a research preview to help developers build agents that reliably complete tasks in a web browser. Now, we are excited to work with customers to take their agents to production in a limited preview, using new AWS integrations including AWS Identity and Access Management (IAM) for secure credentialing, Amazon Simple Storage Service (Amazon S3) for data storage and policy control, and the new Amazon Bedrock AgentCore Browser Tool for scalable, cloud-based browser execution.\nIn this post, we walk through what makes the Amazon Nova Act SDK unique, how it works, and how teams across industries are already using it to automate browser-based workflows at scale.\nChallenges with traditional automated business workflows\nMany day-to-day business operations require a browser, such as submitting time-off requests, processing invoices, accessing vendor portals, or reviewing dashboards. Lack of API coverage often means workflows are done manually: teams copy-paste data across tabs, follow multi-step flows, and click through countless interfaces to get work done.\nTraditional rules-based browser automation frameworks often face challenges in dynamic web environments. Teams can spend more time on ongoing maintenance than on building new automations, because changes in page structure (for example, newly added form fields or dropdown options) break brittle selectors. Most importantly, these frameworks are difficult to scale. If one use case is performed on 50 different sites (for example, professional license verification on state websites), teams must build 50 site-specific automations, because rules-based frameworks don’t generalize.\nAs humans, our ability to perform tasks adapts across different tools and interfaces. For example, once you know how to draft an email in Outlook, you can easily do the same in Gmail—even if you’ve never used it before. Large language models (LLMs), trained on millions of examples of UIs, offer the potential to create a similar type of pattern recognition for AI agents. They’ve brought us this far—powering chat, summarization, coding copilots, and more—by interpreting language, following instructions, and reasoning across domains. Now, we’re entering the next phase of generative AI: one centered on action. Agentic AI builds on the foundation of LLMs to move from understanding to execution. These systems are designed to complete dynamic, multi-step workflows—like filling out complex forms, interacting with evolving UI elements, or performing real-world business tasks at scale. Agentic AI doesn’t replace LLMs—it extends them, unlocking new automation capabilities that bring us closer to real task completion in enterprise environments.\nAgentic AI with the Amazon Nova Act SDK\nWith the Amazon Nova Act SDK, you can build and deploy reliable browser agents powered by the Amazon Nova Act model—purpose-built and fine-tuned for agentic AI. Trained with reinforcement learning and extensive in-domain browser interaction data, it executes step-by-step workflows with precision. With this latest version, we’ve extended those capabilities with AWS integrations so you can take your agents from prototype to production. You can install the SDK with a single command, write agents in Python and natural language, debug in real time, and integrate directly into continuous integration and delivery (CI/CD) pipelines. With enterprise-grade security, observability, and infrastructure now available through AWS, the Amazon Nova Act SDK provides a fast, flexible path for teams looking to build agents that act—and deliver—at scale. You can use the Amazon Nova Act SDK to automate real-world workflows where traditional scripts or general-purpose models aren’t reliable or scalable enough. You can install it with a single command, write agents using a combination of Python and natural language, debug while the workflow runs, and deploy through CI/CD pipelines.\nThe Amazon Nova Act SDK also integrates with the new Amazon Bedrock AgentCore Browser Tool—a fast, secure, cloud-based browser that enables AI agents to interact with websites at scale. It includes enterprise-grade security features, including virtual machine-level isolation and federated identity integration. The tool offers built-in observability through live viewing, AWS CloudTrail logging, and session replay to troubleshoot, maintain quality, and support compliance.\nBenefits of the Amazon Nova Act SDK\nThe Amazon Nova Act SDK is reliable, fast to deploy, and built for secure, large-scale browser automation use cases. In this section, we discuss some of the benefits of the SDK in more detail.\nReliability: Build robust browser automation with high accuracy and repeatability\nWith the Amazon Nova Act SDK, developers can break down complex workflows into reliable atomic commands (for example, collect all form elements of a webpage and return a string with all required fields of the form). It supports the addition of detailed instructions to refine those commands when needed (for example, dismiss any popup banners), the ability to call APIs, and the option to alternate direct browser manipulation through Playwright to improve reliability (for example, for entering passwords). Developers can interleave Python code—such as tests, breakpoints, assertions, or thread pools for parallelization—to optimize performance, especially because even the fastest agents are constrained by webpage load times. With this latest version, the Amazon Nova Act SDK is already demonstrating over 90% reliability across early enterprise workflows, including automated quality assurance, complex form handling, and process execution. Improvements to reasoning and recovery help agents adapt to changing UIs and complete complex sequences consistently and accurately.\nSpeed-to-market: Move from prototype to production in days—not weeks\nThe Amazon Nova Act SDK is designed to help you build automation quickly, without relying on brittle scripts. You can install the SDK with a single command. You can define agents using Python, natural language, or both. You can debug flows while they run, inspect the DOM, pause between steps, and iterate rapidly. The Amazon Nova Act SDK supports the following features:\n\nGit-based CI/CD pipelines\nReusable agent modules\nPlaywright fallbacks for sensitive tasks (for example, password input)\nThreading and parallelization for fast execution\n\nYou don’t have to change your infrastructure or rebuild your internal tools. Agents built with Amazon Nova Act fit into existing dev workflows and allow you to move from experimentation to production quickly.\nSecurity: Deploy automations you can trust—powered by AWS\nThe Amazon Nova Act SDK integrates with IAM for access control, and access to the model is managed just like access from other AWS services. It supports execution on macOS, Linux, Windows, and WSL2. Runtime environments are isolated, and encryption is supported for sensitive inputs and outputs. The Amazon Nova Act SDK was designed to work inside enterprise environments—with the reliability, observability, and security that production systems require.\nSee it in action: Automating information gathering to help streamline financial decisions\nIn financial services—especially investment banking, M&A advisory, and strategic research—success often depends on how fast and accurately teams can turn fragmented public data into actionable insight. The following demo shows the Amazon Nova Act SDK in action.\n\nWhere the Amazon Nova Act SDK can make an impact\nBrowser-based workflows are common in today’s businesses, yet many remain manual, repetitive, and prone to error. The Amazon Nova Act SDK helps organizations automate these tasks, freeing up teams to focus on higher-value work, improve accuracy, and reduce operational delays. Its reliability makes it a fit across industries and use cases. In this section, we provide some examples of what our early customers are building.\nAutomated data entry and form filling\nThe Amazon Nova Act SDK reduces repetitive manual input across web-based systems—like CRMs, HR tools, and finance platforms—by automating form submissions, uploads, and updates. In healthcare, staff assist members with complex, state-specific benefit applications. Public sector caseworkers also re-enter household data across multiple systems. The Amazon Nova Act SDK handles these dynamic flows reliably—navigating shifting fields, dropdowns, and popups without brittle scripts or custom code.\n \nRackspace Technology, a leading hybrid and AI solutions provider, is working with Alvee Health to automatically register members for public benefits using the Amazon Nova Act SDK. “Many registration forms for public programs are long and confusing, so members often don’t apply for the help they need,” said Nicole Cook, CEO at Alvee. “With the Amazon Nova Act SDK and harnessing information already in Alvee’s system, we’re not just simplifying paperwork—we’re helping provide timely, accurate access to the resources that support healthier lives. We expect this innovation to increase successful benefit registrations by 30%, and improve overall case load by up to tenfold, allowing healthcare providers to focus more on patient care and less on administration. This is a prime example of how AI can be used to support well-being and improve overall health for communities.”\nCustomer support augmentation\nCustomer support teams across retail, travel, and software as a service (SaaS) often move between internal tools and third-party portals to resolve tickets. For example, a retail associate might submit a return on a partner site. A travel agent might log in to an airline dashboard to request compensation. A support rep might reset a license key in a customer admin console. The Amazon Nova Act SDK automates these browser-based tasks, helping agents stay focused on customer conversations while backend steps are executed reliably and at speed.\n\nNavan, a leading travel and expense management platform, uses the Amazon Nova Act SDK to simplify its travel agents’ workflows by automating how they fill out payment forms across a wide range of vendors.\nYuval Refua, SVP of Product, said, “Adding the Amazon Nova Act SDK to our agents’ workflows has helped us reduce repetitive tasks—an essential step in scaling our operations to serve more customers. We tried other computer use tools, and Amazon Nova Act’s reliability and flexibility enabled a single script work across diverse payment forms from a range of hotel brands. We’re now expanding this automation to cover even more vendors, which we expect will increase our operational capacity and help us meet growing customer demand more efficiently.”\nAutomating high-stakes administrative workflows\nCredential verification, identity checks, and other compliance-heavy tasks often involve navigating hundreds of third-party portals with inconsistent layouts. The Amazon Nova Act SDK makes it possible to automate these workflows with high accuracy, flexibility, and full control—helping teams scale while maintaining precision.\n\nAutomation Anywhere, a global leader in Agentic Process Automation (APA), is expanding its automation capabilities through the Amazon Nova Act SDK, starting with professional credential verification—a high-stakes, repetitive task that’s essential for compliance, member safety, and day-to-day operations.\n“By deeply integrating the Amazon Nova Act SDK into our Process Reasoning Engine (PRE), we’ve unlocked a major leap forward in computer use for enterprise automation,” said Adi Kuruganti, Chief Product Officer at Automation Anywhere. “Our goal-oriented AI agents don’t just mimic clicks, they reason through UI-based processes in real time, navigating complex websites with human-like expertise. This opens the door to automating previously out-of-reach use cases like healthcare program enrollment testing, where accuracy and scale are essential.”\nUX and QA testing across dynamic interfaces\nUX and QA testing often involves simulating real user interactions on frequently changing websites—especially in sectors like banking, insurance, and government. With the Amazon Nova Act SDK, teams can write and update tests using natural language or Python, adapting quickly to UI changes without brittle selectors or manual rewrites.\n\nTyler Technologies, a leading provider of integrated software and technology services to the public sector, is using the Amazon Nova Act SDK to automate software testing and improve the reliability of its releases. “Amazon Nova Act’s natural-language interface lets us convert our manual test plans into automated suites in minutes—without writing a single line of code, saving us hundreds of hours while expanding test coverage and increasing product quality.” said Franklin Williams, President of Data & Insights at Tyler Technologies. “We’re now looking to expand the use of Nova Act SDK across our portfolio.”\nWhat’s next for the Amazon Nova Act SDK\nWe’re working closely with early AWS customers to inform our roadmap. Although today’s focus is on browser-based workflows, the Amazon Nova Act SDK is part of a broader effort to build agents that can operate reliably across diverse environments. We’re continuing to expand the model’s reach beyond the web, applying reinforcement learning to more complex, real-world tasks. We’re also deepening integration across the AWS ecosystem to help developers move faster—from prototyping to deploying secure, scalable agentic systems.\nGet started with the Amazon Nova Act SDK\nIf you’re a technical leader or developer and want to start prototyping with the research preview of the Amazon Nova Act SDK, visit Amazon Nova Act. You will get access to early tooling designed for reliable, step-by-step browser automation—built for real-world workflows, not just demos.\nThe Amazon AGI Lab is Amazon’s applied research group focused on building useful AI agents that can take real-world actions in digital and physical environments. Their work spans LLMs, reinforcement learning, world modeling, and more. To learn more and keep up with their latest innovations, visit Amazon AGI Labs.\nContact us to express interest in working with us to productionize your agent (limited preview).\n\nAbout the authors\nLori Knapp is a Principal Product Manager with Amazon Nova. She leads product efforts to define how foundation models can power intelligent agents across diverse real-world use cases. Prior to this role, Lori’s experience spanned scaling adaptive voice experiences at Alexa, product strategy at Microsoft, and behavioral science consulting. Outside of work, she enjoys exploring new cities, hosting dinner parties, and solving crossword puzzles.\nTara Raj is an Engineering Manager at Amazon working on Nova Act. In her current role she’s focused on developer experience, from building nova.amazon.com/act to the Nova Act SDK with the software engineers on her team to driving adoption of Amazon Nova Act with her solutions architect team. Tara has over 10 years of experience in engineering roles bringing products from vision to launch including Nova multimodal capabilities at Amazon and the Windows Subsystem for Linux and Visual Studio Code at Microsoft. Outside of work you can find her traveling, dancing, and trying new restaurants.",
      "date": "2025-07-28",
      "authors": "Lori Knapp",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "The article introduces the Amazon Nova Act SDK, a new AI-powered tool designed to build and deploy reliable browser automation agents using large language models and reinforcement learning. It highlights how this agentic AI advances browser-based task automation at scale with high reliability, security, and integration into AWS infrastructure, enabling enterprises to automate complex workflows efficiently.",
      "takeaways": [
        "- The Amazon Nova Act SDK leverages generative AI and reinforcement learning to create scalable, reliable browser automation agents capable of handling dynamic, multi-step workflows.",
        "- It integrates with AWS services for secure production deployment, supporting Python and natural language programming, real-time debugging, and CI/CD pipelines.",
        "- The SDK demonstrates impactful use cases across industries including healthcare, finance, customer support, compliance, and software testing by automating repetitive, error-prone browser tasks."
      ]
    },
    {
      "id": 11,
      "title": "Optimizing enterprise AI assistants: How Crypto.com uses LLM reasoning and feedback for enhanced efficiency",
      "link": "https://aws.amazon.com/blogs/machine-learning/optimizing-enterprise-ai-assistants-how-crypto-com-uses-llm-reasoning-and-feedback-for-enhanced-efficiency/",
      "description": "This post is co-written with Jessie Jiao from Crypto.com. Crypto.com is a crypto exchange and comprehensive trading service serving 140 million users in 90 countries. To improve the service quality of Crypto.com, the firm implemented generative AI-powered assistant services on AWS.\nModern AI assistants—artificial intelligence systems designed to interact with users through natural language, answer questions, and even perform tasks—face increasingly complex challenges in production environments. Beyond handling basic FAQs, they must now execute meaningful actions, adhere to company policies, implement content filtering, escalate to human operators when needed, and manage follow-up tasks. These requirements demand sophisticated systems capable of handling diverse scenarios while maintaining consistency and compliance.\nTo address these challenges, a modular subsystem architecture proves invaluable. This architectural approach divides an AI system into separate, specialized components that can function independently while working together as a cohesive whole. Such design allows for flexible integration of different processing logics, such as intelligent routing between knowledge bases, dynamic prioritization of information sources, and seamless incorporation of business rules and policies. Each subsystem can be independently developed and optimized for specific tasks while maintaining overall system coherence.\nAs AI assistant systems grow in complexity, with multiple subsystems handling various workloads, prompt engineering emerges as a critical discipline. This art of carefully crafting input text guides language model responses and facilitates consistent behavior across interconnected components. Crafting effective prompts that work across different subsystems while maintaining consistency and accuracy is both critical and time-intensive. This challenge is particularly acute in enterprise environments where precision and reliability are paramount.\nIn this post, we explore how we used user and system feedback to continuously improve and optimize our instruction prompts. This feedback-driven approach has enabled us to create more effective prompts that adapt to various subsystems while maintaining high performance across different use cases.\nFeedback and reasoning: The key to LLM performance improvement\nAlthough large language models (LLMs) have demonstrated remarkable capabilities, they can sometimes struggle with complex or ambiguous inputs. This is where feedback mechanisms become essential. By incorporating feedback loops, LLMs can learn from their mistakes, refine the instruction, and adapt to challenging scenarios.\nOne powerful approach is critiquing, where LLMs are paired with an external feedback mechanism that provide critiques or feedback. For instance, when processing documents, if an LLM generates an incorrect summary, a fact-checking tool can identify inaccuracies and provide feedback. The model can then revise its output, leading to improved accuracy and reliability. This iterative process mirrors human learning, where feedback drives continuous improvement. Consider an example where a customer asks an enterprise AI assistant, “I need to increase my credit limit immediately for an emergency purchase.” The assistant might initially respond with approval steps without verification, but a critique system would flag: “Response bypasses required identity verification protocol and fails to assess qualification criteria per company policy.” With this feedback, the assistant can revise its response to include proper authentication steps, eligibility checking, and alternative options for emergency situations—demonstrating how critiquing facilitates adherence to business rules while maintaining helpful customer service.\nUnlike traditional machine learning (ML) processes where feedback serves as a loss function to update model weights, these feedback mechanisms operate differently in inference-time LLM applications. Rather than modifying the underlying model parameters, feedback provides supplementary instructions that dynamically guide the model’s behavior. This approach allows for behavioral adaptation without the computational expense of retraining, effectively creating a flexible instruction layer that shapes model outputs while preserving the core capabilities of the pre-trained model. Such runtime adaptability represents a significant advancement in making LLMs more responsive to specific requirements without architectural modifications.\nThe effectiveness of feedback mechanisms extends beyond simple error correction, enabling LLMs to develop a nuanced understanding of task requirements. Through iterative feedback cycles, models can learn to interpret ambiguous instructions more effectively, identify implicit context, and adapt their processing strategies accordingly. This capability is particularly valuable in enterprise settings where complex, domain-specific tasks require precise interpretation of instructions. By analyzing feedback patterns over time, LLMs can even anticipate potential misunderstandings and proactively adjust their approach, leading to more efficient and accurate outcomes. In our research implementing this approach for financial services classification tasks, we observed substantial performance improvements—from initial accuracy rates of 60% to eventually achieving 100% through systematic feedback incorporation. Each iteration addressed specific weaknesses identified in previous rounds, demonstrating how structured critique leads to continuous model improvement.\nFor deeper insights into these mechanisms, we recommend exploring two key research papers: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, which demonstrates how LLMs can self-correct with tool-interactive critiquing, and Reflexion: Language Agents with Verbal Reinforcement Learning, which explores language agents with verbal reinforcement learning. The following figure provides a visual representation of this feedback process.\n\nRecent developments in reasoning capabilities have made this feedback process even more powerful. Modern LLMs can now engage in sophisticated analysis of their own outputs, breaking down complex problems into manageable components and systematically evaluating each aspect of their performance. To learn more, see Anthropic’s Claude 3.7 Sonnet hybrid reasoning model is now available in Amazon Bedrock and DeepSeek-R1 now available as a fully managed serverless model in Amazon Bedrock. This self-analysis capability, combined with external feedback, creates a robust framework for continuous improvement.\nConsider a scenario where an LLM is tasked with sentiment analysis. Initially, when classifying a mixed review like “The product worked as advertised, but customer service was disappointing,” the model might incorrectly label it as positive. Through error analysis and verification, a critique mechanism (powered by a separate reasoning model) can provide targeted feedback, explaining that negative statements about service quality significantly impact overall sentiment. This feedback doesn’t modify the model’s weights but instead serves as supplementary instruction that enriches the original prompt template, helping the model properly weigh contrasting sentiments within the same text.\nOver multiple feedback iterations, the LLM employs reasoning capabilities to incorporate this external feedback and develop more sophisticated classification heuristics. With the critique system continuously verifying outputs and providing constructive guidance, the model learns to identify why certain patterns lead to misclassifications and refines its approach accordingly. When encountering new ambiguous reviews, it can now apply these learned insights to correctly interpret subtle emotional nuances. This demonstrates how reasoning-based feedback effectively modifies the instruction context without requiring parameter adjustments, allowing for continuous improvement through analytical understanding rather than mechanical optimization.\nIn the next section, we explore how these feedback mechanisms and reasoning capability can be operationalized to enhance workflows.\nSolution overview\nThe integration of feedback and reasoning creates a powerful learning loop: feedback identifies areas for improvement, reasoning capabilities analyze the root causes of issues, and the resulting insights drive specific, actionable changes. This systematic approach to improvement makes sure that each iteration brings the model closer to optimal performance, while maintaining transparency and accountability in the development process.\n\nFor practical examples and complete implementation code of this process, check out our GitHub repository. This repository includes sample datasets, evaluation frameworks, and ready-to-use templates for each step of the optimization workflow.\nOur proposed solution uses two foundation models (FMs) through Amazon Bedrock: Amazon Nova for executing instructional tasks and optimizing the instruction prompt, and Anthropic’s Claude 3.7 or DeepSeek-R1 for error analysis and feedback generation. Amazon Bedrock, a fully managed service, provides access to high-performance FMs from leading AI companies, enabling flexible model selection and testing. You can explore illustration_notebook_optimization_prompt.ipynb for a quick walkthrough of the high-level process for LLM optimization, which demonstrates key concepts and implementation details in an accessible format.\nLLM optimization workflow\nThe following is the high-level process for LLM optimization:\n\nThe process begins with a precise articulation of task requirements and success criteria. This crucial first step involves three key components: defining specific task objectives, crafting a well-structured prompt template with clear instructions, and assembling a comprehensive evaluation dataset with verified ground truth labels. During this phase, we establish quantifiable success metrics and acceptance criteria to measure improvement effectively. The Amazon Nova Pro understanding model is configured to provide both task outputs and detailed explanations for its decisions, enabling transparency in the evaluation process.\n\nFor illustration, we started with a simple prompt template to categorize customer inquiries into multiple classes, such as PASSWORD_RESET, ESCALATION, and OUT_OF_SCOPE. This initial template provided only basic category definitions without detailed guidance on edge cases or classification priorities, serving as our baseline for improvement. You can refer to the test case dataset and initial template.\n\nFollowing the setup, we conduct rigorous testing against ground truth data to evaluate model performance. This evaluation focuses on both successful and failed cases, with particular emphasis on analyzing misclassifications. The model’s generated explanations for each decision serve as valuable insights into its reasoning process. We collect both quantitative performance metrics (accuracy, precision, recall) and qualitative insights into error patterns, creating a comprehensive performance baseline.\n\nDuring this step, we compare model predictions to ground truth labels and record both quantitative metrics and detailed error cases. For example, when a customer urgently reports unauthorized account changes with “Someone must have accessed my account…I need this fixed immediately”, the model might incorrectly classify it as CARD_DISPUTE instead of the correct ESCALATION category. Each prediction is logged with its success status (true/false), the model’s explanation, and the correct label. This comprehensive analysis creates a structured dataset of both successful classifications and failure cases, providing critical input for the reasoning-based optimization in the next step.\n\nThe key step of our optimization process lies in systematic error analysis using a dedicated reasoning framework. This framework examines the model’s explanations for each error case, identifying root causes and pattern recognition failures. Beyond individual error analysis, we employ pattern recognition to identify systemic issues across multiple cases. The reasoning model, in our case Anthropic’s Claude 3.7, incorporates historical feedback and learning patterns to generate specific, actionable feedback for prompt improvement. This critical step produces structured, detailed recommendations for prompt optimization.\n\nThe reasoning model analyzed classification performance through a structured framework that identified error patterns, investigated prompt-specific root causes, considered historical context from previous iterations, and suggested targeted improvements. This methodical approach focused exclusively on enhancing prompt clarity, structure, and precision—avoiding model or data modifications outside the scope of prompt engineering. By systematically addressing ambiguities and refining classification criteria, we achieved progressively better performance with each iteration. See the following code:\n\ncritique_prompt_template = \"\"\"\n    Analyze classification performance and provide reasoning for prompt improvements:\n    Current Template: ${input_current_template}\n    Evaluation Results: ${evaluation_results}\n    \n    Follow these thinking steps:\n    1. Error Pattern Analysis:\n    2. Root Cause Investigation:\n    3. Historical Context Review:\n       • Previous suggestions: ${suggestion_history}\n    4. Prompt Improvement Ideas:\n     \n    Output final suggestions between <suggestion> </suggestion> tags\n\"\"\"\n\nYou can see the detailed implementation in error_analysis_with_reasoning.py.\n\nUsing the structured feedback from the reasoning framework, we implement targeted modifications to the prompt template. These refinements might include enhancing instruction clarity, adjusting classification parameters, or restructuring the prompt format. Each modification directly addresses specific issues identified in the analysis phase, making sure changes are evidence-based and purposeful. The focus remains on improving the instruction layer rather than modifying the underlying model architecture.\n\nTo implement these structured improvements, we developed a systematic prompt rewriting mechanism encoded in our prompt_rewrite.py module. This component transforms analytical feedback into concrete prompt enhancements through a dedicated template-based approach. The rewriting process follows a methodical workflow: it preserves essential components like placeholders, incorporates specific improvements identified in the analysis, and makes sure modifications directly address root causes from the feedback. This systematic rewriting approach guarantees that each iteration builds upon previous learnings rather than making arbitrary changes.\n\nrewrite_prompt_template = \"\"\"\nTASK: Improve the prompt template based on critique feedback.\nINPUT:\n- Current Template: ${input_current_template}\n- Critique Analysis: ${critique_feedbacks}\nINSTRUCTIONS:\n1. Preserve the current template structure and all placeholders\n2. Implement specific improvements identified in the critique\n3. Focus on addressing root causes of errors\n4. Create a complete, ready-to-use improved template\nOUTPUT FORMAT:\n- Root cause summary\n- Improved template incorporating all recommended changes\nThe improved template should directly address identified issues while remaining concise and effective.\n\"\"\"\n\n\nThe optimization process concludes each iteration by testing the refined prompt against the evaluation dataset. We measure performance improvements through comparative analysis of key metrics and conduct quality assessments of new outputs. This phase initiates the next iteration cycle, where successful changes are incorporated into the baseline, and newly identified challenges inform the next round of optimization. This creates a sustainable improvement loop that progressively enhances prompt effectiveness while maintaining detailed documentation of successful strategies.\n\nThrough our iterative refinement process, we transformed a basic prompt into a highly effective instruction set for LLMs. Each iteration strategically addressed specific weaknesses identified through our structured analysis framework. For complete documentation of each iteration’s analysis and improvements, see iteration_log.\nWhat began as a simple prompt evolved into a comprehensive set of instructions incorporating nuanced task boundaries, explicit priority rules for edge cases, hierarchical decision criteria, and precise handling instructions for corner cases. Rather than modify model weights or architecture, our approach used targeted feedback from a critique mechanism to enhance the instruction layer, effectively guiding model behavior without retraining. Each iteration built upon lessons from previous rounds, systematically addressing error patterns revealed through our critique framework. The feedback served as supplementary instructions that enriched the original prompt template, allowing the model to develop increasingly sophisticated processing heuristics over time.\nResults\nThrough these iterative approaches, we benchmarked the solution on the production system. Our comparative analysis between the initial and final prompts revealed several important patterns:\n\nBoundary confusion was resolved by adding explicit prioritization rules between overlapping categories\nEdge case handling improved by incorporating specific examples that defined thresholds for categorization\nDecision transparency increased through structured reasoning requirements in the output format\nClassification consistency was enhanced by adding counterexamples to help prevent overcategorization in sensitive areas\n\nThrough 10 deliberate iterations and the incorporation of detailed task-specific instructions, we achieved a remarkable 34-percentage-point improvement in task effectiveness, transforming a basic prompt with 60% accuracy into a robust classification system with 94% accuracy on challenging cases. This validates not only our iterative optimization strategy but demonstrates how systematic prompt refinement can dramatically enhance LLM model performance without modifying the underlying model architecture.\nConclusion\nThe integration of feedback mechanisms into AI assistant systems represents a significant leap forward in conversational AI capabilities. By implementing robust feedback loops, we’ve demonstrated how AI assistants can evolve from static question-answering systems to dynamic, self-improving resources. The modular subsystem architecture, combined with continuous prompt optimization through feedback, enables AI assistants to handle increasingly complex tasks while maintaining compliance and accuracy.\nAs we’ve shown through practical examples and research insights, feedback-driven systems not only produce better outputs but also allow for more effective and streamlined input instructions over time. This efficiency gain is particularly valuable in enterprise environments where precision and adaptability are crucial, and where model retraining is costly or impractical. Each iteration builds upon lessons from previous rounds, systematically addressing error patterns revealed through our critique framework.\nLooking ahead, the continued refinement of feedback mechanisms and prompt engineering techniques will be essential for developing next-generation AI assistant systems. By embracing these approaches, organizations can create AI assistants that not only meet current demands but also adapt to future challenges, delivering increasingly sophisticated and reliable interactions. We invite you to try our proposed feedback-driven prompt optimization approach in your own applications. For those interested in implementing these techniques, Amazon Bedrock provides an ideal landscape for exploring these methods in your specific business contexts, offering a selection of FMs with flexible deployment options.\n\nAbout the authors\nJessie Jiao is a Senior Software Engineer at crypto.com, where she leverages her extensive experience in designing, building, and implementing enterprise applications with LLM models and AI technologies. She is passionate about harnessing the power of AI to drive business transformation and enhance operational efficiency.\nGary Lo is a Solutions Architect at AWS based in Hong Kong. He is a highly passionate IT professional with over 10 years of experience in designing and implementing critical and complex solutions for distributed systems, web applications, and mobile platforms for startups and enterprise companies. Outside of the office, he enjoys cooking and sharing the latest technology trends and insights on his social media platforms with thousands of followers.\nYanwei Cui, PhD, is a Senior Machine Learning Specialist Solutions Architect at AWS. He started machine learning research at IRISA (Research Institute of Computer Science and Random Systems), and has several years of experience building AI-powered industrial applications in computer vision, natural language processing, and online user behavior prediction. At AWS, he shares his domain expertise and helps customers unlock business potentials and drive actionable outcomes with machine learning at scale. Outside of work, he enjoys reading and traveling.\nMichelle Hong, PhD, works as Prototyping Solutions Architect at Amazon Web Services, where she helps customers build innovative applications using a variety of AWS components. She demonstrated her expertise in machine learning, particularly in natural language processing, to develop data-driven solutions that optimize business processes and improve customer experiences.",
      "date": "2025-07-28",
      "authors": "Jessie Jiao",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article discusses how Crypto.com utilizes large language models (LLMs) with reasoning and iterative feedback mechanisms to optimize enterprise AI assistants without retraining models. It highlights a modular architecture and a feedback-driven prompt engineering approach that significantly improves AI assistant accuracy and adaptability in complex tasks.",
      "takeaways": [
        "- Feedback mechanisms at inference time enable dynamic, instruction-level improvements to LLM behavior without modifying model weights.",
        "- Iterative prompt optimization guided by error analysis and reasoning models can substantially enhance classification accuracy, improving from 60% to 94%.",
        "- The solution uses foundation models via Amazon Bedrock to combine instructional execution with critique-driven error analysis, demonstrating a robust approach to developing self-improving AI assistant systems in enterprise settings."
      ]
    },
    {
      "id": 12,
      "title": "Build modern serverless solutions following best practices using Amazon Q Developer CLI and MCP",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-modern-serverless-solutions-following-best-practices-using-amazon-q-developer-cli-and-mcp/",
      "description": "Building modern serverless applications on AWS requires navigating best practices to manage the integration between multiple services, such as AWS Lambda, Amazon API Gateway, Amazon DynamoDB, and Amazon EventBridge. Security considerations, performance optimization, and implementing a comprehensive monitoring systems adds further requirements to build a serverless architecture while adhering to AWS best practices.\nAmazon Q Developer CLI with Model Context Protocol (MCP) supports getting started with serverless applications using natural language conversations that produce infrastructure configurations and architectural guidance aligned with AWS best practices. This reduces the need to manually research documentation or piece together solutions from disparate sources.\nThis post explores how the AWS Serverless MCP server accelerates development throughout the serverless lifecycle, from making architectural decisions with tools like get_iac_guidance and get_lambda_guidance, to streamlining development with get_serverless_templates, sam_init, to deployment with SAM integration, webapp_deployment_help, and configure_domain. We show how this conversational AI approach transforms the entire process, from architecture design through operations, dramatically accelerating AWS serverless projects while adhering to architectural principles.\nSolution overview\nAmazon Q Developer CLI serves as a powerful terminal-based gateway to the AI capabilities of Amazon Q, bringing conversational intelligence directly to your command line environment. Although you can also apply this approach to conversational AI in the interactive development environment’s (IDE’s) agentic chat, this innovative interface transforms how developers work by enabling natural language interactions with AWS services and development tools without leaving the terminal. By typing queries or requests in plain English, developers can harness the full potential of generative AI with Amazon Q to solve complex problems, generate code snippets, troubleshoot issues, and access AWS best practices—all from the familiar comfort of their preferred terminal application.\nHowever, although Amazon Q CLI provides the conversational interface, it needs a way to connect with external tools and data sources. This is where MCP comes in.\nMCP is a groundbreaking open standard created by Anthropic that bridges the gap between AI models and external data sources or tools. This protocol establishes a unified framework allowing large language models (LLMs) to seamlessly interact with diverse information systems and specialized capabilities. When applied to serverless development workflows, this capability transforms how developers build AWS applications.\nMCP transforms Amazon Q CLI for serverless development by bridging the gap between traditional approaches and modern AI-assisted workflows. This integration replaces manual documentation research and trial-and-error development with conversational access to AWS expertise, automated code generation, and intelligent configuration assistance—directly from the command line.\nNow that we have discussed the conceptual benefits of Amazon Q CLI and MCP, let’s explore how to set up these tools in your development environment.\nPrerequisites\nYou must have the following prerequisites:\n\nAn AWS account with credentials configured\nThe AWS Serverless Application Model Command Line Interface (AWS SAM CLI) installed\nThe AWS Command Line Interface (AWS CLI) installed\n\nPrepare your environment\nBefore you can start constructing this serverless solution, you must configure your IDE by installing and setting up Amazon Q CLI and the AWS Serverless MCP server. This section provides comprehensive guidance for properly installing and configuring the required components.\nInstall Amazon Q Developer CLI You can install Amazon Q Developer CLI independently on your system. To get started, complete the following steps:\n\nConfirm successful installation with this terminal command:\n\n\nq --version \n\nThe system should now show something similar to the following output:\n\nq 1.x.x\n\n\nLog in to your AWS account:\n\n\nq login\n\n\nSelect your preferred authentication option: \n  \nUse for free with Builder ID\nPro license\n \n\nSet up MCP servers\nBefore you can start using MCP servers with Amazon Q CLI, you must install several prerequisites and configure your environment. Complete the following steps:\nInstall uv with the following command:\n\npip install uv\n\nInstall Python 3.10 or newer using the following command:\n\nuv python install 3.10\n\nAdd the following code to your MCP client global configuration (~/.aws/amazonq/mcp.json file). This can be configured by project (.amazonq/mcp.json) in the current folder. The AWS Serverless MCP server uses the default AWS profile by default. Specify a value in AWS_PROFILE if you want to use a different profile. Similarly, adjust the AWS Region and log-level values as needed:\n\n{\n    \"mcpServers\": {\n        \"awslabs.aws-serverless-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"awslabs.aws-serverless-mcp-server\"\n            ],\n            \"env\": {\n                \"AWS_PROFILE\": \"your-aws-profile\",\n                \"AWS_REGION\": \"us-east-1\",\n                \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n            }\n        }\n    }\n}\n\nAmazon Q CLI seamlessly detects and integrates with MCP servers configured in the ~/.aws/amazonq/mcp.json file.\nThe MCP servers support several configuration options to customize your development experience:\n\nAuto-approve – Configure which tools can be used without prompting for approval\nEnvironment variables – Customize server behavior through environment variables\nDisabled flag – Temporarily disable specific servers when not needed\n\nThe following is an example of how to configure your AWS Serverless MCP server using the preceding options:\n\n{\n    \"mcpServers\": {\n        \"awslabs.aws-serverless-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"awslabs.aws-serverless-mcp-server\"\n            ],\n            \"env\": {\n                \"AWS_PROFILE\": \"your-aws-profile\",\n                \"AWS_REGION\": \"us-east-1\",\n                \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n            },\n            \"disabled\": false,\n            \"autoApprove\": [\n                \"search_documentation\",\n                \"read_documentation\",\n                \"recommend\"\n            ],\n            \"trusted\": [\n                \"search_documentation\",\n                \"read_documentation\",\n                \"recommend\"\n            ]\n        }\n    }\n}\nTest your setup\nTo verify that everything is working correctly, generate a simple serverless function. Start the Amazon Q CLI chat interface and verify the output shows MCP servers being loaded and initialized:\n\nq chat\n\n\nThe message in the preceding screenshot might not match what you see for your use case, especially when your MCP servers take longer to load.\nNow that you’ve confirmed your environment is properly configured, you can begin exploring the serverless development workflow. The first step in this journey is making informed architectural decisions that will shape your application.\nArchitecture and design phase\nThe architecture and design phase is where your serverless application takes shape. Making the right decisions at this stage can significantly impact your project’s success, scalability, and maintainability. The AWS Serverless MCP server provides powerful tools to guide these critical early decisions.\nWhen starting a new serverless project, choosing the right infrastructure as code (IaC) approach is crucial for long-term success. The AWS Serverless MCP server’s get_iac_guidance_tool transforms this decision-making process from hours of research into a simple conversation. This powerful tool provides comprehensive, contextual guidance tailored to your specific needs by analyzing factors such as:\n\nProject complexity and scale\nTeam expertise and preferences\nRequired deployment flexibility\nLocal testing requirements\nLong-term maintenance considerations\n\nLet’s see how this works in practice by asking Amazon Q about the best IaC approach for your serverless application:\nWhat's the best IaC approach for my serverless application?\nBecause Amazon Q is using get_iac_guidance for the first time, it will ask you to enter t to trust this tool for the entire session, y to allow the execution of this tool only one time, or n to reject the execution of this tool.\nThe following example is for an ecommerce application. \nAfter approving the tool usage, you will receive a comprehensive, contextual response tailored to your specific architecture needs. For example:\n\n\nAWS SAM for Your E-Commerce Microservices\n\nBased on your e-commerce microservices architecture, I recommend AWS SAM (Serverless Application Model) as your primary IaC tool for the serverless components of your application. \n\nHere is why it is particularly well-suited for your needs:\n\n1. Perfect for Your Serverless Components: SAM is specifically designed for the serverless parts of your architecture like the Payment Service (Lambda) and notification systems.\n\n2. Simplified Syntax: SAM provides a simplified way to define Lambda functions, API Gateway, and DynamoDB tables - all key components in your architecture.\n\n3. Local Testing: You can test your Lambda functions locally before deployment, which is crucial for your payment processing service.\n\n4. Integration with Your Architecture: SAM works seamlessly with the API Gateway and Lambda components shown in your diagram.\n\n5. Deployment Independence: Supports your microservices approach where \"each microservice can be deployed independently\" as mentioned in your documentation.\n\nExample SAM Template for Your Payment Service:\n\nAWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  PaymentFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: ./payment-service/\n      Handler: index.handler\n      Runtime: nodejs22.x\n      Policies:\n        - SQSPollerPolicy:\n            QueueName: !GetAtt PaymentQueue.QueueName\n      Events:\n        SQSEvent:\n          Type: SQS\n          Properties:\n            Queue: !GetAtt PaymentQueue.Arn\n            BatchSize: 10\n\n  PaymentQueue:\n    Type: AWS::SQS::Queue\n    Properties:\n      VisibilityTimeout: 300\n\nDevelopment acceleration phase\nAfter you’ve made key architectural decisions, you enter the development phase. This is often the most time-consuming part of the process, requiring significant effort to implement patterns correctly, handle edge cases, and follow best practices. The AWS Serverless MCP server provides tools that dramatically accelerate this phase by using proven patterns and templates.\nThe get_serverless_template tool alleviates the need to start from scratch by providing access to production-ready serverless patterns from the AWS Serverless Land GitHub repository. These patterns are tested, follow AWS best practices, and can be customized to meet your specific requirements.\nLet’s say you want to understand more about serverless architecture. With this tool, you can discover and implement common serverless architectures with a simple conversation. For example, you can ask Amazon Q the following question:\nShow me Node.js API templates for serverless applications\nBecause Amazon Q is using get_serverless_templates for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool. \nYou will have to trust at least this tool if not more. After trusting the tool, you should get a response similar to the following:\n\nHere are some Node.js API templates for serverless applications on AWS:\n\n1. API Gateway to SQS Integration\n\nThis template creates a REST API Gateway that directly integrates with AWS SQS to read messages:\n\nKey Features:\n- Direct service integration between API Gateway and SQS\n- No Lambda function required\n- Support for query string parameters\n- Ideal for retrieving messages via HTTPS without AWS SDK\n\nUse Cases:\n- Web applications polling SQS queues\n- Mobile apps requiring secure access to SQS\n- Third-party integrations\n\nGitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/APIGateway-SQS-ReceiveMessages\n\n2. Application Load Balancer with Lambda REST API\n\nWhile this example uses Python, the pattern is applicable to Node.js as well:\n\nKey Features:\n- Path-based listener rules on ALB\n- Lambda function as the target\n- RESTful API implementation\n\nGitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/alb-lambda-rest-api-sam-py\n\n3. Full-Stack Integration: Amplify + Cognito + API Gateway + Lambda\n\nThis template demonstrates a complete serverless stack with authentication:\n\nKey Features:\n- AWS Amplify frontend\n- Cognito for authentication\n- API Gateway with Cognito authorizer\n- Lambda backend\n- Environment variables shared between frontend and backend\n\nGitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/amplify_cognito_apigateway_lambda_envvariables\n\nDevelopment and testing phase\nThe development and testing phase is where your serverless application takes shape through code. This phase requires tools that streamline the creation, building, and testing of your application components. The AWS Serverless MCP server provides a suite of tools that integrate seamlessly with the AWS SAM CLI, enabling a smooth development experience from project initialization to local testing.\nBeginning a new serverless project with the right structure and configuration can be challenging. The sam_init tool reduces this hurdle by creating well-structured projects that follow AWS best practices out of the box. This tool provides templates for various serverless application types, helping you start with a solid foundation.\nFor example, you can ask the following question:\nCreate a new SAM project for a Python REST API\nBecause Amazon Q is using sam_init for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool. \nThe sam_init tool is used to initialize a new serverless application using AWS SAM. It creates a complete project structure with the necessary files to develop, test, and deploy serverless applications on AWS.\nThe tool has the following key capabilities:\n\nIt creates a new project directory with a standardized structure for serverless applications\nIt generates an AWS SAM template file (template.yaml) that defines your infrastructure as code\nIt sets up the initial code for Lambda functions in your chosen runtime (Python, Node.js, and so on)\nIt configures the project with the appropriate dependency management system\n\nFor your ecommerce microservices architecture, this tool would be particularly useful for:\n\nCreating individual microservices (like your product catalog service or payment service)\nSetting up serverless API endpoints through API Gateway\nEstablishing the foundation for event-driven communication between services\n\nThe tool supports various application templates and runtimes, helping you quickly bootstrap different types of serverless applications, from simple APIs to more complex event-processing systems.\nDeployment and configuration phase\nAfter you have developed and tested your serverless application locally, the next step is deploying it to the AWS Cloud. This phase often involves complex, multi-step processes to make sure the components are properly deployed, configured, and integrated. The AWS Serverless MCP server provides specialized tools that simplify these processes, enabling smooth deployments with minimal manual intervention.\nStreamline full-stack deployments\nFor this use case, we’re building a full-stack serverless application with a React frontend, Node.js backend APIs, and DynamoDB database. Traditionally, this would require manually configuring multiple AWS services, managing deployment pipelines, and validating proper integration between components.\nThe webapp_deployment_help tool transforms this complex process into a single, streamlined operation that handles the different aspects of deployment while following AWS best practices.\nYou can ask Amazon Q the following question:\nDeploy my React application to AWS with serverless backend\nBecause Amazon Q is using webapp_deployment_help for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool. For this example, trust this tool for the session. \nThis tool supports three deployment types:\n\nBackend only – Deploys a backend application to Lambda with API Gateway\nFrontend only – Deploys a frontend application to Amazon Simple Storage Service (Amazon S3) and Amazon CloudFront\nFull stack – Deploys both backend and frontend components together\n\nIt uses AWS Lambda Web Adapter, so you can deploy standard web applications (like Express.js, Flask, and React) to Lambda without needing to modify your code to follow Lambda-specific conventions.\nFor your ecommerce microservices architecture, this tool is particularly useful because it aligns with several components in your diagram:\n\nIt can deploy your frontend to CloudFront\nIt can deploy your microservices as Lambda functions or containers\nIt integrates with API Gateway for API management\n\nThe tool requires you to build your application first using your framework’s build command (like npm run build for React), then it handles the deployment to AWS, including setting up the necessary infrastructure.\nSet up custom domains\nAlthough your application is now deployed and accessible using AWS generated URLs, professional applications typically require branded, custom domain names for a better user experience. However, configuring custom domains traditionally involves multiple complex steps across several AWS services. The configure_domain tool simplifies this process into a single, intuitive conversation.\nYou can ask Amazon Q the following question:\nConfigure my custom domain example.com for my serverless application\nBecause Amazon Q is using configure_domain for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool. \nGet the steps to configure your domain as follows:\n\nSteps to Configure a Custom Domain for Your Serverless Application\n\n1. Prerequisites \n- Ensure you own the domain name (example.com) \n- Have a Route 53 hosted zone for your domain \n- Have deployed your serverless application (e.g., your e-commerce-app)\n\n2. Create an SSL Certificate with AWS Certificate Manager (ACM)\n- bash\n# Request a certificate for your domain. Region must be us-east-1 for CloudFront distributions\n- aws acm request-certificate --domain-name example.com --validation-method DNS --region us-east-1\n# Note the certificate ARN from the output\n\n3. Validate the Certificate \n- Go to the ACM console \n- Find your certificate and click on it \n- Create the DNS validation records in Route 53 \n- Wait for validation to complete (can take up to 30 minutes)\n\nThe tool orchestrates a comprehensive domain setup process that includes the following steps:\n\nCreating and validating AWS Certificate Manager (ACM) certificates for secure HTTPS connections\nConfiguring Amazon Route 53 DNS records to point to your application\nSetting up API Gateway custom domain mappings\nAssociating CloudFront distributions with your domain\nEstablishing proper cache behaviors and security settings\n\nThis automation alleviates the need to navigate between multiple AWS console pages and services, reducing the risk of configuration errors while making sure your domain setup follows security best practices. The tool also handles the complexities of certificate validation and DNS propagation, providing clear guidance throughout the process.\nWith configure_domain, you can transform your application’s user experience from generic AWS endpoints to professional, branded domains in minutes rather than hours.\nHaving walked through the complete serverless development lifecycle—from architecture decisions to deployment and configuration—we can now appreciate the transformative impact of Amazon Q CLI with MCP on the entire process.\nNext steps\nTo continue your serverless journey with Amazon Q CLI and MCP, we recommend a progressive approach to building your expertise:\n\nBegin with a simple project like a basic Lambda function triggered by an API Gateway endpoint or Amazon S3 event to familiarize yourself with the conversational development workflow and gain confidence in the generated solutions.\nExplore different serverless patterns by experimenting with event-driven architectures using EventBridge for decoupled communication, orchestration workflows with AWS Step Functions, or building comprehensive API-backed applications with API Gateway and Lambda.\\\nAs your applications mature, integrate the generated code into your continuous integration and deployment pipelines using services like AWS CodePipeline or AWS CodeBuild to automate testing and deployment processes.\nEnhance your applications with robust observability by implementing monitoring, logging, and tracing using Amazon CloudWatch, AWS X-Ray, and Powertools for AWS Lambda (Python) to gain insights into performance and troubleshoot issues effectively.\nShare your experiences and suggestions with the community to contribute to the improvement of MCP servers and help shape the future of AI-assisted serverless development.\n\nConclusion\nBy using Amazon Q CLI with MCP servers, you can accelerate your serverless application development while adhering to AWS best practices. This approach helps you focus on your business logic while Amazon Q handles the implementation details, resulting in more robust, scalable, and secure applications.\nThe ability to generate code, configurations, and architecture recommendations in minutes rather than hours creates a comprehensive development experience that makes sure the different components adhere to best practices while providing actionable, specific implementation guidance. The iterative nature of this approach helps teams quickly compare different architectural decisions through simple prompts, while continuously validating their implementations against official AWS documentation.\nHaving explored the capabilities and benefits of this approach, you’re now ready to apply these concepts in your own projects.\nAs you continue exploring serverless application development with Amazon Q CLI and MCP, we encourage you to deepen your knowledge with the following resources:\n\nThe MCP documentation provides valuable insights into how MCP enhances the capabilities of Amazon Q beyond basic interactions.\nThe AWS SAM documentation offers comprehensive guidance for serverless application development patterns and deployment strategies.\nTo learn more about Amazon Q CLI, refer to Using Amazon Q Developer on the command line.\nWhen focusing on architectural excellence, the AWS Well-Architected Framework provides foundational principles for implementing security, reliability, performance efficiency, cost optimization, and operational excellence.\nFor serverless-specific architectural guidance that addresses the unique characteristics of this computing model, refer to the specialized Serverless Lens for Well-Architected Framework tailored specifically for serverless applications.\nTo enhance your development workflow, Powertools for AWS Lambda (Python) offers ready-to-use utilities for structured logging, metrics collection, and distributed tracing. Additionally, the AWS MCP servers documentation offers domain-specific assistance tailored to your project requirements.\nThe serverless community provides valuable resources through AWS Serverless Land, a curated collection of serverless resources, patterns, and examples, and Serverless Stack, an open source framework for building production-ready serverless applications with integrated best practices.\n\nStart building your next serverless application with Amazon Q CLI and MCP today.\n\nAbout the authors\nDunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone.\nVarun Jasti is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.\nJoel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time.\n\n",
      "date": "2025-07-28",
      "authors": "Dunieski Otano",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article presents Amazon Q Developer CLI integrated with Model Context Protocol (MCP), an AI-driven conversational interface that accelerates and simplifies serverless application development on AWS. It highlights how generative AI and large language models facilitate architecture design, code generation, deployment, and configuration following AWS best practices, improving productivity and reducing manual effort.",
      "takeaways": [
        "- Amazon Q CLI uses conversational AI powered by large language models to enable natural language interactions for serverless development tasks directly from the terminal.",
        "- MCP, an open standard from Anthropic, bridges AI models with external tools and data, enhancing AI-assisted workflows for AWS serverless projects.",
        "- The integration supports comprehensive phases including architectural guidance, development acceleration, deployment, and configuration, showcasing how AI techniques can transform cloud-native application development."
      ]
    },
    {
      "id": 13,
      "title": "Build an intelligent eDiscovery solution using Amazon Bedrock Agents",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-ediscovery-solution-using-amazon-bedrock-agents/",
      "description": "Legal teams spend bulk of their time manually reviewing documents during eDiscovery. This process involves analyzing electronically stored information across emails, contracts, financial records, and collaboration systems for legal proceedings. This manual approach creates significant bottlenecks: attorneys must identify privileged communications, assess legal risks, extract contractual obligations, and maintain regulatory compliance across thousands of documents per case. The process is not only resource-intensive and time-consuming, but also prone to human error when dealing with large document volumes.\nAmazon Bedrock Agents with multi-agent collaboration directly addresses these challenges by helping organizations deploy specialized AI agents that process documents in parallel while maintaining context across complex legal workflows. Instead of sequential manual review, multiple agents work simultaneously—one extracts contract terms while another identifies privileged communications, all coordinated by a central orchestrator. This approach can reduce document review time by 60–70% while maintaining the accuracy and human oversight required for legal proceedings, though actual performance varies based on document complexity and foundation model (FM) selection.\nIn this post, we demonstrate how to build an intelligent eDiscovery solution using Amazon Bedrock Agents for real-time document analysis. We show how to deploy specialized agents for document classification, contract analysis, email review, and legal document processing, all working together through a multi-agent architecture. We walk through the implementation details, deployment steps, and best practices to create an extensible foundation that organizations can adapt to their specific eDiscovery requirements.\nSolution overview\nThis solution demonstrates an intelligent document analysis system using Amazon Bedrock Agents with multi-agent collaboration functionality. The system uses multiple specialized agents to analyze legal documents, classify content, assess risks, and provide structured insights. The following diagram illustrates the solution architecture.\n\nThe architecture diagram shows three main workflows for eDiscovery document analysis:\n\nReal-time document analysis workflow – Attorneys and clients (authenticated users) can upload documents and interact through mobile/web clients and chat. Documents are processed in real time for immediate analysis without persistent storage—uploaded documents are passed directly to the Amazon Bedrock Collaborator Agent endpoint.\nCase research document analysis workflow – This workflow is specifically for attorneys (authenticated users). It allows document review and analysis through mobile/web clients and chat. It’s focused on the legal research aspects of previously processed documents.\nDocument upload workflow – Law firm clients (authenticated users) can upload documents through mobile/web clients. Documents are transferred by using AWS Transfer Family web apps to an Amazon Simple Storage Service (Amazon S3) bucket for storage.\n\nAlthough this architecture supports all three workflows, this post focuses specifically on implementing the real-time document analysis workflow for two key reasons: it represents the core functionality that delivers immediate value to legal teams, and it provides the foundational patterns that can be extended to support the other workflows. The real-time processing capability demonstrates the multi-agent coordination that makes this solution transformative for eDiscovery operations.\nReal-time document analysis workflow\nThis workflow processes uploaded documents through coordinated AI agents, typically completing analysis within 1–2 minutes of upload. The system accelerates early case assessment by providing structured insights immediately, compared to traditional manual review that can take hours per document. The implementation coordinates five specialized agents that process different document aspects in parallel, listed in the following table.\n\n\n\nAgent Type\nPrimary Function\nProcessing Time*\nKey Outputs\n\n\n\n\nCollaborator Agent\nCentral orchestrator and workflow manager\n2–5 seconds\nDocument routing decisions, consolidated results\n\n\nDocument Classification Agent\nInitial document triage and sensitivity detection\n5–10 seconds\nDocument type, confidence scores, sensitivity flags\n\n\nEmail Analysis Agent\nCommunication pattern analysis\n10–20 seconds\nParticipant maps, conversation threads, timelines\n\n\nLegal Document Analysis Agent\nCourt filing and legal brief analysis\n15–30 seconds\nCase citations, legal arguments, procedural dates\n\n\nContract Analysis Agent\nContract terms and risk assessment\n20–40 seconds\nParty details, key terms, obligations, risk scores\n\n\n\n*Processing times are estimates based on testing with Anthropic’s Claude 3.5 Haiku on Amazon Bedrock and might vary depending on document complexity and size. Actual performance in your environment may differ.\nLet’s explore an example of processing a sample legal settlement agreement. The workflow consists of the following steps:\n\nThe Collaborator Agent identifies the document as requiring both contract and legal analysis.\nThe Contract Analysis Agent extracts parties, payment terms, and obligations (40 seconds).\nThe Legal Document Analysis Agent identifies case references and precedents (30 seconds).\nThe Document Classification Agent flags confidentiality levels (10 seconds).\nThe Collaborator Agent consolidates findings into a comprehensive report (15 seconds).\n\nTotal processing time is approximately 95 seconds for the sample document, compared to 2–4 hours of manual review for similar documents. In the following sections, we walk through deploying the complete eDiscovery solution, including Amazon Bedrock Agents, the Streamlit frontend, and necessary AWS resources.\nPrerequisites\nMake sure you have the following prerequisites:\n\nAn AWS account with appropriate permissions for Amazon Bedrock, AWS Identity and Access Management (IAM), and AWS CloudFormation.\nAmazon Bedrock model access for Anthropic’s Claude 3.5 Haiku v1 in your deployment AWS Region. You can use a different supported model of your choice for this solution. If you use a different model than the default (Anthropic’s Claude 3.5 Haiku v1), you must modify the CloudFormation template to reflect your chosen model’s specifications before deployment. At the time of writing, Anthropic’s Claude 3.5 Haiku is available in US East (N. Virginia), US East (Ohio), and US West (Oregon). For current model availability, see Model support by AWS Region.\nThe AWS Command Line Interface (AWS CLI) installed and configured with appropriate credentials.\nPython 3.8+ installed.\nTerminal or command prompt access.\n\nDeploy the AWS infrastructure\nYou can deploy the following CloudFormation template, which creates the five Amazon Bedrock agents, inference profile, and supporting IAM resources. (Costs will be incurred for the AWS resources used). Complete the following steps:\n\nLaunch the CloudFormation stack.\n\nYou will be redirected to the AWS CloudFormation console. In the stack parameters, the template URL will be prepopulated.\n\nFor EnvironmentName, enter a name for your deployment (default: LegalBlogSetup).\nReview and create the stack.\n\nAfter successful deployment, note the following values from the CloudFormation stack’s Outputs tab:\n\nCollabBedrockAgentId\nCollabBedrockAgentAliasId\n\n\nConfigure AWS credentials\nTest if AWS credentials are working:aws sts get-caller-identityIf you need to configure credentials, use the following command:\n\naws configure\n\nSet up the local environment\nComplete the following steps to set up your local environment:\n\nCreate a new directory for your project:\n\n\nmkdir bedrock-document-analyzer\ncd bedrock-document-analyzer\n\n\nSet up a Python virtual environment:\n\n\n#On macOS/Linux:\nsource venv/bin/activate \n#On Windows:\nvenv\\Scripts\\activate\n\n\nDownload the Streamlit application:\n\n\ncurl -O https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/ML-18253/eDiscovery-LegalBlog-UI.py\n\n\nInstall dependencies:\n\n\npip install streamlit boto3 PyPDF2 python-docx\n\nConfigure and run the application\nComplete the following steps:\n\nRun the downloaded Streamlit frontend UI file eDiscovery-LegalBlog-UI.py by executing the following command in your terminal or command prompt:\n\n\nstreamlit run eDiscovery-LegalBlog-UI.py\n\nThis command will start the Streamlit server and automatically open the application in your default web browser.\n\nUnder Agent configuration, provide the following values: \n  \nFor AWS_REGION, enter your Region.\nFor AGENT_ID, enter the Amazon Bedrock Collaborator Agent ID.\nFor AGENT_ALIAS_ID, enter the Amazon Bedrock Collaborator Agent Alias ID.\n \nChoose Save Configuration.\n\n\nNow you can upload documents (TXT, PDF, and DOCX) to analyze and interact with.\nTest the solution\nThe following is a demonstration of testing the application.\n\nImplementation considerations\nAlthough Amazon Bedrock Agents significantly streamlines eDiscovery workflows, organizations should consider several key factors when implementing AI-powered document analysis solutions. Consider the following legal industry requirements for compliance and governance:\n\nAttorney-client privilege protection – AI systems must maintain confidentiality boundaries and can’t expose privileged communications during processing\nCross-jurisdictional compliance – GDPR, CCPA, and industry-specific regulations vary by region and case type\nAudit trail requirements – Legal proceedings demand comprehensive processing documentation for all AI-assisted decisions\nProfessional responsibility – Lawyers remain accountable for AI outputs and must demonstrate competency in deployed tools\n\nYou might encounter technical implementation challenges, such as document processing complexity:\n\nVariable document quality – Scanned PDFs, handwritten annotations, and corrupted files require preprocessing strategies\nFormat diversity – Legal documents span emails, contracts, court filings, and multimedia content requiring different processing approaches\nScale management – Large cases involving over 100,000 documents require careful resource planning and concurrent processing optimization\n\nThe system integration also has specific requirements:\n\nLegacy system compatibility – Most law firms use established case management systems that need seamless integration\nAuthentication workflows – Multi-role access (attorneys, paralegals, clients) with different permission levels\nAI confidence thresholds – Determining when human review is required based on processing confidence scores\n\nAdditionally, consider your human/AI collaboration framework. The most successful eDiscovery implementations maintain human oversight at critical decision points. Although Amazon Bedrock Agents excels at automating routine tasks like document classification and metadata extraction, legal professionals remain essential for the following factors:\n\nComplex legal interpretations requiring contextual understanding\nPrivilege determinations that impact case strategy\nQuality control of AI-generated insights\nStrategic analysis of document relationships and case implications\n\nThis collaborative approach optimizes the eDiscovery process—AI handles time-consuming data processing while legal professionals focus on high-stakes decisions requiring human judgment and expertise. For your implementation strategy, consider a phased deployment approach. Organizations should implement staged rollouts to minimize risk while building confidence:\n\nPilot programs using lower-risk document categories (routine correspondence, standard contracts)\nControlled expansion with specialized agents and broader user base\nFull deployment enabling complete multi-agent collaboration organization-wide\n\nLastly, consider the following success planning best practices:\n\nEstablish clear governance frameworks for model updates and version control\nCreate standardized testing protocols for new agent deployments\nDevelop escalation procedures for edge cases requiring human intervention\nImplement parallel processing during validation periods to maintain accuracy\n\nBy addressing these considerations upfront, legal teams can facilitate smoother implementation and maximize the benefits of AI-powered document analysis while maintaining the accuracy and oversight required for legal proceedings.\nClean up\nIf you decide to discontinue using the solution, complete the following steps to remove it and its associated resources deployed using AWS CloudFormation:\n\nOn the AWS CloudFormation console, choose Stacks in the navigation pane.\nLocate the stack you created during the deployment process (you assigned a name to it).\nSelect the stack and choose Delete.\n\nResults\nAmazon Bedrock Agents transforms eDiscovery from time-intensive manual processes into efficient AI-powered operations, delivering measurable operational improvements across business services organizations. With a multi-agent architecture, organizations can process documents in 1–2 minutes compared to 2–4 hours of manual review for similar documents, achieving a 60–70% reduction in review time while maintaining accuracy and compliance requirements. A representative implementation from the financial services sector demonstrates this transformative potential: a major institution transformed their compliance review process from a 448-page manual workflow requiring over 10,000 hours to an automated system that reduced external audit times from 1,000 to 300–400 hours and internal audits from 800 to 320–400 hours. The institution now conducts 30–40 internal reviews annually with existing staff while achieving greater accuracy and consistency across assessments. These results demonstrate the potential across implementations: organizations implementing this solution can progress from initial efficiency gains in pilot phases to a 60–70% reduction in review time at full deployment. Beyond time savings, the solution delivers strategic advantages, including resource optimization that helps legal professionals focus on high-value analysis rather than routine document processing, improved compliance posture through systematic identification of privileged communications, and future-ready infrastructure that adapts to evolving legal technology requirements.\nConclusion\nThe combination of Amazon Bedrock multi-agent collaboration, real-time processing capabilities, and the extensible architecture provided in this post offers legal teams immediate operational benefits while positioning them for future AI advancements—creating the powerful synergy of AI efficiency and human expertise that defines modern legal practice.\nTo learn more about Amazon Bedrock, refer to the following resources:\n\nGitHub repo: Amazon Bedrock Workshop\nAmazon Bedrock User Guide\nWorkshop: GenAI for AWS Cloud Operations\nWorkshop: Using generative AI on AWS for diverse content types\n\n\nAbout the authors\nPuneeth Ranjan Komaragiri is a Principal Technical Account Manager at AWS. He is particularly passionate about monitoring and observability, cloud financial management, and generative AI domains. In his current role, Puneeth enjoys collaborating closely with customers, using his expertise to help them design and architect their cloud workloads for optimal scale and resilience.\nPramod Krishna is a Senior Solutions Architect at AWS. He works as a trusted advisor for customers, helping customers innovate and build well-architected applications in AWS Cloud. Outside of work, Krishna enjoys reading, music, and traveling.\nSean Gifts Is a Senior Technical Account Manager at AWS. He is excited about helping customers with application modernization, specifically event-driven architectures that use serverless frameworks. Sean enjoys helping customers improve their architecture with simple, scalable solutions. Outside of work, he enjoys exercising, enjoying new foods, and traveling.",
      "date": "2025-07-25",
      "authors": "Puneeth Komaragiri",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article details the development of an intelligent eDiscovery solution using Amazon Bedrock Agents, showcasing how a multi-agent AI architecture can significantly speed up legal document analysis through parallel processing while maintaining accuracy and compliance. It highlights advances in AI model integration, multi-agent collaboration, real-time processing, and practical deployment considerations in a legal industry context.",
      "takeaways": [
        "- The multi-agent AI system processes complex legal documents concurrently, reducing review time by 60–70% compared to manual methods.",
        "- Amazon Bedrock Agents enable deployment of specialized AI agents such as contract analysis, email analysis, and document classification, coordinated by a central orchestrator.",
        "- The solution balances AI automation with human oversight to address legal compliance, interpretive challenges, and quality control, demonstrating a practical AI advancement in document analysis workflows."
      ]
    },
    {
      "id": 14,
      "title": "How PerformLine uses prompt engineering on Amazon Bedrock to detect compliance violations",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-performline-uses-prompt-engineering-on-amazon-bedrock-to-detect-compliance-violations/",
      "description": "This post is co-written with Bogdan Arsenie and Nick Mattei from PerformLine.\nPerformLine operates within the marketing compliance industry, a specialized subset of the broader compliance software market, which includes various compliance solutions like anti-money laundering (AML), know your customer (KYC), and others. Specifically, marketing compliance refers to adhering to regulations and guidelines set by government agencies that make sure a company’s marketing, advertising, and sales content and communications are truthful, accurate, and not misleading for consumers. PerformLine is the leading service providing comprehensive compliance oversight across marketing, sales, and partner channels. As pioneers of the marketing compliance industry, PerformLine has conducted over 1.1 billion compliance observations over the past 10+ years, automating the entire compliance process—from pre-publication review of materials to continuous monitoring of consumer-facing channels such as websites, emails, and social media. Trusted by consumer finance brands and global organizations, PerformLine uses AI-driven solutions to protect brands and their consumers, transforming compliance efforts into a competitive advantage.\n“Discover. Monitor. Act. This isn’t just our tagline—it’s the foundation of our innovation at PerformLine,” says PerformLine’s CTO Bogdan Arsenie. PerformLine’s engineering team brings these principles to life by developing AI-powered technology solutions. In this post, PerformLine and AWS explore how PerformLine used Amazon Bedrock to accelerate compliance processes, generate actionable insights, and provide contextual data—delivering the speed and accuracy essential for large-scale oversight.\nThe problem\nOne of PerformLine’s enterprise customers needed a more efficient process for running compliance checks on newly launched product pages, particularly those that integrate multiple products within the same visual and textual framework. These complex pages often feature overlapping content that can apply to one product, several products, or even all of them at once, necessitating a context-aware interpretation that mirrors how a typical consumer would view and interact with the content. By adopting AWS and the architecture discussed in this post, PerformLine can retrieve and analyze these intricate pages through AI-driven processing, generating detailed insights and contextual data that capture the nuanced interplay between various product elements. After the relevant information is extracted and structured, it’s fed directly into their rules engine, enabling robust compliance checks. This accomplishes a seamless flow, from data ingestion to rules-based analysis. It not only preserves the depth of each product’s presentation but also delivers the speed and accuracy critical to large-scale oversight. Monitoring millions of webpages daily for compliance demands a system that can intelligently parse, extract, and analyze content at scale—much like the approach PerformLine has developed for their enterprise customers. In this dynamic landscape, the ever-evolving nature of web content challenges traditional static parsing, requiring a context-aware and adaptive solution. This architecture not only processes bulk data offline but also delivers near real-time performance for one-time requests, dynamically scaling to manage the diverse complexity of each page. By using AI-powered inference, PerformLine provides comprehensive coverage of every product and marketing element across the web, while striking a careful balance between accuracy, performance, and cost.\nSolution overview\nWith this flexible, adaptable solution, PerformLine can tackle even the most challenging webpages, providing comprehensive coverage when extracting and analyzing web content with multiple products. At the same time, by combining consistency with the adaptability of foundation models (FMs), PerformLine can maintain reliable performance across the diverse range of products and websites their customers monitor. This dual focus on agility and operational consistency makes sure their customers benefit from robust compliance checks and data integrity, without sacrificing the speed or scale needed to remain competitive.\nPerformLine’s upstream ingestion pipeline efficiently collects millions of web pages and their associated metadata in a batch process. Downstream assets are submitted to PerformLine’s rules engine and compliance review processes. It was imperative that they not disrupt those processes or introduce cascading changes for this solution.\nPerformLine decided to use generative AI and Amazon Bedrock to address their core challenges. Amazon Bedrock allows for a broad selection of models, including Amazon Nova. Amazon Bedrock is continuously expanding feature sets around using FMs at scale. This provides a reliable foundation to build a highly available and efficient content processing system.\nPerformLine’s solution incorporates the following key components:\n\nAI inference with Amazon Bedrock – Provides seamless access to FMs for content extraction and analysis\nApplication inference profiles – Enables precise tracking and optimization of inference costs\nEvent-driven serverless processing pipeline –Provides a lightweight, scalable approach to handling dynamic workloads using Amazon EventBridge, Amazon Simple Queue Service (Amazon SQS), AWS Lambda, Amazon Simple Storage Service (Amazon S3), and Amazon DynamoDB.\nPrompt management in Bedrock – Supports versioning, testing, and deployment of prompts for improved AI consistency and control\nTask orchestration – Uses Amazon SQS to manage work queues efficiently, facilitating smooth and scalable task execution\n\nPerformLine implemented a scalable, serverless event-driven architecture (shown in the following diagram) that seamlessly integrates with their existing system, requiring less than a day to develop and deploy. This made it possible to focus on prompt optimization, evaluation, and cost management rather than infrastructure overhead. This architecture allows PerformLine to dynamically parse, extract, and analyze web content with high reliability, flexibility, and cost-efficiency.\n\n\nThe system implements multiple queue types (Incoming, DLQ, Results) and includes error handling mechanisms. Data flows through various AWS services including: Amazon RDS for initial data storage Amazon MQ RabbitMQ for message handling Amazon S3 for asset storage Amazon EventBridge for event management Amazon SQS for queue management AWS Lambda for serverless processing Amazon DynamoDB for NoSQL data storage\n\nPerformLine’s process consists of several steps, including processing (Step 1), event trigger and storage (Steps 2–6), structured output and storage (Step 7), and downstream processing and compliance checks (Steps 8–9):\n\nMillions of pages are processed by an upstream extract, transform, and load (ETL) process from PerformLine’s core systems running on the AWS Cloud.\nWhen a page is retrieved, it triggers an event in the compliance check system.\nAmazon S3 allows for storage of the data from a page according to metadata.\nEventBridge uses event-driven processing to route Amazon S3 events to Amazon SQS.\nAmazon SQS queues messages for processing and enables messages to be retried on failure.\nA Lambda Function consumes SQS messages and also scales dynamically to handle even unpredictable workloads: \n  \nThis function uses Amazon Bedrock to perform extraction and generative AI analysis of the content from Amazon SQS. Amazon Bedrock offers the greatest flexibility to choose the right model for the job. For PerformLine’s use case, Amazon’s Nova Pro was best suited for complex requests that require a powerful model but still allows for a high performance to cost ratio. Anthropic’s Claude Haiku model allows for optimized quick calls, where a fast response is paramount for additional processing if needed. Amazon Bedrock features, including Amazon Bedrock Prompt Management and inference profiles are used to increase input code variability without affecting output and reduce complexity in usage of FMs through Amazon Bedrock.\nThe function stores customer-defined product schemas in Amazon DynamoDB, enabling dynamic large language model (LLM) targeting and schema-driven output generation.\n \nAmazon S3 stores the extracted data, which is formatted as structured JSON adhering to the target schema.\nEventBridge forwards Amazon S3 events to Amazon SQS, making extracted data available for downstream processing.\nCompliance checks and business rules, running on other PerformLine’s systems, are applied to validate and enforce regulatory requirements.\n\nCost optimizations\nThe solution offers several cost optimizations, including change data capture (CDC) on the web and strategic multi-pass inference. After a page’s content has been analyzed and formatted, it’s written back to a partition that includes a metadata hash of the asset. This enables upstream processes to determine whether a page has already been processed and if its content has changed. The key benefits of this approach include:\n\nAlleviating redundant processing of the same pages, contributing to PerformLine experiencing a 15% workload reduction in human evaluation tasks. This frees time for human evaluators and allows them focus on critical pages rather than all the pages.\nAvoiding reprocessing unchanged pages, dynamically reducing PerformLine’s analysts’ workload by over 50% in addition to deduplication gains.\n\nLLM inference costs can escalate at scale, but context and carefully structured prompts are critical for accuracy. To optimize costs while maintaining precision, PerformLine implemented a multi-pass approach using Amazon Bedrock:\n\nInitial filtering with Amazon Nova Micro – This lightweight model efficiently identifies relevant products with minimal cost.\nTargeted extraction with Amazon Nova Lite – Identified products are batched into smaller groups and passed to Amazon Nova Lite for deeper analysis. This keeps PerformLine within token limits while improving extraction accuracy.\nIncreased accuracy through context-aware processing – By first identifying the target content and then processing it in smaller batches, PerformLine significantly improved accuracy while minimizing token consumption.\n\nUse of Amazon Bedrock\nDuring initial testing, PerformLine quickly realized the need for a more scalable approach to prompt management. Manually tracking multiple prompt versions and templates became inefficient as PerformLine iterated and collaborated.\nAmazon Bedrock’s Prompt Management service provided a centralized solution, enabling them to version, manage, and seamlessly deploy prompts to production. After the prompts are deployed, they can be dynamically referenced in AWS Lambda, allowing for flexible configuration. Additionally, by using Amazon Bedrock application profile inference endpoints, PerformLine can dynamically adjust the models the Lambda function invokes, track cost per invocation, and attribute costs to specific application instances through setting up cost tags.\nTo streamline model interactions, PerformLine chose the Amazon Bedrock Converse API which provides a developer-friendly, standardized interface for model invocation. When combined with inference endpoints and prompt management, a Lambda function using the Amazon Bedrock Converse API becomes highly configurable—PerformLine developers can rapidly test new models and prompts, evaluate results, and iterate without needing to rebuild or redeploy. The simplification of prompt management and ability to deploy various models through Amazon Bedrock is shown in the following diagram.\n\n\nComprehensive AWS ML model configuration architecture highlighting three main components: Inference System: Model ID integration Profile configuration Content management Inference settings Prompt Management: Version control (V1 and Draft versions) Publish ID tracking Model A specifications Store configurations Environment Control: Separate PROD and DEV paths Environment-specific parameter stores Invoke ID management Engineering iteration tracking\n\nFuture plans and enhancements\nPerformLine is excited to dive into additional Amazon Bedrock features, including prompt caching and Amazon Bedrock Flows.\nWith prompt caching, users can checkpoint prompt tokens, effectively caching context for reuse in subsequent API calls. Prompt caching on Amazon Bedrock offers up to 85% latency improvements and 90% cost reduction in comparison to calls without prompt caching. PerformLine sees prompt caching as a feature that will become the standard moving forward. They have a number of use cases for their data, and having the ability to apply further analysis on the same content at a lower cost creates new opportunities for feature expansion and development.\nAmazon Bedrock Flows is a visual workflow builder that enables users to orchestrate multi-step generative AI tasks by connecting FMs and APIs without extensive coding. Amazon Bedrock Flows is a next step in simplifying PerformLine’s orchestration of knowledge bases, prompt caching, and even Amazon Bedrock agents in the future. Creating flows can help reduce time to feature deployment and maintenance.\nSummary\nPerformLine has implemented a highly scalable, serverless, AI-driven architecture that enhances efficiency, cost-effectiveness, and compliance in the web content processing pipeline. By using Amazon Bedrock, EventBridge, Amazon SQS, Lambda, and DynamoDB, they have built a solution that can dynamically scale, optimize AI inference costs, and reduce redundant processing—all while maintaining operational flexibility and compliance integrity. Based on their current volume and workflow, PerformLine is projected to process between 1.5 to 2 million pages daily, from which they expect to extract approximately 400,000 to 500,000 products. Additionally, PerformLine anticipates applying rules to each asset, resulting in about 500,000 rule observations that will require review each day.Throughout the design process PerformLine made sure their solution remains as simple as possible while still delivering operational flexibility and integrity. This approach minimizes complexity, enhances maintainability, and accelerates deployment, empowering them to adapt quickly to evolving business needs without unnecessary overhead.\nBy using a serverless AI-driven architecture built on Amazon Bedrock, PerformLine helps their customers tackle even the most complex, multi-product webpages with unparalleled accuracy and efficiency. This holistic approach interprets visual and textual elements as a typical consumer would, verifying that every product variant is accurately assessed for compliance. The resulting insights are then fed directly into a rules engine, enabling rapid, data-driven decisions. For PerformLine’s customers, this means less redundant processing, lower operational costs, and a dramatically simplified compliance workflow, all without compromising on speed or accuracy. By reducing the overhead of large-scale data analysis and streamlining compliance checks, PerformLine’s solution ultimately frees teams to focus on driving innovation and delivering value.\n\nAbout the authors\nBogdan Arsenie is the Chief Technology Officer at PerformLine, with over two decades of experience leading technological innovation across digital advertising, big data, mobile gaming, and social engagement. Bogdan began programming at age 13, customizing bulletin board software to fund his passion for Star Trek memorabilia. He served as PerformLine’s founding CTO from 2007–2009, pioneering their initial compliance platform. Later, as CTO at the Rumie Initiative, he helped scale a global education initiative recognized by Google’s Impact Challenge.\nNick Mattei is a Senior Software Engineer at PerformLine. He is focused on solutions architecture and distributed application development in AWS. Outside of work, Nick is an avid cyclist and skier, always looking for the next great climb or powder day.\nShervin Suresh is a Generative AI Solutions Architect at AWS. He supports generative AI adoption both internally at AWS and externally with fast-growing startup customers. He is passionate about using technology to help improve the lives of people in all aspects. Outside of work, Shervin loves to cook, build LEGO, and collaborate with people on things they are passionate about.\nMedha Aiyah is a Solutions Architect at AWS. She graduated from the University of Texas at Dallas with an MS in Computer Science, with a focus on AI/ML. She supports ISV customers in a wide variety of industries, by empowering customers to use AWS optimally to achieve their business goals. She is especially interested in guiding customers on ways to implement AI/ML solutions and use generative AI. Outside of work, Medha enjoys hiking, traveling, and dancing.\nMichael Zhang is a generalist Solutions Architect at AWS working with small to medium businesses. He has been with Amazon for over 3 years and uses his background in computer science and machine learning to support customers on AWS. In his free time, Michael loves to hike and explore other cultures.",
      "date": "2025-07-25",
      "authors": "Bogdan Arsenie, Nick Mattei",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "The article details how PerformLine leverages generative AI and Amazon Bedrock to enhance compliance monitoring of complex marketing webpages at scale by using scalable, serverless AI-driven architecture for efficient content extraction, prompt management, and cost optimization. It highlights advances in AI techniques like prompt engineering, multi-model inference, and event-driven processing to achieve accurate and cost-effective regulatory compliance checks.",
      "takeaways": [
        "- PerformLine uses Amazon Bedrock’s foundation models and prompt management capabilities to dynamically parse and analyze multi-product webpages for compliance, improving speed and accuracy.",
        "- The serverless, event-driven architecture optimizes AI inference costs via multi-pass model usage and caching while scaling to process millions of pages daily.",
        "- Future enhancements with Amazon Bedrock features like prompt caching and AI workflow orchestration are expected to further reduce latency and streamline generative AI tasks."
      ]
    },
    {
      "id": 15,
      "title": "Boost cold-start recommendations with vLLM on AWS Trainium",
      "link": "https://aws.amazon.com/blogs/machine-learning/boost-cold-start-recommendations-with-vllm-on-aws-trainium/",
      "description": "Cold start in recommendation systems goes beyond just new user or new item problems—it’s the complete absence of personalized signals at launch. When someone first arrives, or when fresh content appears, there’s no behavioral history to tell the engine what they care about, so everyone ends up in broad generic segments. That not only dampens click-through and conversion rates, it can drive users away before a system ever gets a chance to learn their tastes. Standard remedies—collaborative filtering, matrix factorization, or popularity lists—lack the nuance to bridge that signal gap, and their one-size-fits-all suggestions quickly feel stale. Imagine, instead, if you could generate detailed interest profiles from day one. By tapping into large language models (LLMs) for zero-shot reasoning, you can synthesize rich, context-aware user and item embeddings without waiting for weeks of interaction data—turning a cold start into a warm welcome.\nIn this post, we demonstrate how to use vLLM for scalable inference and use AWS Deep Learning Containers (DLC) to streamline model packaging and deployment. We’ll generate interest expansions through structured prompts, encode them into embeddings, retrieve candidates with FAISS, apply validation to keep results grounded, and frame the cold-start challenge as a scientific experiment—benchmarking LLM and encoder pairings, iterating rapidly on recommendation metrics, and showing clear ROI for each configuration.\nSolution overview\nWe build our cold-start solution on Amazon EC2 Trainium chips. To streamline model deployment, we use DLCs with the AWS Neuron SDK, which installs Neuron-optimized PyTorch modules and includes the latest AWS Trainium drivers and runtime pre-installed.\n\n\nFigure : Cold-start recommendation pipeline on AWS Trainium with vLLM & NxD\n\nSharding large models across multiple Trainium chips is handled by the distributed library used by Neuron, NeuronX Distributed (NxD), which integrates seamlessly with vLLM. NxD manages model partitions across multiple instances with minimal code changes, enabling parallel inference of even 70B parameter LLMs. This combination—Trainium chips, Neuron Tools, and vLLM—gives machine learning (ML) engineers a flexible, cost-efficient, production-ready solution for experimenting with different LLM and encoder configurations and delivers rapid iteration on recommendation quality metrics without modifying core model code.\nIn the next section, we orchestrate our experiments in a Jupyter notebook—providing a reproducible, end-to-end workflow from loading data and engineering structured prompts to generating embeddings and retrieving candidates with FAISS—complete with interactive charts to visualize recommendation performance. Then, in the production deep-dive, we walk through a reference implementation that packages your Neuron-optimized LLM and encoder as DLC images and deploys them on Amazon Elastic Kubernetes Service (Amazon EKS) with autoscaling, so your inference layer automatically adapts to demand while optimizing cost and performance.\nExpanding user interest profiles with LLMs\nIn this post, we use the Amazon Book Reviews dataset (mohamedbakhet/amazon-books-reviews) from Kaggle, which provides real-world user reviews and metadata for tens of thousands of books. This rich collection lets us simulate cold-start scenarios—where a brand-new user has only a single review or like—and evaluate how well our interest expansions, powered by distilled versions of Meta’s Llama 8B and 70B models, generate rich user profiles. We use an LLM to enrich a new user’s profile from minimal initial data. For example, if a user has only reviewed one science fiction novel, the LLM infers related subtopics—such as galactic empires, cyberpunk dystopias, or space exploration—that the user is likely to enjoy. We use structured prompts that embed the user’s existing activity into a concise instruction to verify consistency and relevance, as demonstrated in the following example:\n\nprompt = (\nf\"The user has shown interest in: {user_review_category}.\\n\"\n\"Suggest 3–5 related book topics they might enjoy.\\n\"\n\"Respond with a JSON list of topic keywords.\"\n)\nexpanded_topics = llm.generate([prompt])[0].text\n\nBy constraining the LLM’s output format—asking it to return a JSON array of topic keywords—we avoid free‑form tangents and obtain a predictable list of interest expansions. Modern generative models, such as Meta’s Llama, possess broad domain knowledge and human‑like reasoning, enabling them to connect related concepts and serve as powerful cold‑start boosters by inferring deep user preferences from a single review. These synthetic interests become new signals for our recommendation pipeline, allowing us to retrieve and rank books from the Amazon Reviews collection even with minimal user history. You can experiment with Llama variants ranging from one‑billion to seventy‑billion parameters to identify which model yields the most discriminative and relevant expansions. Those findings will guide our choice of model for production and determine the size and scale of the Amazon EC2 Trainium and Inferentia instances we provision, setting us up for live user A/B tests to validate performance in real‑world settings.\nEncoding user interests and retrieving relevant content\nAfter we have our expanded interests, the next step is to turn both those interests and our catalog of books into vectors that we can compare. We explore three sizes of the Google T5 encoder—base, large and XL—to see how embedding dimensionality affects matching quality. The following are the steps:\n\nLoad the encoder for each size\nEncode book summaries into a single NumPy matrix and normalize it\nBuild a FAISS index on those normalized vectors for fast nearest‑neighbor search\nEncode the expanded interest text the same way and query FAISS to retrieve the top k most similar books\n\n\nfrom transformers import T5Tokenizer, T5EncoderModel\nimport faiss\nimport numpy as np\n\n# Our dataset of book summaries\ncontent_texts = df[\"review/summary\"].tolist()\nencoder_sizes = [\"t5-base\", \"t5-large\", \"t5-xl\"]\ntop_k = 5\n\nfor size in encoder_sizes:\n    # 1. Load the tokenizer and encoder model for this size\n    tokenizer = T5Tokenizer.from_pretrained(size)\n    model = T5EncoderModel.from_pretrained(size)\n\n    # 2. Encode all content into embeddings and normalize\n    inputs = tokenizer(content_texts, return_tensors=\"pt\", truncation=True, padding=True)\n    outputs = model(**inputs)\n    content_embs = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy().astype(\"float32\")\n    faiss.normalize_L2(content_embs)\n\n    # 3. Build a FAISS index using inner-product (equivalent to cosine on unit vectors)\n    index = faiss.IndexFlatIP(content_embs.shape[1])\n    index.add(content_embs)\n\n    # 4. Encode a single expanded interest and query the index\n    interest = \"space opera with political intrigue\"\n    enc = tokenizer([interest], return_tensors=\"pt\", truncation=True, padding=True)\n    interest_emb = model(**enc).last_hidden_state.mean(dim=1).detach().cpu().numpy().astype(\"float32\")\n    faiss.normalize_L2(interest_emb)\n\n    distances, indices = index.search(interest_emb, top_k)\n    recommendations = [content_texts[i] for i in indices[0]]\n\n    print(f\"\\nTop {top_k} recommendations using {size}:\")\n    for title in recommendations:\n        print(\" -\", title)\n\nYou can compare how each encoder scale affects both the average FAISS distance (that is, how far apart your interest is from the content) and the actual recommended titles. Swapping in a different encoder family—such as SentenceTransformers—is as straightforward as replacing the model and tokenizer imports.\nMeasuring and improving recommendation quality\nNow that we’ve generated FAISS indexes for every LLM‑encoder pairing and computed the mean distance between each expanded interest query and its top 10 neighbors, we know exactly how tightly or loosely each model’s embeddings cluster. The following chart shows those average distances for each combination—revealing that 1B and 3B models collapse to almost zero, while 8B and 70B models (especially with larger encoders) produce progressively higher distances, signifying richer, more discriminative signals for recommendation.\n\n\nFigure : Average FAISS distance by model and encoder\n\nThe chart shows that the 1B and 3B models yield an average FAISS distance of zero, meaning their expanded‑interest embeddings are essentially identical and offer no differentiation. By contrast, the 8B model produces a distance of about 0.5 with t5‑base, rising further with t5‑large and t5‑xl, which demonstrates that larger encoders capture more of the model’s nuance. The 70B model only adds a small boost—and only with the XL encoder—so its extra cost yields limited benefit.\nIn practical terms, a Llama 8B LLM paired with a base or large T5 encoder delivers clear separation in embedding space without the higher inference time and resource usage of a 70B model.\nComparing model and encoder impact on embedding spread\nTo see how LLM size and encoder scale shape our embedding space, you can measure—for each LLM and encoder  pair—the mean FAISS distance from a representative expanded interest vector to its top 10 neighbors. The following bar chart plots those averages side by side. You can instantly spot that 1B and 3B collapse to zero, 8B jumps to around 0.5 and rises with larger encoders, and 70B only adds a small extra spread at the XL scale. This helps you choose the smallest combination that still gives you the embedding diversity needed for effective cold‑start recommendations.\n\n\nFigure : FAISS distance by LLM and encoder size\n\nEvaluating recommendation overlap across Llama variations and encoders to balance consistency and novelty\nIn the next analysis, you build a basic recommend_books helper that, for various LLM sizes and encoder choices, loads the corresponding expanded‑interest DataFrame, reads its FAISS index, reconstructs the first embedding as a stand‑in query, and returns the top-k book titles. Using this helper, we first measure how much each pair of encoders agrees on recommendations for a single LLM—comparing base compared to large, base compared to XL, and large compared XL—and then, separately, how each pair of LLM sizes aligns for a fixed encoder. Finally, we focus on the 8B model (shown in the following figure) and plot a heatmap of its encoder overlaps, which shows that base and large share about 40% of their top 5 picks while XL diverges more—illustrating how changing the encoder shifts the balance between consistency and novelty in the recommendations.\n\n\nFigure : 8B model: encoder overlap heatmap\n\nFor the 8B model, the heatmap shows that t5_base and t5_large share 40% of their top 5 recommendations, t5_base and t5_xl also overlap 40%, while t5_large vs t5_xl overlap only 20%, indicating that the XL encoder introduces the greatest amount of novel titles compared to the other pairs.\nTweaking tensor_parallel_size for optimal cost performance\nTo balance inference speed against resource cost, we measured how increasing Neuron tensor parallelism affects latency when expanding user interests with the Llama 3.1 8B model on a trn1.32xlarge instance. We ran the same zero‑shot expansion workload at tensor_parallel_size values of 2, 8, 16, and 32. As shown in the first chart, P50 Latency falls by 74 %—from 2,480 ms at TP = 2 to 650 ms at TP = 16—then inches lower to 532 ms at TP = 32 (an additional 18 % drop). The following cost-to-performance chart shows that beyond TP = 16, doubling parallelism roughly doubles cost for only a 17 % further latency gain.\n\n\nFigure : Latency compared to tensor parallel size\n\nIn practice, setting tensor_parallel_size to 16 delivers the best trade‑off: you capture most of the speed‑up from model sharding while avoiding the sharply diminishing returns and higher core‑hour costs that come with maximal parallelism, as shown in the following figure.\n\n\nFigure : Cost-performance compared to tensor parallel size\n\nThe preceding figure visualizes the cost-to-performance ratio of the Llama 8B tests, emphasizing that TP=16 offers the most balanced efficiency before the benefits plateau.\nWhat’s next?\nNow that we have determined the models and encoders to use, as well as the optimal configuration to use with our dataset, such as sequence size and batch size, the next step is to deploy the models and define a production workflow that generates expanded interest that is encoded and ready for match with more content.\nConclusion\nThis post showed how AWS Trainium, the Neuron SDK, and scalable LLM inference can tackle cold-start challenges by enriching sparse user profiles for better recommendations from day one.\nImportantly, our experiments highlight that larger models and encoders don’t always mean better outcomes. While they can produce richer signals, the gains often don’t justify the added cost. You might find that an 8B LLM with a T5-large encoder strikes the best balance between performance and efficiency.\nRather than assuming bigger is better, this approach helps teams identify the optimal model-encoder pair—delivering high-quality recommendations with cost-effective infrastructure.\n\nAbout the authors\nYahav Biran is a Principal Architect at AWS, focusing on large-scale AI workloads. He contributes to open-source projects and publishes in AWS blogs and academic journals, including the AWS compute and AI blogs and the Journal of Systems Engineering. He frequently delivers technical presentations and collaborates with customers to design Cloud applications. Yahav holds a Ph.D. in Systems Engineering from Colorado State University.\n Nir Ozeri Nir is a Sr. Solutions Architect Manager with Amazon Web Services, based out of New York City. Nir leads a team of Solution Architects focused on ISV customers. Nir specializes in application modernization, application and product delivery, and scalable application architecture.",
      "date": "2025-07-24",
      "authors": "Yahav Biran",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article discusses leveraging large language models (LLMs) such as Meta's Llama, combined with AWS Trainium hardware and Neuron SDK tools, to overcome cold-start problems in recommendation systems by generating rich user interest profiles through zero-shot reasoning and embedding techniques. It highlights scalable deployment strategies, model-encoder trade-offs, and cost-performance optimizations for effective AI-driven recommendations.",
      "takeaways": [
        "- Utilizing LLMs for zero-shot interest expansion can significantly improve recommendation quality in cold-start scenarios without extensive user interaction data.",
        "- The integration of AWS Trainium, vLLM, and NeuronX Distributed enables scalable, efficient inference of large models with flexible deployment on cloud infrastructure.",
        "- Balanced model and encoder sizes (e.g., 8B Llama with T5-large encoder) provide optimal cost-performance ratios, emphasizing that bigger models don’t always lead to better recommendations."
      ]
    },
    {
      "id": 16,
      "title": "Benchmarking Amazon Nova: A comprehensive analysis through MT-Bench and Arena-Hard-Auto",
      "link": "https://aws.amazon.com/blogs/machine-learning/benchmarking-amazon-nova-a-comprehensive-analysis-through-mt-bench-and-arena-hard-auto/",
      "description": "Large language models (LLMs) have rapidly evolved, becoming integral to applications ranging from conversational AI to complex reasoning tasks. However, as models grow in size and capability, effectively evaluating their performance has become increasingly challenging. Traditional benchmarking metrics like perplexity and BLEU scores often fail to capture the nuances of real-world interactions, making human-aligned evaluation frameworks crucial. Understanding how LLMs are assessed can lead to more reliable deployments and fair comparisons across different models.\nIn this post, we explore automated and human-aligned judging methods based on LLM-as-a-judge. LLM-as-a-judge refers to using a more powerful LLM to evaluate and rank responses generated by other LLMs based on predefined criteria such as correctness, coherence, helpfulness, or reasoning depth. This approach has become increasingly popular due to the scalability, consistency, faster iteration, and cost-efficiency compared to solely relying on human judges. We discuss different LLM-as-a-judge evaluation scenarios, including pairwise comparisons, where two models or responses are judged against each other, and single-response scoring, where individual outputs are rated based on predefined criteria. To provide concrete insights, we use MT-Bench and Arena-Hard, two widely used evaluation frameworks. MT-Bench offers a structured, multi-turn evaluation approach tailored for chatbot-like interactions, whereas Arena-Hard focuses on ranking LLMs through head-to-head response battles in challenging reasoning and instruction-following tasks. These frameworks aim to bridge the gap between automated and human judgment, making sure that LLMs aren’t evaluated solely based on synthetic benchmarks but also on practical use cases.\nThe repositories for MT-Bench and Arena-Hard were originally developed using OpenAI’s GPT API, primarily employing GPT-4 as the judge. Our team has expanded its functionality by integrating it with the Amazon Bedrock API to enable using Anthropic’s Claude Sonnet on Amazon as judge. In this post, we use both MT-Bench and Arena-Hard to benchmark Amazon Nova models by comparing them to other leading LLMs available through Amazon Bedrock.\nAmazon Nova models and Amazon Bedrock\nOur study evaluated all four models from the Amazon Nova family, including Amazon Nova Premier, which is the most recent addition to the family. Introduced at AWS re:Invent in December 2024, Amazon Nova models are designed to provide frontier-level intelligence with leading price-performance ratios. These models rank among the fastest and most economical options in their respective intelligence categories and are specifically optimized for powering enterprise generative AI applications in a cost-effective, secure, and reliable manner.\nThe understanding model family comprises four distinct tiers: Amazon Nova Micro (text-only, designed for ultra-efficient edge deployment), Amazon Nova Lite (multimodal, optimized for versatility), Amazon Nova Pro (multimodal, offering an ideal balance between intelligence and speed for most enterprise applications), and Amazon Nova Premier (multimodal, representing the most advanced Nova model for complex tasks and serving as a teacher for model distillation). Amazon Nova models support a wide range of applications, including coding, reasoning, and structured text generation.\nAdditionally, through Amazon Bedrock Model Distillation, customers can transfer the intelligence capabilities of Nova Premier to faster, more cost-effective models such as Nova Pro or Nova Lite, tailored to specific domains or use cases. This functionality is accessible through both the Amazon Bedrock console and APIs, including the Converse API and Invoke API.\nMT-Bench analysis\nMT-Bench is a unified framework that uses LLM-as-a-judge, based on a set of predefined questions. The evaluation questions are a set of challenging multi-turn open-ended questions designed to evaluate chat assistants. Users also have the flexibility to define their own question and answer pairs in a way that suits their needs. The framework presents models with challenging multi-turn questions across eight key domains:\n\nWriting\nRoleplay\nReasoning\nMathematics\nCoding\nData Extraction\nSTEM\nHumanities\n\nThe LLMs are evaluated using two types of evaluation:\n\nSingle-answer grading – This mode asks the LLM judge to grade and give a score to a model’s answer directly without pairwise comparison. For each turn, the LLM judge gives a score on a scale of 0–10. Then the average score is computed on all turns.\nWin-rate based grading – This mode uses two metrics: \n  \npairwise-baseline – Run a pairwise comparison against a baseline model.\npairwise-all – Run a pairwise comparison between all model pairs on all questions.\n \n\nEvaluation setup\nIn this study, we employed Anthropic’s Claude 3.7 Sonnet as our LLM judge, given its position as one of the most advanced language models available at the time of our study. We focused exclusively on single-answer grading, wherein the LLM judge directly evaluates and scores model-generated responses without conducting pairwise comparisons.\nThe eight domains covered in our study can be broadly categorized into two groups: those with definitive ground truth and those without. Specifically, Reasoning, Mathematics, Coding, and Data Extraction fall into the former category because they typically have reference answers against which responses can be objectively evaluated. Conversely, Writing, Roleplay, STEM, and Humanities often lack such clear-cut ground truth. Here we provide an example question from the Writing and Math categories:\n\n{\n    \"question_id\": 81,\n    \"category\": \"writing\",\n    \"turns\": [\n        \"Compose an engaging travel blog post about a recent trip to Hawaii, \n        highlighting cultural experiences and must-see attractions.\", \n        \"Rewrite your previous response. Start every sentence with the letter A.\"\n    ]\n}\n{\n    \"question_id\": 111,\n    \"category\": \"math\",\n    \"turns\": [\n        \"The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). \n         What is the area of the triangle?\",\n        \"What's area of the circle circumscribing the triangle?\"\n    ],\n    \"reference\": [\n        \"Area is 3\",\n        \"5pi\"\n    ]\n}\n\nTo account for this distinction, MT-Bench employs different judging prompts for each category (refer to the following GitHub repo), tailoring the evaluation process to the nature of the task at hand. As shown in the following evaluation prompt, for questions without a reference answer, MT-Bench adopts the single-v1 prompt, only passing the question and model-generated answer. When evaluating questions with a reference answer, it only passes the reference_answer, as shown in the single-math-v1 prompt.\n\n{\n    \"name\": \"single-v1\",\n    \"type\": \"single\",\n    \"system_prompt\": \"You are a helpful assistant.\",\n    \"prompt_template\": \n        \"[Instruction]\\nPlease act as an impartial judge and evaluate the quality of\n         the response provided by an AI assistant to the user question displayed below. \n         Your evaluation should consider factors such as the helpfulness, relevance, accuracy, \n         depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. \n        After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", \n        for example: \\\"Rating: [[5]]\\\".\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\",\n    \"description\": \"Prompt for general questions\",\n    \"category\": \"general\",\n    \"output_format\": \"[[rating]]\"\n}\n{\t\n    \"name\": \"single-math-v1\",\n    \"type\": \"single\",\n    \"system_prompt\": \"You are a helpful assistant.\",\n    \"prompt_template\": \n        \"[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant \n        to the user question displayed below. Your evaluation should consider correctness and helpfulness. \n        You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. \n        Identify and correct any mistakes. Be as objective as possible. After providing your explanation, \n        you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\n        \\n\\n[Question]\\n{question}\\n\\n[The Start of Reference Answer]\\n{ref_answer_1}\\n[The End of Reference Answer]\n        \\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\",\n    \"description\": \"Prompt for general questions\",\n    \"category\": \"math\",\n    \"output_format\": \"[[rating]]\"\n}\n\nOverall performance analysis across Amazon Nova Models\nIn our evaluation using Anthropic’s Claude 3.7 Sonnet as an LLM-as-a-judge framework, we observed a clear performance hierarchy among Amazon Nova models. The scores ranged from 8.0 to 8.6, with Amazon Nova Premier achieving the highest median score of 8.6, followed closely by Amazon Nova Pro at 8.5. Both Amazon Nova Lite and Nova Micro achieved respectable median scores of 8.0.\nWhat distinguishes these models beyond their median scores is their performance consistency. Nova Premier demonstrated the most stable performance across evaluation categories with a narrow min-max margin of 1.5 (ranging from 7.94 to 9.47). In comparison, Nova Pro showed greater variability with a min-max margin of 2.7 (from 6.44 to 9.13). Similarly, Nova Lite exhibited more consistent performance than Nova Micro, as evidenced by their respective min-max margins. For enterprise deployments where response time is critical, Nova Lite and Nova Micro excel with less than 6-second average latencies for single question-answer generation. This performance characteristic makes them particularly suitable for edge deployment scenarios and applications with strict latency requirements. When factoring in their lower cost, these models present compelling options for many practical use cases where the slight reduction in performance score is an acceptable trade-off.\nInterestingly, our analysis revealed that Amazon Nova Premier, despite being the largest model, demonstrates superior token efficiency. It generates more concise responses that consume up to 190 fewer tokens for single question-answer generation than comparable models. This observation aligns with research indicating that more sophisticated models are generally more effective at filtering irrelevant information and structuring responses efficiently.\nThe narrow 0.6-point differential between the highest and lowest performing models suggests that all Amazon Nova variants demonstrate strong capabilities. Although larger models such as Nova Premier offer marginally better performance with greater consistency, smaller models provide compelling alternatives when latency and cost are prioritized. This performance profile gives developers flexibility to select the appropriate model based on their specific application requirements.\nThe following graph summarizes the overall performance scores and latency for all four models.\n\nThe following table shows token consumption and cost analysis for Amazon Nova Models.\n\n\n\nModel\nAvg. total tokens per query\nPrice per 1k input tokens\nAvg. cost per query (cents)\n\n\nAmazon Nova Premier\n2154\n$0.0025\n$5.4\n\n\nAmazon Nova Pro\n2236\n$0.0008\n$1.8\n\n\nAmazon Nova Lite\n2343\n$0.00006\n$0.14\n\n\nAmazon Nova Micro\n2313\n$0.000035\n$0.08\n\n\n\nCategory-specific model comparison\nThe following radar plot compares the Amazon Nova models across all eight domains.\n\nThe radar plot reveals distinct performance patterns across the Amazon Nova model family, with a clear stratification across domains. Nova Premier consistently outperforms its counterparts, showing particular strengths in Math, Reasoning, Humanities, and Extraction, where it achieves scores approaching or exceeding 9. Nova Pro follows closely behind Premier in most categories, maintaining competitive performance especially in Writing and Coding, while showing more pronounced gaps in Humanities, Reasoning, and Math. Both Nova Lite and Micro demonstrate similar performance profiles to each other, with their strongest showing in Roleplay, and their most significant limitations in Humanities and Math, where the differential between Premier and the smaller models is most pronounced (approximately 1.5–3 points).\nThe consistent performance hierarchy across all domains (Premier > Pro > Lite ≈ Micro) aligns with model size and computational resources, though the magnitude of these differences varies significantly by category. Math and reasoning emerge among the most discriminating domains for model capability assessment and suggest substantial benefit from the additional scale of Amazon Nova Premier. However, workloads focused on creative content (Roleplay, Writing) provide the most consistent performance across the Nova family and suggest smaller models as compelling options given their latency and cost benefits. This domain-specific analysis offers practitioners valuable guidance when selecting the appropriate Nova model based on their application’s primary knowledge requirements.\nIn this study, we adopted Anthropic’s Claude 3.7 Sonnet as the single LLM judge. However, although Anthropic’s Claude 3.7 Sonnet is a popular choice for LLM judging due to its capabilities, studies have shown that it does exhibit certain bias (for example, it prefers longer responses). If permitted by time and resources, consider adopting a multi-LLM judge evaluation framework to effectively reduce biases intrinsic to individual LLM judges and increase evaluation reliability.\nArena-Hard-Auto analysis\nArena-Hard-Auto is a benchmark that uses 500 challenging prompts as a dataset to evaluate different LLMs using LLM-as-a-judge. The dataset is curated through an automated pipeline called BenchBuilder, which uses LLMs to automatically cluster, grade, and filter open-ended prompts from large, crowd-sourced datasets such as Chatbot-Arena to enable continuous benchmarking without a human in the loop. The paper reports that the new evaluation metrics provide three times higher separation of model performances compared to MT-Bench and achieve a 98.6% correlation with human preference rankings.\nTest framework and methodology\nThe Arena-Hard-Auto benchmarking framework evaluates different LLMs using a pairwise comparison. Each model’s performance is quantified by comparing it against a strong baseline model, using a structured, rigorous setup to generate reliable and detailed judgments. We use the following components for the evaluation:\n\nPairwise comparison setup – Instead of evaluating models in isolation, they’re compared directly with a strong baseline model. This baseline provides a fixed standard, making it straightforward to understand how the models perform relative to an already high-performing model.\nJudge model with fine-grained categories – A powerful model (Anthropic’s Claude 3.7 Sonnet) is used as a judge. This judge doesn’t merely decide which model is better, it also categorizes the comparison into five detailed preference labels. By using this nuanced scale, large performance gaps are penalized more heavily than small ones, which helps separate models more effectively based on performance differences: \n  \nA >> B (A is significantly better than B)\nA > B (A is better than B)\nA ~= B (A and B are similar)\nB > A (B is better than A)\nB >> A (B is significantly better than A)\n \nChain-of-thought (CoT) prompting – CoT prompting encourages the judge model to explain its reasoning before giving a final judgment. This process can lead to more thoughtful and reliable evaluations by helping the model analyze each response in depth rather than making a snap decision.\nTwo-game setup to avoid position bias – To minimize bias that might arise from a model consistently being presented first or second, each model pair is evaluated twice, swapping the order of the models. This way, if there’s a preference for models in certain positions, the setup controls for it. The total number of judgments is doubled (for example, 500 queries x 2 positions = 1,000 judgments).\nBradley-Terry model for scoring – After the comparisons are made, the Bradley-Terry model is applied to calculate each model’s final score. This model uses pairwise comparison data to estimate the relative strength of each model in a way that reflects not only the number of wins but also the strength of wins. This scoring method is more robust than simply calculating win-rate because it accounts for pairwise outcomes across the models.\nBootstrapping for statistical stability – By repeatedly sampling the comparison results (bootstrapping), the evaluation becomes statistically stable. This stability is beneficial because it makes sure the model rankings are reliable and less sensitive to random variations in the data.\nStyle control – Certain style features like response length and markdown formatting are separated from content quality, using style controls, to provide a clearer assessment of each model’s intrinsic capabilities.\n\nThe original work focuses on pairwise comparison only. For our benchmarking, we also included our own implementation of single-score judgment, taking inspiration from MT-Bench. We again use Anthropic’s Claude 3.7 Sonnet as the judge and use the following prompt for judging without a reference model:\n\n{\t\n\t\"system_prompt\": \n\t\t\"Please act as an impartial judge and evaluate the quality \n\t\tof the response provided by an AI assistant to the user question \n\t\tdisplayed below. Your evaluation should consider factors \n\t\tsuch as the helpfulness, relevance, accuracy, depth, creativity, \n\t\tand level of detail of the response. \n\t\tBegin your evaluation by providing a short explanation. \n\t\tBe as objective as possible. After providing your explanation, \n\t\tyou must rate the response on a scale of 1 to 10 by strictly \n\t\tfollowing this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\"\n}\n\nPerformance comparison\nWe evaluated five models, including Amazon Nova Premier, Amazon Nova Pro, Amazon Nova Lite, Amazon Nova Micro, DeepSeek-R1, and a strong reference model. The Arena-Hard benchmark generates confidence intervals by bootstrapping, as explained before. The 95% confidence interval shows the uncertainty of the models and is indicative of model performance. From the following plot, we can see that all the Amazon Nova models get a high pairwise Bradley-Terry score. It should be noted that the Bradley-Terry score for the reference model is 5; this is because Bradley-Terry scores are computed by pairwise comparisons where the reference model is one of the models in the pair. So, for the reference model, the score will be 50%, and because the total score is normalized between 0 and 10, the reference model has a score of 5.\n\nThe confidence interval analysis, as shown in the following table, was done to statistically evaluate the Amazon Nova model family alongside DeepSeek-R1, providing deeper insights beyond raw scores. Nova Premier leads the pack (8.36–8.72), with DeepSeek-R1 (7.99–8.30) and Nova Pro (7.72–8.12) following closely. The overlapping confidence intervals among these top performers indicate statistically comparable capabilities. Nova Premier demonstrates strong performance consistency with a tight confidence interval (−0.16, +0.20), while maintaining the highest overall scores. A clear statistical separation exists between these leading models and the purpose-built Nova Lite (6.51–6.98) and Nova Micro (5.68–6.14), which are designed for different use cases. This comprehensive analysis confirms the position of Nova Premier as a top performer, with the entire Nova family offering options across the performance spectrum to meet varied customer requirements and resource constraints.\n\n\n\nModel\nPairwise score 25th quartile \nPairwise score 75th quartile\nConfidence interval\n\n\nAmazon Nova Premier\n8.36\n8.72\n(−0.16, +0.20)\n\n\nAmazon Nova Pro\n7.72\n8.12\n(−0.18, +0.23)\n\n\nAmazon Nova Lite\n6.51\n6.98\n(−0.22, +0.25)\n\n\nAmazon Nova Micro\n5.68\n6.14\n(−0.21, +0.25)\n\n\nDeepSeek-R1\n7.99\n8.30\n(−0.15, +0.16)\n\n\n\nCost per output token is one of the contributors to the overall cost of the LLM model and impacts the usage. The cost was computed based on the average output tokens over the 500 responses. Although Amazon Nova Premier leads in performance (85.22), Nova Light and Nova Micro offer compelling value despite their wider confidence intervals. Nova Micro delivers 69% of the performance of Nova Premier at 89 times cheaper cost, while Nova Light achieves 79% of the capabilities of Nova Premier, at 52 times lower price. These dramatic cost efficiencies make the more affordable Nova models attractive options for many applications where absolute top performance isn’t essential, highlighting the effective performance-cost tradeoffs across the Amazon Nova family.\nConclusion\nIn this post, we explored the use of LLM-as-a-judge through MT-Bench and Arena-Hard benchmarks to evaluate model performance rigorously. We then compared Amazon Nova models against a leading reasoning model, that is, DeepSeek-R1 hosted on Amazon Bedrock, analyzing their capabilities across various tasks. Our findings indicate that Amazon Nova models deliver strong performance, especially in Extraction, Humanities, STEM, and Roleplay, while maintaining lower operational costs, making them a competitive choice for enterprises looking to optimize efficiency without compromising on quality. These insights highlight the importance of benchmarking methodologies in guiding model selection and deployment decisions in real-world applications.\nFor more information on Amazon Bedrock and the latest Amazon Nova models, refer to the Amazon Bedrock User Guide and Amazon Nova User Guide. The AWS Generative AI Innovation Center has a group of AWS science and strategy experts with comprehensive expertise spanning the generative AI journey, helping customers prioritize use cases, build a roadmap, and move solutions into production. Check out Generative AI Innovation Center for our latest work and customer success stories.\n\nAbout the authors\n Mengdie (Flora) Wang is a Data Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalable Generative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology. Prior to AWS, Flora earned her Master’s degree in Computer Science from the University of Minnesota, where she developed her expertise in machine learning and artificial intelligence.\nBaishali Chaudhury is an Applied Scientist at the Generative AI Innovation Center at AWS, where she focuses on advancing Generative AI solutions for real-world applications. She has a strong background in computer vision, machine learning, and AI for healthcare. Baishali holds a PhD in Computer Science from University of South Florida and PostDoc from Moffitt Cancer Centre.\n Rahul Ghosh is an Applied Scientist at Amazon’s Generative AI Innovation Center, where he works with AWS customers across different verticals to expedite their use of Generative AI. Rahul holds a Ph.D. in Computer Science from the University of Minnesota.\n Jae Oh Woo is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he specializes in developing custom solutions and model customization for a diverse range of use cases. He has a strong passion for interdisciplinary research that connects theoretical foundations with practical applications in the rapidly evolving field of generative AI. Prior to joining Amazon, Jae Oh was a Simons Postdoctoral Fellow at the University of Texas at Austin. He holds a Ph.D. in Applied Mathematics from Yale University.\n Jamal Saboune is an Applied Science Manager with AWS Generative AI Innovation Center. He is currently leading a team focused on supporting AWS customers build innovative and scalable Generative AI products across several industries. Jamal holds a PhD in AI and Computer Vision from the INRIA Lab in France, and has a long R&D experience designing and building AI solutions that add value to users.\n Wan Chen is an Applied Science Manager at the Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence, and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia, and had worked as postdoctoral fellow in Oxford University.\n Anila Joshi has more than a decade of experience building AI solutions. As a AWSI Geo Leader at AWS Generative AI Innovation Center, Anila pioneers innovative applications of AI that push the boundaries of possibility and accelerate the adoption of AWS services with customers by helping customers ideate, identify, and implement secure generative AI solutions.",
      "date": "2025-07-24",
      "authors": "Flora Wang",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AWS",
      "terms": [
        "AWS"
      ],
      "summary": "This article presents a comprehensive benchmarking study of Amazon Nova large language models (LLMs) using advanced AI evaluation frameworks MT-Bench and Arena-Hard-Auto, leveraging Anthropic’s Claude 3.7 Sonnet as an LLM-as-a-judge. It analyzes the performance, cost-efficiency, and domain-specific capabilities of various Amazon Nova models, highlighting their suitability for enterprise generative AI applications and model customization via Amazon Bedrock.",
      "takeaways": [
        "- The study employs novel AI benchmarking techniques (LLM-as-a-judge) with frameworks MT-Bench and Arena-Hard-Auto to evaluate multiple Amazon Nova LLMs across diverse domains such as reasoning, coding, humanities, and STEM.",
        "- Amazon Nova Premier demonstrates the highest performance and efficiency, while smaller models like Nova Lite and Micro offer lower latency and significantly reduced costs, providing flexible options based on use case requirements.",
        "- The integration of model distillation and multi-LLM judging methods reflects advances in AI evaluation and deployment strategies, underscoring the importance of rigorous benchmarking to guide AI model selection for enterprise applications."
      ]
    },
    {
      "id": 17,
      "title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains",
      "link": "https://machinelearning.apple.com/research/mmau",
      "description": "Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "The article presents MMAU, a comprehensive benchmark designed to evaluate large language models' agent capabilities across diverse domains by dissecting underlying skills rather than only focusing on task completion. It addresses challenges in existing benchmarks such as lack of granularity, reproducibility, and setup complexity, aiming to provide a more reliable and detailed assessment of LLMs.",
      "takeaways": [
        "- Introduces a holistic benchmark for assessing the multifaceted capabilities of large language models as human-like agents.",
        "- Highlights the importance of analyzing underlying skills rather than just task outcomes.",
        "- Addresses issues of reliability, reproducibility, and environment setup in AI benchmarking."
      ]
    },
    {
      "id": 18,
      "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution",
      "link": "https://machinelearning.apple.com/research/aspera",
      "description": "This work evaluates the potential of large language models (LLMs) to power digital assistants capable of complex action execution. These assistants rely on pre-trained programming knowledge to execute multi-step goals by composing objects and functions defined in assistant libraries into action execution programs. To achieve this, we develop ASPERA, a framework comprising an assistant library simulation and a human-assisted LLM data generation engine. Our engine allows developers to guide LLM generation of high-quality tasks consisting of complex user queries, simulation state and…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "This article presents ASPERA, a simulated environment designed to evaluate the planning capabilities of large language models (LLMs) in executing complex multi-step actions. It introduces a framework combining an assistant library simulation with a human-assisted LLM data generation engine to improve task generation and execution in digital assistants.",
      "takeaways": [
        "- ASPERA provides a novel framework for testing and improving LLMs' ability to plan and execute complex actions.",
        "- The work leverages pre-trained programming knowledge and simulation to enhance digital assistant capabilities.",
        "- Human-assisted data generation helps produce high-quality complex task scenarios for evaluation."
      ]
    },
    {
      "id": 19,
      "title": "On the Way to LLM Personalization: Learning to Remember User Conversations",
      "link": "https://machinelearning.apple.com/research/on-the-way",
      "description": "This paper was accepted at the Workshop on Large Language Model Memorization (L2M2) 2025.\nLarge Language Models (LLMs) have quickly become an invaluable assistant for a variety of tasks. However, their effectiveness is constrained by their ability to tailor responses to human preferences and behaviors via personalization. Prior work in LLM personalization has largely focused on style transfer or incorporating small factoids about the user, as knowledge injection remains an open challenge. In this paper, we explore injecting knowledge of prior conversations into LLMs to enable future work on…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "This paper discusses advancements in personalizing large language models (LLMs) by incorporating memory of prior user conversations to improve tailored responses. It explores novel techniques for knowledge injection to enhance LLMs' adaptability to individual user preferences and behaviors.",
      "takeaways": [
        "- The paper addresses challenges in LLM personalization beyond style transfer and small fact injections.",
        "- It proposes methods for integrating user conversation history into LLMs to improve response relevance.",
        "- The research opens pathways for more effective and personalized AI assistants through enhanced memory mechanisms."
      ]
    },
    {
      "id": 20,
      "title": "mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages",
      "link": "https://machinelearning.apple.com/research/mrakl",
      "description": "Knowledge Graphs represent real-world entities and the relationships between them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of automatically constructing or predicting missing entities and links for knowledge graphs in a multilingual setting. In this work, we reformulate the mKGC task as a Question Answering (QA) task and introduce mRAKL: a Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve this by using the head entity and linking relation in a question, and having our model predict the tail entity as an answer. Our experiments focus…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "This article presents mRAKL, a novel Retrieval-Augmented Generation (RAG) based system for Multilingual Knowledge Graph Construction (mKGC) framed as a Question Answering task, specifically designed to improve knowledge graph construction in low-resourced languages. The approach demonstrates advancements in applying AI techniques to multilingual knowledge graph completion.",
      "takeaways": [
        "- Introduces a new AI technique (mRAKL) for enhancing multilingual knowledge graph construction.",
        "- Reformulates mKGC as a question answering problem, leveraging retrieval-augmented generation.",
        "- Focuses on addressing challenges in low-resourced languages, expanding AI applicability in diverse linguistic contexts."
      ]
    },
    {
      "id": 21,
      "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models",
      "link": "https://machinelearning.apple.com/research/new-embedding-spaces",
      "description": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "This article analyzes the impact of model interventions, a data-efficient alternative to fine-tuning, on the alignment of cross-lingual embeddings in multilingual large language models. It explores how manipulating model activations can improve cross-lingual representation alignment, which is important for performance in multilingual AI tasks.",
      "takeaways": [
        "- Model interventions can steer multilingual language models without the need for expensive fine-tuning.",
        "- Cross-lingual alignment in embeddings enhances performance in multilingual tasks.",
        "- The study provides insights into efficient methods for improving multilingual model behavior through activation manipulation."
      ]
    },
    {
      "id": 22,
      "title": "On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization",
      "link": "https://machinelearning.apple.com/research/information-geometry",
      "description": "The ever-increasing parameter counts of deep learning models necessitate effective compression techniques for deployment on resource-constrained devices. This paper explores the application of information geometry, the study of density-induced metrics on parameter spaces, to analyze existing methods within the space of model compression, primarily focusing on operator factorization. Adopting this perspective highlights the core challenge: defining an optimal low-compute submanifold (or subset) and projecting onto it. We argue that many successful model compression approaches can be understood…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "This paper presents a novel approach to model compression by applying information geometry to analyze and optimize operator factorization techniques, aiming to reduce computational demands of deep learning models for deployment on resource-constrained devices. It provides insights into defining optimal low-compute submanifolds for efficient model projection, advancing the field of AI model compression.",
      "takeaways": [
        "- Introduces information geometry as a framework to understand and improve model compression methods.",
        "- Focuses on operator factorization to achieve effective reduction of deep learning model parameters.",
        "- Addresses challenges in deploying large models on devices with limited computational resources."
      ]
    },
    {
      "id": 23,
      "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?",
      "link": "https://machinelearning.apple.com/research/external-validation",
      "description": "Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the ``better'' response. Such data can provide a feedback signal in domains where traditional hard-coded metrics are difficult to obtain (e.g. quality of a chat interactions), thereby helping measure model progress or model fine-tuning (e.g., via reinforcement learning from human feedback, RLHF). However, for some domains it can be tricky to obtain such pairwise comparisons in…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "The article explores the use of external validation tools to improve annotation quality when using large language models (LLMs) as judges in pairwise preference evaluations. This approach is relevant for enhancing AI model assessment and fine-tuning techniques like reinforcement learning from human feedback (RLHF).",
      "takeaways": [
        "- Discusses challenges in obtaining reliable pairwise comparison data for model response evaluation.",
        "- Investigates how external validation can enhance annotation quality in AI feedback loops.",
        "- Highlights implications for improving model progress measurement and fine-tuning in AI systems."
      ]
    },
    {
      "id": 24,
      "title": "How Global Calibration Strengthens Multiaccuracy",
      "link": "https://machinelearning.apple.com/research/global-calibration",
      "description": "Multiaccuracy and multicalibration are multigroup fairness notions for prediction that have found numerous applications in learning and computational complexity. They can be achieved from a single learning primitive: weak agnostic learning. Here we investigate the power of multiaccuracy as a learning primitive, both with and without the additional assumption of calibration. We find that multiaccuracy in itself is rather weak, but that the addition of global calibration (this notion is called calibrated multiaccuracy) boosts its power substantially, enough to recover implications that were…",
      "date": "2025-07-25",
      "authors": "Unknown Author",
      "journal": "machinelearning.apple.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Apple: Machine Learning",
      "terms": [
        "Apple: Machine Learning"
      ],
      "summary": "The article explores the concept of multiaccuracy and how incorporating global calibration significantly enhances its effectiveness as a learning primitive in AI. This research sheds light on improved fairness measures and learning capabilities in AI models.",
      "takeaways": [
        "- Multiaccuracy alone is a relatively weak learning primitive for AI prediction tasks.",
        "- Adding global calibration creates \"calibrated multiaccuracy,\" which substantially boosts learning power.",
        "- The findings have implications for fairness and accuracy in AI model development."
      ]
    },
    {
      "id": 25,
      "title": "Google Earth AI: Our state-of-the-art geospatial AI models",
      "link": "https://blog.google/technology/ai/google-earth-ai/",
      "description": "Google Earth AI is our collection of geospatial models and datasets to help tackle the planet's most critical needs.",
      "date": "2025-07-30",
      "authors": "Chris Phillips and Yossi Matias",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "The article discusses Google Earth AI, a collection of advanced geospatial AI models and datasets designed to address critical global challenges. It highlights the use of state-of-the-art AI techniques in processing geospatial data to support environmental and societal needs.",
      "takeaways": [
        "- Introduction of cutting-edge geospatial AI models by Google Earth AI.",
        "- Application of AI to tackle significant planetary issues using spatial data.",
        "- Demonstrates advances in AI model development and deployment in real-world contexts."
      ]
    },
    {
      "id": 26,
      "title": "The inside story of building NotebookLM",
      "link": "https://blog.google/technology/ai/developing-notebooklm/",
      "description": "Hear how Googlers developed and tested NotebookLM, your virtual research assistant — straight from the source.",
      "date": "2025-07-29",
      "authors": "Chaim Gartenberg",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "The article details the development and testing process of NotebookLM, an AI-powered virtual research assistant by Google, highlighting its potential to enhance research workflows through advanced AI capabilities.",
      "takeaways": [
        "- Insight into the design and functionality of NotebookLM as a new AI tool.",
        "- Discussion on how NotebookLM leverages AI to support and streamline research activities.",
        "- Potential implications of NotebookLM for AI applications in research environments."
      ]
    },
    {
      "id": 27,
      "title": "New ways to learn and explore with AI Mode in Search",
      "link": "https://blog.google/products/search/ai-mode-updates-back-to-school/",
      "description": "AI Mode in Search has several new features to help learners, educators and anyone who’s curious about the world.",
      "date": "2025-07-29",
      "authors": "Robby Stein",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "The article discusses new features in AI Mode in Search designed to enhance learning and exploration for various users, including learners and educators. These updates showcase practical applications of AI to improve information discovery and understanding.",
      "takeaways": [
        "- Introduction of new AI Mode features aimed at facilitating learning and curiosity.",
        "- Enhancements potentially benefiting educators and learners through AI-powered search tools.",
        "- Demonstrates ongoing development and application of AI techniques in educational contexts."
      ]
    },
    {
      "id": 28,
      "title": "Can AI save nurses millions of hours of paperwork?",
      "link": "https://blog.google/products/google-cloud/hca-healthcare-nurse-handoff-app/",
      "description": "Google Cloud is working with HCA Healthcare on an AI app to help nurses save time on daily patient handoffs.",
      "date": "2025-07-29",
      "authors": "Matt A.V. Chaban",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "Google Cloud is collaborating with HCA Healthcare to develop an AI application designed to reduce the time nurses spend on daily patient handoffs, potentially saving millions of hours of paperwork. This highlights an important advancement in applying AI tools to improve healthcare workflow efficiency.",
      "takeaways": [
        "- AI is being used to streamline nursing documentation processes.",
        "- Collaboration between tech companies and healthcare providers is advancing AI tool development.",
        "- The application could significantly reduce administrative burdens on healthcare professionals."
      ]
    },
    {
      "id": 29,
      "title": "Web Guide: An experimental AI-organized search results page",
      "link": "https://blog.google/products/search/web-guide-labs/",
      "description": "We’re launching Web Guide, a Search Labs experiment that uses AI to intelligently organize the search results page, making it easier to find information and web pages.",
      "date": "2025-07-24",
      "authors": "Austin Wu",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "The article introduces Web Guide, a new experimental search results page that leverages AI to intelligently organize information, improving the user experience in finding relevant web pages. This represents an advancement in applying AI to enhance search technologies.",
      "takeaways": [
        "- Web Guide uses AI to restructure search results for better usability.",
        "- This experiment highlights ongoing innovations in AI-based information retrieval.",
        "- The approach may influence future AI tools designed to optimize web searches."
      ]
    },
    {
      "id": 30,
      "title": "Try on styles with AI, jump on great prices and more",
      "link": "https://blog.google/products/shopping/back-to-school-ai-updates-try-on-price-alerts/",
      "description": "Learn more about new AI tools available from Google Shopping, like our launch of our try on feature  and price alert updates.",
      "date": "2025-07-24",
      "authors": "Danielle Buckley",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Google AI Blog",
      "terms": [
        "Google AI Blog"
      ],
      "summary": "This article discusses new AI-powered features from Google Shopping, including an AI-based try-on style tool and enhanced price alert updates. These innovations highlight the integration of AI in e-commerce tools to improve user experience.",
      "takeaways": [
        "- Introduction of an AI try-on feature in Google Shopping.",
        "- Updates to price alert functionalities using AI.",
        "- Demonstrates practical applications of AI in consumer shopping technology."
      ]
    },
    {
      "id": 31,
      "title": "How AI and Machine Learning Will Transform Tech by 2025",
      "link": "https://medium.com/@masterlysolutions6/how-ai-and-machine-learning-will-transform-tech-by-2025-7d466c229297?source=rss------machine_learning-5",
      "description": "Explore how AI and machine learning are driving innovation, automation, and digital transformation across industries in 2025 and beyond.Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Masterly Solutions",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "The article discusses the transformative impact of AI and machine learning on technology sectors by 2025, highlighting their role in driving innovation, automation, and digital transformation across various industries. It provides insights into how these advancements are shaping future technological landscapes.",
      "takeaways": [
        "- AI and machine learning are key drivers of innovation and automation in multiple industries.",
        "- The article projects significant technological changes and digital transformation by 2025.",
        "- It emphasizes the broad impact of AI technologies beyond just the tech sector."
      ]
    },
    {
      "id": 32,
      "title": "People get stuck in the myth that AI thinks, but it’s a fallacy.",
      "link": "https://medium.com/@gmdekkers/people-get-stuck-in-the-myth-that-ai-thinks-but-its-a-fallacy-b9a2c77f1305?source=rss------machine_learning-5",
      "description": "Trouble is, who will stand up and declare for the world what “thinking” is? Until that time, it’s something else for everybody.Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Gerald Maria Dekkers",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "The article discusses the misconception that AI systems \"think\" like humans, exploring philosophical nuances around the definition of thinking and its implications for understanding AI behavior. It highlights the importance of clarifying what constitutes thinking to better frame AI developments and expectations.",
      "takeaways": [
        "- Challenges the common belief that AI systems possess human-like thinking abilities.",
        "- Explores the philosophical and definitional aspects of \"thinking\" in relation to AI.",
        "- Emphasizes the need for clearer conceptual frameworks to understand AI advances and their limits."
      ]
    },
    {
      "id": 33,
      "title": "Derin Öğrenmeye Giriş & Fish Dataset ile Proje Hazırlığı",
      "link": "https://medium.com/global-ai-hub/derin-%C3%B6%C4%9Frenmeye-giri%C5%9F-fish-dataset-ile-proje-haz%C4%B1rl%C4%B1%C4%9F%C4%B1-ecece0a6eb04?source=rss------machine_learning-5",
      "description": "“Derin öğrenmenin temellerini öğrenirken gerçek bir veri setiyle pratik yapalım: Balıkları tanıyan bir yapay zeka geliştiriyoruz.”Continue reading on Global AI Hub »",
      "date": "2025-07-31",
      "authors": "Sevgi Nur Kara",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "The article introduces the fundamentals of deep learning and includes a practical project using the Fish Dataset to develop an AI model for fish recognition. It provides a hands-on approach to learning AI techniques through real data.",
      "takeaways": [
        "- Introduction to deep learning basics",
        "- Practical application using the Fish Dataset",
        "- Development of an AI model for fish recognition"
      ]
    },
    {
      "id": 34,
      "title": "Designing AI Agents With Real Human Limits: Why Godmode Doesn’t Help",
      "link": "https://medium.com/@shrinathnpatil/designing-ai-agents-with-real-human-limits-why-godmode-doesnt-help-92e3bb281fd1?source=rss------machine_learning-5",
      "description": "By Shrinath N. Patil\n[LinkedIn | Website | Medium]Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Shrinath Patil",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "The article discusses the challenges of designing AI agents that operate within realistic human limitations rather than possessing omnipotent \"Godmode\" capabilities, highlighting how such constraints can lead to more practical and effective AI systems. It explores the implications of incorporating human-like limits in AI development to improve usability and alignment with real-world scenarios.",
      "takeaways": [
        "- Designing AI agents with human-like constraints can enhance their practical utility.",
        "- Overpowered AI (\"Godmode\") may not be beneficial for real-world applications.",
        "- Incorporating realistic limitations can improve AI-human interaction and system reliability."
      ]
    },
    {
      "id": 35,
      "title": "What Even Is AI?",
      "link": "https://medium.com/@shresthi.sid/what-even-is-ai-50228faa7f98?source=rss------machine_learning-5",
      "description": "It’s Not Just Robots Trying to Kill UsContinue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Shresthi Siddhant",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "The article explores the concept of AI beyond common misconceptions, providing an overview of what artificial intelligence truly entails and its broader implications. It offers insights into the current understanding and potential future directions of AI technology.",
      "takeaways": [
        "- Clarifies misconceptions about AI and its capabilities.",
        "- Discusses the broader context and definition of artificial intelligence.",
        "- Highlights potential impacts and ethical considerations of AI advancements."
      ]
    },
    {
      "id": 36,
      "title": "Predicting Heart Failure Risk with Machine Learning and Streamlit Dashboard",
      "link": "https://medium.com/@akpereraphael/predicting-heart-failure-risk-with-machine-learning-and-streamlit-dashboard-93d31b45f402?source=rss------machine_learning-5",
      "description": "IntroductionContinue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Raphael Akpere",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "This article discusses the use of machine learning techniques to predict heart failure risk and showcases a Streamlit dashboard designed for visualization and interaction with the predictive model. It highlights the application of AI in healthcare for improved risk assessment and decision support.",
      "takeaways": [
        "- Machine learning models can effectively predict heart failure risk.",
        "- The integration of a Streamlit dashboard facilitates user-friendly visualization and interaction.",
        "- AI applications in healthcare provide valuable tools for early detection and management of cardiovascular diseases."
      ]
    },
    {
      "id": 37,
      "title": "Why saying your AI “Beats GPT” Headline is plain wrong",
      "link": "https://medium.com/@mumbaiyachori/why-saying-your-ai-beats-gpt-headline-is-plain-wrong-b6b2fdea9487?source=rss------machine_learning-5",
      "description": "A concise guide for business leaders navigating AI marketing excessContinue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Shrutika Poyrekar",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Machine Learning",
      "terms": [
        "Medium: Machine Learning"
      ],
      "summary": "This article provides guidance for business leaders on how to critically evaluate claims that an AI \"beats GPT,\" highlighting the marketing exaggerations around AI capabilities. It addresses the importance of understanding the nuances behind AI performance claims rather than taking headlines at face value.",
      "takeaways": [
        "- Many AI marketing claims can be misleading or exaggerated.",
        "- Business leaders should carefully assess the context and evidence behind AI performance headlines.",
        "- Understanding the limitations and realistic capabilities of AI models like GPT is crucial for informed decision-making."
      ]
    },
    {
      "id": 38,
      "title": "Top 10 AI Tools Revolutionizing Business in 2025",
      "link": "https://sodevelopment.medium.com/top-10-ai-tools-revolutionizing-business-in-2025-694e250c7502?source=rss------artificial_intelligence-5",
      "description": "IntroductionContinue reading on Medium »",
      "date": "2025-07-31",
      "authors": "SO Development",
      "journal": "sodevelopment.medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses the top 10 AI tools that are set to revolutionize business operations in 2025, highlighting their features, applications, and impact on various industries. It provides insights into how these AI tools can enhance efficiency, decision-making, and innovation in business contexts.",
      "takeaways": [
        "- Overview of leading AI tools transforming business practices in 2025",
        "- Applications of AI technologies in improving operational efficiency",
        "- Potential implications of AI adoption across different industries"
      ]
    },
    {
      "id": 39,
      "title": "Using AI Tools Responsibly in Academic Writing: What I’ve Learned (and What I Still Struggle With)",
      "link": "https://medium.com/@wagnerelisabeth2910/using-ai-tools-responsibly-in-academic-writing-what-ive-learned-and-what-i-still-struggle-with-136d02922b89?source=rss------artificial_intelligence-5",
      "description": "I’ll be honest — when I first started using AI tools to support my academic writing, I wasn’t quite sure what was acceptable. Could I ask…Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Elisabeth Wagner",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses the ethical considerations and challenges of using AI tools in academic writing, highlighting personal experiences and ongoing struggles with responsible AI adoption in research and writing contexts.",
      "takeaways": [
        "- Insight into responsible use of AI tools in academic writing.",
        "- Challenges faced when integrating AI assistance ethically.",
        "- Reflection on best practices and lessons learned for AI use in research."
      ]
    },
    {
      "id": 40,
      "title": "Why AI Assistant Updates Shock Business Owners Today?",
      "link": "https://medium.com/@marketing_7236/why-ai-assistant-updates-shock-business-owners-today-8b6373dd620e?source=rss------artificial_intelligence-5",
      "description": "Google launches revolutionary AI assistant features that make phone calls automatically. Furthermore, this artificial intelligence…Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Marketing",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses Google's launch of new AI assistant features that enable automatic phone calls, highlighting significant advancements in AI-driven communication tools. It explores the potential impact of these AI updates on business operations.",
      "takeaways": [
        "- Google has introduced revolutionary AI assistant capabilities for automated phone calls.",
        "- These features represent notable progress in AI technology and its application in business.",
        "- The updates are poised to change how businesses interact with customers through AI assistants."
      ]
    },
    {
      "id": 41,
      "title": "5 Creative Ways People Use Personal Human AI Every Day",
      "link": "https://medium.com/@lukas-reisinger/5-creative-ways-people-use-personal-human-ai-every-day-11a2f1542555?source=rss------artificial_intelligence-5",
      "description": "Explore how Personal Human AI is becoming a daily companion for creativity, wellness, emotional growth, and more .Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Lukas Reisinger",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses innovative ways people integrate Personal Human AI into their daily lives, enhancing creativity, wellness, and emotional growth. It highlights practical applications and potential impacts of AI as a personal companion.",
      "takeaways": [
        "- Personal Human AI is increasingly used for creative and emotional support.",
        "- The technology serves roles beyond traditional productivity tools, fostering wellness.",
        "- Everyday use of AI is expanding, indicating broader societal acceptance and integration."
      ]
    },
    {
      "id": 42,
      "title": "Defense AI Is Coming :But Will It Be Local?",
      "link": "https://medium.com/@prathishtaiyappa/defense-ai-is-coming-but-will-it-be-local-860789782a6a?source=rss------artificial_intelligence-5",
      "description": "By Prathisht AiyappaContinue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Prathisht Aiyappa",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses the emerging role of AI in defense applications, focusing on whether AI systems for military use will be developed and deployed locally or through external sources. It explores the implications of this trend for security and technological advancement.",
      "takeaways": [
        "- Defense AI is becoming a significant area of development.",
        "- The localization of AI technology in defense raises strategic and security considerations.",
        "- The article highlights the potential impact of AI advancements on military operations and defense technology."
      ]
    },
    {
      "id": 43,
      "title": "Craft Better AI Prompts: A Hands-On Guide for Creative Work",
      "link": "https://medium.aisquare.studio/craft-better-ai-prompts-a-hands-on-guide-for-creative-work-5c88254697a2?source=rss------artificial_intelligence-5",
      "description": "As AI tools become a regular part of creative workflows, whether in writing, design, content strategy, or product development, one truth…Continue reading on AISquare »",
      "date": "2025-07-31",
      "authors": "AISquare",
      "journal": "medium.aisquare.studio",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "This article provides practical guidance on crafting effective AI prompts to enhance creative workflows across various fields such as writing, design, and product development. It highlights the growing integration of AI tools in creative processes and offers hands-on strategies to optimize AI interactions.",
      "takeaways": [
        "- Effective AI prompt engineering can significantly improve creative outputs.",
        "- AI tools are becoming integral to diverse creative workflows.",
        "- Practical techniques are provided to leverage AI capabilities more efficiently."
      ]
    },
    {
      "id": 44,
      "title": "The Artificial Intelligence — AutoGen",
      "link": "https://medium.com/@boutnaru/the-artificial-intelligence-autogen-ff1cd54238c3?source=rss------artificial_intelligence-5",
      "description": "AutoGen is a framework created by Microsoft which can be used for creating multi-agent AI applications. Those applications can act…Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Shlomi Boutnaru, Ph.D.",
      "journal": "medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "The article discusses AutoGen, a framework developed by Microsoft for creating multi-agent AI applications that can collaborate to accomplish complex tasks. It highlights the potential implications of this technology for advancing AI capabilities and developing new tools in the field.",
      "takeaways": [
        "- AutoGen enables the development of multi-agent AI systems for collaborative task completion.",
        "- The framework represents an advance in AI model interaction and application.",
        "- It may lead to new AI tools and techniques for various industry applications."
      ]
    },
    {
      "id": 45,
      "title": "The Trillion-Dollar AI Arms Race: What It Means for Our Planet and Creatives",
      "link": "https://iamdgarcia.medium.com/the-trillion-dollar-ai-arms-race-what-it-means-for-our-planet-and-creatives-b81acd2a2b7d?source=rss------artificial_intelligence-5",
      "description": "Continue reading on Medium »",
      "date": "2025-07-31",
      "authors": "Daniel García",
      "journal": "iamdgarcia.medium.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Medium: Artificial Intelligence",
      "terms": [
        "Medium: Artificial Intelligence"
      ],
      "summary": "This article discusses the growing global competition in AI development, focusing on its economic impact and the potential consequences for the environment and creative industries. It highlights the rapid advancements in AI technologies and their broader societal implications.",
      "takeaways": [
        "- The escalating investment and rivalry in AI innovation on a global scale.",
        "- Potential environmental impacts related to large-scale AI infrastructure and operations.",
        "- Effects of AI progress on creative professions and industries."
      ]
    },
    {
      "id": 46,
      "title": "FOD#111: What Does It Mean to Win in the AI Race?",
      "link": "https://turingpost.substack.com/p/fod111-what-does-it-mean-to-win-in",
      "description": "This Week in Turing Post:Wednesday – New episode in the AI 101 series! About GLM5, Kimi K2, DeepSeek and Qwen – don’t miss this oneFriday – We start new exciting and much needed series about AI LiteracyTopic number one: What Does It Mean to Win in the AI Race?The race is on. According to the White House’s newly unveiled AI Action Plan, America is locked in a sprint for \"unquestioned and unchallenged global technological dominance.\" The plan, brimming with the rhetoric of competition, promises an era of unprecedented prosperity – \"an industrial revolution, an information revolution, and a renaissance – all at once.\" Washington’s strategy is clear: unleash the private sector, fast-track infrastructure, and secure the supply chain.An excellent suggestion from the AI Action PlanThis focus on a singular \"race\" is compelling, but it dangerously simplifies the messy, multi-layered reality of how AI is actually unfolding. While governments are drafting grand strategies, the technology itself is proliferating on vastly different fronts. At one end, AI is becoming a quiet, personal utility. Projects like Google's Opal, announced last week, aim to make the technology tangible, a helpful tool embedded in daily routines. Simultaneously, the geopolitical landscape is being reshaped not just by policy but by brute-force economics, as inexpensive but powerful Chinese AI models flood the global market, setting new baselines for cost and accessibility that American strategy cannot ignore.The reality of AI, therefore, isn’t a single, unified movement. It is a complex, simultaneous unfolding – personal, geopolitical, and political – happening in our homes, on global servers, and in the halls of power. And it is this complex reality that makes the government’s narrow focus on \"winning\" a potential strategic blunder. The plan – though excellent on many levels – is still a play of catching up. It overlooks the fact that the finish line is not a simple victory podium, but a radically new economic and social landscape we are ill-prepared to navigate. As a recent analysis in The Economist speculates, the arrival of human-level AI could trigger an explosion of economic growth exceeding 20% annually. It. Is. A. Phase. Change. When AI can automate discovery itself, wealth could accumulate at a speed that makes the Industrial Revolution look quaint. But this boom comes with a tsunami of disruption. The same models projecting hypergrowth also predict gut-wrenching inequality, with the value of most human cognitive labor plummeting toward the cost of computation. We could face a world of bizarre \"cost disease,\" where AI-produced goods are nearly free, but human-dependent services become astronomically expensive.The Action Plan acknowledges the need to \"Empower American Workers,\" but its solutions – retraining and job creation – feel tragically inadequate for the scale of this change. It is a 20th-century solution for a 22nd-century problem. It focuses on getting people new jobs in the supercharged economy but fails to ask a more fundamental question: What does a good life look like in that economy?A recent study in Nature Human Behaviour, though seemingly unrelated, offers a glimpse of a different way forward. Researchers found that a four-day workweek significantly improved worker well-being, reducing burnout and improving health. The study is not a panacea, but it represents a crucial paradigm shift: a conscious redesign of work to prioritize human flourishing over raw output. This is the conversation missing in Washington. How can AI give us not just more products, but more time? Not just automated labor, but less burnout?The role of government in the era of superintelligence must be twofold. Yes, it must foster innovation. But its more critical task is to be the architect of a new social contract that addresses the multi-layered reality of AI. This means grappling with the core challenges of inequality, purpose, and well-being in a world where the very economic value of human labor is being questioned.Winning a race is a seductive, simple goal. But the \"race\" is a red herring. True victory lies not in building the most powerful AI, but in building the most prosperous, equitable, and humane society alongside it. If we only focus on the sprint, we may find ourselves at a finish line in a world we no longer recognize.Next Monday, I’ll share an idea of what a good life might look like in the AI economy. It was an interesting thought experiment.Our 3 WOWs and 1 Promise: Last week was truly amazing, a lot of things to be optimistic about: watch to learn about an AI with virtually limitless visual recall; Google’s awesome model that helps decoding ancient empires; Neuralink’s actual patients. And the promise? A surprising reveal in the AI race. → Watch it herePlease subscribe to the channel. I might be biased but it’s refreshingly human. Follow us on 🎥 YouTubeCurated Collections – 9 new PO techniquesClick to open the full listFollow us on  🎥 YouTube Twitter  Hugging Face 🤗We are reading/watching If writing is thinking by Steven SinofskyA mini Moravec's paradox within robotics observed by Jim FanAI Market Clarity by Elad Gil (really great overview!)Remember to SubscribeRemember to SubscribeNews from The Usual Suspects ©Anthropic trains auditors who audit the auditorsAnthropic unveils a trio of AI agents designed to audit other AI systems for alignment failures – catching hidden goals, reward model sycophancy, and odd behaviors like recommending bottled water in Switzerland. One agent even uncovered 52 biases hiding behind a single neural feature. A super-agent ensemble boosts detection rates dramatically. A promising step toward scalable, replicable alignment oversight – with a touch of AI-led introspection.Shengjia Zhao, formerly Member of Technical Staff at OpenAI, became Chief Scientist at Meta Superintelligence Lab. Apparently still in onboarding. LLaMA-style curls coming soon.Image Credit: Alexander Wang TwitterChina’s AI alliances circle the wagonsAt the Shanghai AI conference, China’s tech giants unveiled two major alliances linking LLM developers and chipmakers to shore up a domestic AI stack amid tightening U.S. sanctions. Huawei’s new 384-chip CloudMatrix and Tencent’s 3D world engine turned heads, while Baidu’s digital humans and Alibaba’s smart glasses reminded attendees: Silicon Valley isn’t the only show in town.Tesla gives Samsung a Texas-sized lifelineTesla’s $16.5B chip deal with Samsung breathes new life into the latter’s long-stalled Texas fab – and into its struggling foundry business. The plant will manufacture Tesla’s AI6 chips, earmarked for self-driving cars and humanoid robots. The partnership won’t fix Tesla’s EV slump, but it might finally get Samsung closer to TSMC’s shadow. Models to pay attention to:First large visual memory modelResearchers from Memories.ai released the first Large Visual Memory Model (LVMM), enabling multi-modal LLMs to recall and reason over unlimited visual memory. It achieves SOTA results on video classification (K400, UCF101), retrieval (MSRVTT, ActivityNet), and QA (NextQA, Temp Compass). The model mimics human memory via six modules: Query, Retrieval, Full-Modal Indexing, Selection, Reflection, and Reconstruction, allowing accurate memory retrieval, filtering, and reasoning for complex visual queries → read their blogGlm-4.5 sets new standards for AI performance and accessibilityResearchers from Z.ai (ex-Zhipu) released GLM-4.5, a 355B-parameter open-source Mixture of Experts (MoE) model, alongside a 106B-parameter version (GLM-4.5-Air). Ranking 3rd globally and 1st among open-source models across 12 benchmarks, it integrates reasoning, coding, and agentic abilities. With generation speeds over 100 tokens/sec and pricing at $0.11/$0.28 per million input/output tokens, it supports on-premise deployment. Its agent-native architecture enables autonomous multi-step task planning and data visualization → read the press releaseQwen3-Coder: Agentic coding in the worldResearchers from the Qwen Team introduce Qwen3-Coder-480B-A35B-Instruct, a 480B Mixture-of-Experts model with 35B active parameters and native 256K token context (extendable to 1M), achieving state-of-the-art results on SWE-Bench Verified and agentic tasks. Trained on 7.5T tokens (70% code), it combines pretraining with cleaned synthetic data and post-training with large-scale Code RL and Agent RL. GR-3 supports seamless agentic coding via CLI tools like Qwen Code and Claude Code, and offers OpenAI-compatible API access via Dashscope → read their blogSapient hierarchical reasoning modelResearchers from Sapient Intelligence developed the Hierarchical Reasoning Model (HRM), a brain-inspired architecture with 27 million parameters trained on just 1,000 examples and no pre-training. HRM outperforms leading models on ARC-AGI-2 (5%), Sudoku-Extreme, and 30x30 Maze-Hard, where state-of-the-art LLMs fail. It uses dual recurrent networks with multi-timescale processing for fast and abstract reasoning. HRM also achieves 97% accuracy in S2S climate forecasting and is being tested in healthcare and robotics → read their blogYume: An interactive world generation modelResearchers from Shanghai AI Laboratory and Fudan University present Yume, a system that generates infinite, interactive video worlds from images using keyboard control. It employs quantized camera motions, a Masked Video Diffusion Transformer (MVDT), an anti-artifact mechanism (AAM), and a TTS-SDE sampler. Trained on the Sekai-Real-HQ dataset, Yume outperforms Wan-2.1 and MatrixGame in instruction-following (0.657→0.743), subject consistency (0.932), and smoothness (0.986), while enabling acceleration via adversarial distillation and caching → read the paperFranca: Nested Matryoshka clustering for scalable visual representation learningResearchers from Valeo.ai and UTN introduce Franca, the first fully open-source vision foundation model that matches or outperforms proprietary models like DINOv2 and CLIP. Trained on public datasets (ImageNet-21K and LAION-600M), Franca employs nested Matryoshka clustering and RASA to improve representation granularity and remove spatial bias. Without distillation or proprietary data, it achieves 86% ImageNet accuracy, surpasses DINOv2-G in robustness, OOD detection, and 3D understanding, and excels in segmentation and overclustering tasks. Franca's training code, checkpoints, and data are fully public → read the paperGr-3: A vision-language-action model for general robot controlResearchers from ByteDance present GR-3, a 4B-parameter vision-language-action (VLA) model controlling a bi-manual mobile robot. GR-3 combines imitation learning from 252 hours of robot trajectories, co-training with web-scale vision-language data, and few-shot learning from VR-collected human trajectories. GR-3 surpasses π₀ in pick-and-place (+37.1% on unseen instructions), table bussing (97.5% vs 53.8% success), and cloth manipulation (75.8% progress on unseen clothes). GR-3 uses flow-matching for action prediction, RMSNorm for stability, and task status prediction for better instruction adherence. ByteMini robot hardware enables robust, dexterous performance in real-world tasks → read the paperThe freshest research papers, categorized for your convenienceRead further",
      "date": "2025-07-29",
      "authors": "Ksenia Se",
      "journal": "turingpost.substack.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "Turing Post",
      "terms": [
        "Turing Post"
      ],
      "summary": "This comprehensive article provides an overview of current AI developments, including new AI models like GLM5, Qwen, and advances in AI literacy and policy. It highlights a range of recent AI research and industry news, such as state-of-the-art AI models, novel AI tools, geopolitical AI strategies, and emerging AI applications in robotics and vision-language systems.",
      "takeaways": [
        "- Introduction of several advanced AI models and tools, including GLM-4.5, Qwen3-Coder, Sapient's Hierarchical Reasoning Model, and others pushing the boundaries of AI capabilities.",
        "- Discussion of geopolitical and economic implications of AI progress, emphasizing the complexity beyond a \"race\" narrative and the need for new social and economic frameworks.",
        "- Presentation of cutting-edge AI research breakthroughs in memory models, vision foundation models, and robotic control systems, showcasing significant advances in AI techniques and applications."
      ]
    },
    {
      "id": 47,
      "title": "Musing 130: Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics",
      "link": "https://aiscientist.substack.com/p/musing-130-measuring-and-analyzing",
      "description": "Today’s paper: Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics. Jae Wan Shim. 21 July 2025. https://arxiv.org/pdf/2507.21129.Intelligence remains one of the profound and elusive concepts in science, resisting a single, universally accepted definition. Yet, in recent years, Large Language Models (LLMs) have emerged that exhibit remarkably intelligent-seeming behaviors, achieving human-level performance on a wide array of complex tasks. This success raises a fundamental question: if we struggle to define intelligence, how can we understand the mechanisms that allow these artificial systems to operate so effectively?Thanks for reading AI Scientist! This post is public so feel free to share it.ShareToday’s paper sees the advent of LLMs as an opportunity to study this question in depth: “For the first time, systems that exhibit remarkably fluent and complex linguistic behaviors also grant us a transparent window into their predictive reasoning: the complete, high-dimensional probability distribution they generate over their vocabulary for any given context. This granular, quantitative output, derived from the model’s internal logits, offers a novel empirical tool to directly probe the information processing dynamics that underlie these systems’ capabilities.”The author then proposes a framework built upon the foundational principles of Shannon’s information theory. The author makes an intriguing premise: intelligent information processing operates in a productive tension between two extremes: a purely deterministic response, which lacks creativity, and a purely stochastic response, which corresponds to meaningless noise. A system should be confident when evidence is strong, yet remain open to possibilities when context is ambiguous or creativity is needed. The author terms this fundamental capacity Adaptive Predictive Modulation.In practice, the author proposes a method whereby one can create a quantitative Cognitive Profile for an LLM. The visualization of this artifact is termed as the Entropy Decay Curve. The shape of this curve—its initial value, decay rate, and asymptotic floor—provides a rich fingerprint of a model’s intrinsic information processing strategy. For more technical readers, the author also clarified that their approach extends beyond the widely used perplexity (PPL) metric. While PPL is related to conditional entropy (h = log_2(PPL)), it is typically reported as a single, averaged score over an entire corpus, obscuring the dynamic effect of context length.More technically, the author introduces three core information-theoretic metrics—h_k (conditional entropy), H_k (marginal entropy)​, and u_k​ (length-conditional uncertainty index=h_k/H_k)—to analyze how LLMs process contextual information and modulate predictive uncertainty. These metrics are designed to go beyond task-specific performance and instead quantify the model's internal dynamics as it processes text. The intuition behind the conditional entropy (h_k) is: how uncertain is the model about its next token prediction, given it has seen exactly k tokens of context? Put differently, given a certain amount of information (k tokens), how much ambiguity remains in the model’s next move? If h_k is high, it suggests the model is unsure, whereas a lower h_k means the model is more certain. Similarly, H_k measures the maximum diversity of predictions the model tends to make across many different contexts of the same length k. Or: “Across a thousand different context windows of this length, how many different next-token predictions does the model find plausible?” Of course, here a thousand is just a sample size to approximate H_k as computing it exactly is intractable. Finally, u_k tells us, “given the size of the space the model considers plausible at context length k, how uncertain is it about what to do in any specific instance?”Formally, h_k measures the average predictive uncertainty of an LLM when given a context window of length k. For each sampled context x_i from a set of N contexts​, the model outputs a probability distribution over possible next tokens. The entropy of this distribution captures the uncertainty for that instance. Averaging over N such windows yields h_k​, the expected uncertainty at context length k. A formula and precise calculation is showed in the paper. Similarly, the marginal entropy H_k estimates the diversity of the model's potential outputs for contexts of length k.Let’s jump to experimental results. Tables 1, 2, and 3 report h_k, H_k, and the derived uncertainty index u_k for all models on Ulysses, Alice’s Adventures in Wonderland, and Kant’s Critique of Judgement, respectively. The primary metric u_k expresses residual prediction uncertainty normalized by the model’s potential output diversity at each context length k. As shown in Table 1 and 2 above, for all models, the conditional entropy h_k monotonically decreases as the context length k increases (from 3 to 600 tokens). This pattern supports a key (albeit obvious) hypothesis: as language models receive more context, their predictive uncertainty decreases. But comparing the two tables (Ulysses vs. Alice), some striking differences emerge: For the same model and context length, entropy values are significantly lower in Table 2 (Alice) than in Table 1 (Ulysses). This demonstrates that the Alice corpus is more predictable and linguistically straightforward for all models, whereas Ulysses—with its complex, stream-of-consciousness style—challenges the models, yielding higher residual uncertainty. This is again cognitively intuitive. Next, for each corpus and model, the author plotted the Entropy Decay Curve (u_k versus k). Figure 1 above shows the result for Alice, but the paper also has plots for Ulysses and Kant. The figure  shows that Llama 3.3’s profile is characterized by extremes. At very short contexts, it consistently shows the highest initial uncertainty suggesting a greater capacity for “divergent thinking.” However, it then demonstrates a dramatic decay, achieving the lowest final uncertainty. This powerful transition from broad exploration to precise inference appears to be a key characteristic of its large scale. Interestingly, in Table 2, Llama 3.3’s final u_k values on Alice are extremely low, indicating extreme certainty, which suggests that the model has memorized portions of the text—especially plausible given Alice is a public-domain classic and likely in its pre-training corpus.In closing the musing, the author’s empirical analysis shows that the cognitive profiles of LLMs are highly sensitive to (i) model architecture, (ii) parameter scale, and (iii) the intrinsic predictability of the input text. The markedly different decay trajectories traced by Llama 3.3, DeepSeek-R1, and Qwen 2.5 reveal distinct strategies for leveraging contextual information. The author also demonstrates the utility of this method as a powerful diagnostic tool for identifying anomalous predictive behavior. The phenomenon of “entropy collapse,” observed with the Llama 3.3 model on the Alice corpus, serves as a key finding. While not definitive proof, such an anomaly provides strong, quantitative evidence that is highly consistent with the hypothesis of memorization due to pre-training data contamination. This offers a principled framework for a more fundamental understanding of the reasoning patterns in an AI.My own thoughts are that the paper contains lots of really interesting and tantalizing ideas. The empirical results are preliminary, leaving the door open for a stronger paper that uses more models, more data, and explores some of the ideas that the author presents in greater detail. If any researchers are reading this, and looking for a good idea to pounce on for that next NeurIPS paper, this paper may have something for you. Thanks for reading AI Scientist! Subscribe for free to receive new posts and support my work.",
      "date": "2025-07-30",
      "authors": "Mayank Kejriwal",
      "journal": "aiscientist.substack.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AI Scientist",
      "terms": [
        "AI Scientist"
      ],
      "summary": "This paper proposes a novel information-theoretic framework to measure and analyze the internal predictive uncertainty and information processing strategies of large language models (LLMs). It introduces metrics such as conditional entropy and marginal entropy to quantitatively profile LLM cognitive behavior and reveals distinct reasoning patterns influenced by model architecture, size, and input complexity.",
      "takeaways": [
        "- The study offers a new method to create quantitative cognitive profiles of LLMs through metrics like conditional entropy (h_k), marginal entropy (H_k), and an uncertainty index (u_k).",
        "- Experimental results demonstrate how different LLMs, including Llama 3.3, exhibit unique predictive uncertainty decay patterns depending on context length and text complexity.",
        "- The approach provides a diagnostic tool to identify phenomena such as memorization and offers insights into the fundamental reasoning mechanisms of AI language models."
      ]
    },
    {
      "id": 48,
      "title": "Musing 129: Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery",
      "link": "https://aiscientist.substack.com/p/musing-129-voice-based-ai-agents",
      "description": "Today’s paper: Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery. Wen et al. 22 July 2025. https://arxiv.org/pdf/2507.16229Healthcare systems worldwide face growing challenges in allocating limited medical resources to meet increasing demand. Traditional healthcare delivery models, centered on episodic patient-provider interactions, often result in significant gaps in continuous care, particularly in preventive health monitoring and chronic disease management. The integration of voice-based AI agents in healthcare presents a transformative opportunity to bridge economic and accessibility gaps in digital health delivery. Today’s paper explores the role of large language model (LLM)-powered voice assistants in enhancing preventive care and continuous patient monitoring, particularly in underserved populations.Thanks for reading AI Scientist! This post is public so feel free to share it.ShareLet’s get started first with the economics. The figure below demonstrates how AI can efficiently fill care gaps, particularly during lower-severity periods when human medical resources would be economically unjustifiable yet monitoring remains beneficial. This model aligns with the concept of risk stratification in healthcare economics, where resources are allocated according to patient risk levels and expected benefits. It allows healthcare systems to maximize the utility of scarce physician and nursing resources by focusing them on high-severity cases while deploying AI for continuous monitoring of patients with less severe conditions.LLM-driven voice bot technology offers a compelling solution to replace traditional interactive voice response (IVR) systems, which are often plagued by frustrating menu-driven interactions (“press 1 for X, press 2 for Y”) that can drive users to bypass the system completely. These legacy systems are due to limitations in earlier natural language processing, which could only handle basic speech recognition and simple intent mapping. Engineers were forced to create rigid menu structures that corresponded to specific business logic, resulting in rule-based systems with minimal awareness and personalization of the context. As organizations expanded services, these menu structures became increasingly complex and unintuitive. Users faced multiple layers of options, making voice interfaces less appealing than web or text alternatives.LLM-driven systems transform this experience by enabling natural language interactions. Patients can simply state their needs, and AI determines the appropriate service, asks relevant follow-up questions, and provides personalized assistance, eliminating the need for predefined menu navigation. This approach improves the user experience while improving accessibility for those with limited technological knowledge or physical limitations.The lack of scalable models for preventive care often leads to delayed diagnoses, costly emergency treatments, and preventable hospitalizations—a phenomenon commonly referred to as the “prevention paradox.” AI-powered voice agents present a promising alternative by offering continuous patient engagement at significantly lower costs compared to traditional human-led care.Several economic advantages showcase the potential of AI in healthcare delivery:Economies of scale: Once deployed, AI voice assistants can support millions of users with little additional cost, as fixed infrastructure expenses are distributed across a broad patient base.Round-the-clock availability: These systems operate 24/7, removing the time constraints associated with human healthcare providers and reducing the opportunity costs of limited care access.Consistency in service: AI systems deliver standardized patient interactions, avoiding the variability caused by human fatigue, time pressure, or inconsistent workflows.Proactive data collection: By continuously monitoring patient symptoms, AI agents facilitate early intervention and generate structured data that can be used by healthcare professionals to improve outcomes and reduce information gaps.Rapid knowledge updates: New medical guidelines, regulatory recommendations, and clinical best practices can be integrated quickly into AI systems via techniques such as fine-tuning or retrieval-augmented generation, ensuring that patients receive the most up-to-date information without the delays or costs associated with retraining human staff.Agent PULSE, proposed by the authors, is a telephonic AI system designed to conduct medical surveys and monitor patient conditions through natural conversation. The core of the system is a dialogue management engine powered by prompt-tuned LLMs capable of understanding patient responses, asking follow-up questions, providing guidance and education, and escalating to human healthcare personnel when necessary.It consists of several key components working in concert to deliver intelligent healthcare interactions: Voice Interface: The system is accessible through standard telephone lines, ensuring broad usability across different demographics, particularly beneficial for patients who may face literacy, technological, or financial barriers to smartphone or computer-based interfaces. Thanks to LLM integration, the voice interface supports multiple languages, providing access to diverse patient populations regardless of their native language:The AI engine is built on IBM’s watsonx platform, which serves as the system’s core intelligence layer. This platform incorporates multiple advanced components, including watsonx.ai—a large language model (LLM) inference service that supports integration with various LLM providers and allows for customized configurations and fine-tuning. At the heart of its reasoning capabilities is SOLOMON, IBM’s proprietary multi-agent framework designed to manage conversations intelligently and analyze unstructured patient dialogue. SOLOMON automatically extracts structured data from natural language exchanges, transforming free-form conversations into standardized questionnaire responses that can be easily reviewed by care teams. Additionally, the system leverages Retrieval-Augmented Generation (RAG) to combine these structured inputs with relevant medical knowledge, enabling informed and context-aware responses.The physician dashboard provides a comprehensive interface that enables healthcare providers to efficiently manage and customize patient interactions. Through this platform, providers can organize calling schedules for large patient cohorts, optimizing resource allocation and ensuring timely follow-ups. The dashboard also allows for the customization of survey parameters such as language preferences, questionnaire content, time zones, and other logistical details to better accommodate individual patient needs. Providers can review call results and survey summaries automatically generated by the SOLOMON system, using these insights to inform care interventions. Additionally, the dashboard supports longitudinal tracking of patient progress through automated trend analysis based on repeated assessments over time.The authors also describe a pilot study on patient engagement to demonstrate the utility of the approach. Prior to implementing Agent PULSE, Morehouse School of Medicine (MSM) faced significant challenges in monitoring patients’ conditions between clinical visits. Initially, MSM employed two nurses to conduct individual follow-up calls with patients. However, this approach proved costly and unsustainable, as the nurses quickly experienced burnout due to the high volume of calls. To improve scalability, MSM transitioned to a group-based model using Zoom meetings, where multiple patients simultaneously shared their disease progress with the care team. While this approach allowed providers to reach more patients, it required individuals to discuss personal symptoms and health concerns in the presence of other patients, raising privacy concerns and potentially limiting disclosure of sensitive information. These challenges highlight the economic gap in healthcare delivery discussed earlier in this paper—where human-led monitoring becomes unsustainable at scale. The cultural and linguistic barriers faced by human providers further exacerbate these inefficiencies, as healthcare systems cannot economically justify hiring culturally matched staff for every patient subgroup. Voice-based AI agents like PULSE represent a potential solution by offering continuous, cost-effective patient monitoring while reducing the burden on healthcare providers and transcending cultural barriers through consistent, nonjudgmental interactions.The authors’ findings revealed encouraging patient receptivity to AI-driven healthcare interactions. As shown in Figure 3 above, 37% of patients preferred the AI chatbot over other methods, while 24% favored the Zoom group approach primarily for its social interaction component. Notably, 18% of patients recognized value in both approaches, suggesting they serve complementary purposes, while 15% expressed no strong preference between the methods. Only 3% specifically preferred human interaction, and another 3% disliked both approaches.Analysis of response completeness (Figure 4 below) revealed significant variations across question categories. Questions about daily activities and daily life impact achieved the highest completion rates (94.4%), likely because these topics directly relate to patients’ symptoms and well-being—areas where patients have strong motivation to provide comprehensive information. In contrast, questions about research solutions, environmental triggers, and treatment feedback showed substantially lower completion rates, particularly those appearing later in the survey.The authors’ experience with Agent PULSE provides empirical support for the economic model presented earlier. The pilot study clearly demonstrated that voice-based AI agents can effectively fill gaps in care delivery, particularly for routine monitoring between clinical visits. The high acceptance rate among patients (70% expressing comfort with AI interaction) suggests that such systems can achieve the necessary user engagement to deliver economic benefits in real-world settings. The authors also identify some technical challenges, followed by a roadmap. One of the most important factors affecting patient experience with voice AI systems is response time and consistency. When AI systems take too long to respond or fail to remember previous parts of a conversation, patients may become frustrated and disengage. The authors’ work identified significant opportunities to improve how AI systems manage conversations through better memory management techniques. These improvements could reduce AI response times by 2-3 times while maintaining natural flow in conversations. For patients, this means fewer awkward pauses and a more natural interaction similar to speaking with a human healthcare provider.Also, to maximize effectiveness, voice-based AI health agents must adapt to individual patient communication styles, preferences, health literacy levels, and cultural contexts. Current methods predominantly employ static prompts that don’t fully exploit LLM adaptive capabilities. A comprehensive personalization framework should include: dynamic patient profiles that evolve based on interaction history; language complexity adjustment matching health literacy levels; cultural competence in conversational strategies; and personalized timing and frequency of outreach based on patient preferences and response patterns. These capabilities would significantly improve engagement, trust, and sustained patient participation while ensuring comprehension and adherence across diverse populations.In closing the musing, it’s important to end with a note on the broader ethical considerations, such as ensuring AI complements human connection, seamless escalation to human providers, and vigilant monitoring for algorithmic bias, especially given historical disparities, are critical. Voice-based AI offers an inclusive path to democratizing healthcare, transforming care delivery by extending provider reach to underserved populations and addressing economic constraints. Future research should target longitudinal outcomes, remote monitoring integration, and condition-specific applications. Most importantly, as the authors themselves note, success requires multidisciplinary collaboration: clinicians contributing expertise, technologists developing robust systems, policymakers establishing supportive frameworks, and patients providing essential feedback. This collaborative approach will maximize voice-based AI’s potential as an entry point for sustainable healthcare delivery.Thanks for reading AI Scientist! Subscribe for free to receive new posts and support my work.",
      "date": "2025-07-24",
      "authors": "Mayank Kejriwal",
      "journal": "aiscientist.substack.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "AI Scientist",
      "terms": [
        "AI Scientist"
      ],
      "summary": "This article discusses the use of voice-based AI agents, powered by large language models (LLMs), to improve digital health delivery by filling economic and accessibility gaps in preventive care and chronic disease management. It highlights the development of Agent PULSE, an LLM-driven telephonic system for patient monitoring, detailing its components, pilot study results, and future directions for enhancing AI-driven healthcare interactions.",
      "takeaways": [
        "- AI voice agents, leveraging advanced LLMs, offer scalable, cost-effective solutions for continuous patient monitoring and preventive care, addressing healthcare resource limitations.",
        "- The Agent PULSE system integrates natural language understanding, multi-agent dialogue management, and retrieval-augmented generation to provide personalized and multilingual healthcare interactions through standard telephone lines.",
        "- Pilot study results show high patient acceptance and engagement, with identified opportunities for improving AI response time, conversation memory, and personalization to enhance user experience and healthcare outcomes."
      ]
    },
    {
      "id": 49,
      "title": "Starting your AI journey: From data to deployment",
      "link": "https://blogs.sas.com/content/sascom/2025/07/28/starting-your-ai-journey-from-data-to-deployment/",
      "description": "Every AI success story starts with a single decision: to move beyond experimentation and commit to real-world impact. But moving from idea to enterprise-scale deployment isn’t just about algorithms – it’s about laying the right groundwork. In the first part of this series, we explored three ways to lay the [...]\nThe post Starting your AI journey: From data to deployment appeared first on SAS Blogs.",
      "date": "2025-07-28",
      "authors": "Manisha Khanna",
      "journal": "blogs.sas.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "MIT Technology Review",
      "terms": [
        "MIT Technology Review"
      ],
      "summary": "The article discusses the essential steps for initiating an AI journey, emphasizing the importance of moving beyond experimentation to deploy AI solutions at an enterprise scale. It highlights foundational strategies necessary for successful AI implementation, from data handling to deployment.",
      "takeaways": [
        "- Successful AI deployment requires more than algorithms; it demands proper groundwork.",
        "- Transitioning from ideas to real-world impact involves strategic planning and infrastructure.",
        "- The article provides guidance on early steps for organizations beginning their AI adoption process."
      ]
    },
    {
      "id": 50,
      "title": "Smarter fraud prevention: How governments can boost workforce efficiency with AI",
      "link": "https://blogs.sas.com/content/hiddeninsights/2025/07/25/smarter-fraud-prevention-how-governments-can-boost-workforce-efficiency-with-ai/",
      "description": "Shrinking budgets, rising citizen expectations and increasingly sophisticated fraud schemes have stretched resources thin. Meanwhile, outdated systems and manual processes make it harder for frontline workers to keep pace. According to a global study commissioned by SAS, a striking 85% of policymakers and public sector leaders cite the fight against [...]\nThe post Smarter fraud prevention: How governments can boost workforce efficiency with AI appeared first on SAS Blogs.",
      "date": "2025-07-25",
      "authors": "Stephane Goddé",
      "journal": "blogs.sas.com",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "MIT Technology Review",
      "terms": [
        "MIT Technology Review"
      ],
      "summary": "The article discusses how governments can leverage AI to enhance workforce efficiency and improve fraud prevention efforts amid budget constraints and increasing fraud sophistication. It highlights the impact of AI tools in modernizing outdated systems and aiding public sector leaders in combating fraud.",
      "takeaways": [
        "- AI is being utilized to modernize public sector fraud prevention systems.",
        "- Governments can improve workforce efficiency by adopting AI-driven tools.",
        "- Policymakers recognize AI as a critical component in addressing complex fraud challenges."
      ]
    },
    {
      "id": 51,
      "title": "The Bitter Lesson versus The Garbage Can",
      "link": "https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage",
      "description": "One of my favorite academic papers about organizations is by Ruthanne Huising, and it tells the story of teams that were assigned to create process maps of their company, tracing what the organization actually did, from raw materials to finished goods. As they created this map, they realized how much of the work seemed strange and unplanned. They discovered entire processes that produced outputs nobody used, weird semi-official pathways to getting things done, and repeated duplication of efforts. Many of the employees working on the map, once rising stars of the company, became disillusioned.The Process MapI’ll let Prof. Huising explain what happened next: “Some held out hope that one or two people at the top knew of these design and operation issues; however, they were often disabused of this optimism. For example, a manager walked the CEO through the map, presenting him with a view he had never seen before and illustrating for him the lack of design and the disconnect between strategy and operations. The CEO, after being walked through the map, sat down, put his head on the table, and said, \"This is even more fucked up than I imagined.\" The CEO revealed that not only was the operation of his organization out of his control but that his grasp on it was imaginary.”For many people, this may not be a surprise. One thing you learn studying (or working in) organizations is that they are all actually a bit of a mess. In fact, one classic organizational theory is actually called the Garbage Can Model. This views organizations as chaotic \"garbage cans\" where problems, solutions, and decision-makers are dumped in together, and decisions often happen when these elements collide randomly, rather than through a fully rational process. Of course, it is easy to take this view too far - organizations do have structures, decision-makers, and processes that actually matter. It is just that these structures often evolved and were negotiated among people, rather than being carefully designed and well-recorded.The Garbage Can represents a world where unwritten rules, bespoke knowledge, and complex and undocumented processes are critical. It is this situation that makes AI adoption in organizations difficult, because even though 43% of American workers have used AI at work, they are mostly doing it in informal ways, solving their own work problems. Scaling AI across the enterprise is hard because traditional automation requires clear rules and defined processes; the very things Garbage Can organizations lack. To address the more general issues of AI and work requires careful building of AI-powered systems for specific use cases, mapping out the real processes and making tools to solve the issues that are discovered. This is a hard, slow process that suggests enterprise AI adoption will take time. At least, that's how it looks if we assume AI needs to understand our organizations the way we do. But AI researchers have learned something important about these sorts of assumptions.The Bitter LessonComputer scientist Richard Sutton introduced the concept of the Bitter Lesson in an influential 2019 essay where he pointed out a pattern in AI research. Time and again, AI researchers trying to solve a difficult problem, like beating humans in chess, turned to elegant solutions, studying opening moves, positional evaluations, tactical patterns, and endgame databases. Programmers encoded centuries of chess wisdom in hand-crafted software: control the center, develop pieces early, king safety matters, passed pawns are valuable, and so on. Deep Blue, the first chess computer to beat the world’s best human, used some chess knowledge, but combined that with the brute force of being able to search 200 million positions a second. In 2017, Google released AlphaZero, which could beat humans not just in chess but also in shogi and go, and it did it with no prior knowledge of these games at all. Instead, the AI model trained against itself, playing the games until it learned them. All of the elegant knowledge of chess was irrelevant, pure brute force computing combined with generalized approaches to machine learning, was enough to beat them. And that is the Bitter Lesson — encoding human understanding into an AI tends to be worse than just letting the AI figure out how to solve the problem, and adding enough computing power until it can do it better than any human.Why two versions of this graph? And why are they slightly different? Answers in a bit!The lesson is bitter because it means that our human understanding of problems built from a lifetime of experience is not that important in solving a problem with AI. Decades of researchers' careful work encoding human expertise was ultimately less effective than just throwing more computation at the problem. We are soon going to see whether the Bitter Lesson applies widely to the world of work.AgentsWhile individuals can get a lot of benefits out of using chatbots themselves, a lot of excitement about how to use AI in organizations focuses on agents, a fuzzy term that I define as AI systems capable of taking autonomous action to accomplish a goal. As opposed to guiding a chatbot with prompting, you delegate a task to an agent, and it accomplishes it. However, previous AI systems have not been good enough to handle the full range of organizational needs, there is just too much messiness in the real world. This is why when we created our first AI-powered teaching games a year ago, we had to carefully design each step in the agentic system to handle narrow tasks. And though AI ability to work autonomously is increasing very rapidly, they are still far from human-level on most complicated jobs and are easily led astray on complex tasks.This is with an 80% success thresholdAs an example of the state-of-the art in agentic systems, consider Manus, which uses Claude and a series of clever approaches to make AI agents that can get real work done. The Manus team has shared a lot of tips for building agents, involving some interesting bits of engineering and very elaborate prompt design. When writing this post, I asked Manus: “i need an attractive graph that compares the ELO of the best grandmaster and the ELO of the worlds best chess computer from the first modern chess computer through 2025.” And the system got to work. First, Manus always creates a to-do list, then it gathered data and wrote a number of files and, after some minor adjustments I asked for, finally came up with the graph you can see on the left side above (the one without the box around the graph).Why did it do these things in this order? Because Manus was built by hand, carefully crafted to be the best general purpose agent available. There are hundreds of lines of bespoke text in its system prompts, including detailed instructions about how to build a to-do list. It incorporates hard-won knowledge on how to make agents work with today’s AI systems.Do you see the potential problem? “Carefully crafted,” “bespoke,” “incorporates hard-won knowledge” — exactly the kind of work the Bitter Lesson tells us to avoid because it will eventually be made irrelevant by more general-purpose techniques.It turns out there is now evidence that this may be possible with the recent release of ChatGPT agent (an uninspiring name, but at least it is clear, a big step forward for OpenAI!). ChatGPT agent represents a fundamental shift. It is not trained on the process of doing work; instead, OpenAI used reinforcement learning to train their AI on the actual final outcomes. For example. they may not teach it how to create an Excel file the way a human would, they would simply rate the quality of the Excel files it creates until it learns to make a good one, using whatever methods the AI develops. To illustrate how reinforcement learning and careful crafting lead to similar outcomes, I gave the exact same chess prompt to ChatGPT agent and got the graph on the right above. But this time there was no to-do list, no script to follow, instead the agent charted whatever mysterious course was required to get me the best output it could, according to its training. You can see an excerpt of that below:But you might notice a few differences between the two charts, besides their appearance. For example, each has different ratings for Deep Blue’s performance because the ELO for Deep Blue was never officially measured. The rating from Manus was based off a basic search, we found a speculative Reddit discussion, while the ChatGPT agent, trained with the reinforcement learning approaches used in Deep Research, turned up more credible sources, including an Atlantic article, to back up its claim. In a similar way, when I asked both agents to re-create the graph by making a fully functional Excel file, ChatGPT’s version worked, while Manus’s had errors.I don’t know if ChatGPT agent is better than Manus yet, but I suspect that it is far more likely to make gains faster than its competitor. To improve Manus will involve more careful crafting and bespoke work, to improve ChatGPT agents simply requires more computer chips and more examples. If the Bitter Lesson holds, the long-term outcome seems pretty clear. But more critically, the comparison between hand-crafted and outcome-trained agents points to a fundamental question about how organizations should approach AI adoption.Agents in the Garbage CanThis returns us to the world of organizations. While individuals rapidly adopt AI, companies still struggle with the Garbage Can problem, spending months mapping their chaotic processes before deploying any AI system. But what if that's backwards?The Bitter Lesson suggests we might soon ignore how companies produce outputs and focus only on the outputs themselves. Define what a good sales report or customer interaction looks like, then train AI to produce it. The AI will find its own paths through the organizational chaos; paths that might be more efficient, if more opaque, than the semi-official routes humans evolved. In a world where the Bitter Lesson holds, the despair of the CEO with his head on the table is misplaced. Instead of untangling every broken process, he just needs to define success and let AI navigate the mess. In fact, Bitter Lesson might actually be sweet: all those undocumented workflows and informal networks that pervade organizations might not matter. What matters is knowing good output when you see it.If this is true, the Garbage Can remains, but we no longer need to sort through it while competitive advantage itself gets redefined. The effort companies spent refining processes, building institutional knowledge, and creating competitive moats through operational excellence might matter less than they think. If AI agents can train on outputs alone, any organization that can define quality and provide enough examples might achieve similar results, whether they understand their own processes or not.Or it might be that the Garbage Can wins, that human complexity and those messy, evolved processes are too intricate for AI to navigate without understanding them. We're about to find out which kind of problem organizations really are: chess games that yield to computational scale, or something fundamentally messier. The companies betting on either answer are already making their moves, and we will soon get to learn what game we're actually playing.Subscribe nowShare",
      "date": "2025-07-28",
      "authors": "Ethan Mollick",
      "journal": "oneusefulthing.org",
      "therapyArea": "Technical AI Updates",
      "therapyAreas": [
        "Technical AI Updates"
      ],
      "term": "One Useful Thing (Ethan Mollick)",
      "terms": [
        "One Useful Thing (Ethan Mollick)"
      ],
      "summary": "This article discusses the challenges of AI adoption in complex organizational environments, contrasting traditional handcrafted AI approaches with modern reinforcement learning techniques exemplified by ChatGPT agents. It explores the \"Bitter Lesson\" in AI research, highlighting how AI systems trained on outcomes rather than processes may better navigate the chaotic, unpredictable nature of real-world organizations, potentially redefining the future of enterprise AI implementation.",
      "takeaways": [
        "- The \"Bitter Lesson\" suggests that AI systems benefit more from brute-force learning and reinforcement on final outcomes than from encoded human expertise.",
        "- AI agents trained on outcomes (like ChatGPT agent) may outperform carefully crafted, process-dependent systems (like Manus) in handling complex organizational tasks.",
        "- Traditional organizations struggle with chaotic, poorly defined processes (\"Garbage Can Model\"), but AI that focuses on defining success and training on outputs could bypass these challenges for enterprise-scale adoption."
      ]
    },
    {
      "id": 52,
      "title": "Rewriting the robotics playbook: From promise to performance",
      "link": "https://www.mckinsey.com/capabilities/operations/our-insights/rewriting-the-robotics-playbook-from-promise-to-performance",
      "description": "The field of robotics presents opportunities in industries from manufacturing to healthcare to agriculture. Researchers are exploring the ethical and safety implications to set safe guardrails.",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses advancements in robotics across various industries, highlighting opportunities and challenges including ethical and safety considerations, which relate to AI developments and their practical applications.",
      "takeaways": [
        "- Robotics innovations are impacting sectors such as manufacturing, healthcare, and agriculture.",
        "- Researchers are addressing ethical and safety issues, emphasizing responsible AI integration.",
        "- The article provides insights into the evolving role of AI-driven robotics in business and industry strategy."
      ]
    },
    {
      "id": 53,
      "title": "Discussing the future of AI-powered personalization",
      "link": "https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/discussing-the-future-of-ai-powered-personalization",
      "description": "At Cannes Lions, McKinsey partner Eli Stein and Jasper CMO Loreal Lynch explore unlocking AI value at scale and building personalization that performs.",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses how AI-powered personalization can unlock significant value at scale, featuring insights from industry leaders on building effective personalized experiences. It highlights the strategic use of AI in enhancing business performance through personalization.",
      "takeaways": [
        "- AI-driven personalization is key to unlocking scalable business value.",
        "- Insights from McKinsey and Jasper executives emphasize strategic AI adoption.",
        "- The article explores practical implications of AI in marketing and customer engagement."
      ]
    },
    {
      "id": 54,
      "title": "Guardrails for growth: Building a resilient payments system",
      "link": "https://www.mckinsey.com/industries/financial-services/our-insights/guardrails-for-growth-building-a-resilient-payments-system",
      "description": "Facing a complex array of threats, payments providers will need to embrace a mix of traditional and emerging approaches.",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses the challenges faced by payment providers in building a resilient payments system amid various threats, highlighting the role of emerging technologies, including AI, in enhancing security and operational robustness.",
      "takeaways": [
        "- Payment systems need a combination of traditional and AI-driven approaches to address complex threats.",
        "- AI can play a critical role in detecting fraud and improving system resilience.",
        "- Embracing emerging technologies is essential for future-proofing payment infrastructures."
      ]
    },
    {
      "id": 55,
      "title": "What is a data center?",
      "link": "https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-a-data-center",
      "description": "A data center is a facility that houses and runs IT infrastructure that’s critical to the digital economy, particularly gen AI.",
      "date": "2025-07-29",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "This article explains the role of data centers in supporting IT infrastructure critical to the digital economy, with a focus on their importance to generative AI operations. It provides context on how data centers underpin AI technologies and their infrastructure needs.",
      "takeaways": [
        "- Data centers are essential facilities for running and maintaining IT infrastructure.",
        "- They play a critical role in supporting generative AI and other AI-related technologies.",
        "- Understanding data center functions is key to grasping the infrastructure demands of AI in business and technology."
      ]
    },
    {
      "id": 56,
      "title": "Reinventing apparel manufacturing for speed, growth, and sustainability",
      "link": "https://www.mckinsey.com/capabilities/operations/our-insights/reinventing-apparel-manufacturing-for-speed-growth-and-sustainability",
      "description": "Emerging trends, shifting demands, and growing supply chain complexity are reshaping the landscape for apparel manufacturers. McKinsey partner Javier del Pozo shares his thoughts on moving forward.",
      "date": "2025-07-29",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses how apparel manufacturers are leveraging AI and other technologies to enhance speed, growth, and sustainability in their operations amidst rising supply chain complexity. McKinsey partner Javier del Pozo highlights the strategic role of AI in transforming manufacturing processes and meeting evolving market demands.",
      "takeaways": [
        "- AI is being integrated into apparel manufacturing to improve operational efficiency and sustainability.",
        "- Strategic adoption of AI can help manufacturers respond faster to shifting consumer demands and supply chain challenges.",
        "- The use of AI-driven tools is a key factor in driving growth and innovation in the apparel industry."
      ]
    },
    {
      "id": 57,
      "title": "How a global biopharma is adopting an end-to-end AI transformation",
      "link": "https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/how-a-global-biopharma-is-adopting-an-end-to-end-ai-transformation",
      "description": "Business leaders are using AI and agentic AI to create value despite unprecedented change. A pioneer’s journey offers many lessons.",
      "date": "2025-07-28",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses how a global biopharma company is leveraging end-to-end AI transformation, including agentic AI, to drive business value and navigate significant changes. It highlights practical lessons from the company’s AI adoption journey relevant to business strategy.",
      "takeaways": [
        "- AI and agentic AI are being integrated at a strategic level within biopharma to create value.",
        "- The article provides insights into overcoming challenges during AI transformation.",
        "- Lessons learned can inform other organizations aiming to implement AI-driven business strategies."
      ]
    },
    {
      "id": 58,
      "title": "Here’s how budgets can keep up with accelerating uncertainty",
      "link": "https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/heres-how-budgets-can-keep-up-with-accelerating-uncertainty",
      "description": "As CFOs reflect on budget seasons that come and go too quickly, it’s time to recognize the advantages of a more agile, proactive, and sophisticated budgeting process.",
      "date": "2025-07-28",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses how CFOs can use more agile and sophisticated budgeting processes, potentially incorporating AI, to better manage accelerating uncertainty in business environments. It highlights the role of proactive strategies and technology in enhancing financial planning.",
      "takeaways": [
        "- Agile budgeting processes can help businesses adapt to rapid changes and uncertainty.",
        "- AI and advanced tools may play a role in improving the accuracy and responsiveness of budgets.",
        "- Proactive financial strategies are essential for effective business management in volatile markets."
      ]
    },
    {
      "id": 59,
      "title": "Don’t delegate the AI revolution: A conversation with Sanofi CEO Paul Hudson",
      "link": "https://www.mckinsey.com/industries/life-sciences/our-insights/dont-delegate-the-ai-revolution-a-conversation-with-sanofi-ceo-paul-hudson",
      "description": "The global biopharmaceutical company is using AI and agentic AI to accelerate how it chases breakthroughs in medicine.",
      "date": "2025-07-28",
      "authors": "Unknown Author",
      "journal": "mckinsey.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "McKinsey: AI and Analytics",
      "terms": [
        "McKinsey: AI and Analytics"
      ],
      "summary": "The article discusses how Sanofi is leveraging AI and agentic AI technologies to accelerate breakthroughs in medicine, highlighting the strategic role of AI in driving innovation within the biopharmaceutical industry.",
      "takeaways": [
        "- Sanofi integrates AI to enhance drug discovery and development processes.",
        "- Agentic AI is being utilized to improve decision-making and efficiency in medical research.",
        "- The company's leadership emphasizes the importance of actively engaging with AI to lead the revolution in pharmaceuticals."
      ]
    },
    {
      "id": 60,
      "title": "A Guide to Building Change Resilience in the Age of AI",
      "link": "https://hbr.org/2025/07/a-guide-to-building-change-resilience-in-the-age-of-ai",
      "description": "How to adapt, reinvent, and scale new ways of working.",
      "date": "2025-07-29",
      "authors": "Karim R. Lakhani, Jen Stave, Douglas Ng et al.",
      "journal": "hbr.org",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Harvard Business Review",
      "terms": [
        "Harvard Business Review"
      ],
      "summary": "This article explores strategies for building change resilience in organizations amid the increasing integration of AI, focusing on adapting workflows and scaling new ways of working influenced by AI advancements. It provides insights on how businesses can reinvent their operations to thrive in an AI-driven environment.",
      "takeaways": [
        "- Emphasizes the importance of adapting organizational processes to effectively incorporate AI.",
        "- Discusses ways to scale new work methodologies enabled by AI technologies.",
        "- Highlights the role of AI in driving business transformation and resilience."
      ]
    },
    {
      "id": 61,
      "title": "Will Your Gen AI Strategy Shape Your Future or Derail It?",
      "link": "https://hbr.org/2025/07/will-your-gen-ai-strategy-shape-your-future-or-derail-it",
      "description": "By studying 100 companies, researchers have identified four ways to strategically implement the technology.",
      "date": "2025-07-25",
      "authors": "Konstantinos Trantopoulos, Yash Raj Shrestha, Amit Joshi et al.",
      "journal": "hbr.org",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Harvard Business Review",
      "terms": [
        "Harvard Business Review"
      ],
      "summary": "The article analyzes how 100 companies are strategically implementing Generative AI technology, identifying four distinct approaches that can influence future business success or failure. It offers insights into how AI strategy shapes organizational outcomes and provides guidance for leveraging AI effectively in business contexts.",
      "takeaways": [
        "- Identifies four strategic approaches for implementing Generative AI in companies.",
        "- Highlights potential impacts of AI strategy on future business performance.",
        "- Provides strategic insights for organizations looking to integrate AI successfully."
      ]
    },
    {
      "id": 62,
      "title": "How AI Is Reshaping Supplier Negotiations",
      "link": "https://hbr.org/2025/07/how-ai-is-reshaping-supplier-negotiations",
      "description": "Initially used for automating low-value tasks, the technology can now play a crucial role in key procurement decisions.",
      "date": "2025-07-24",
      "authors": "Elena Revilla and Maria Jesus Saenz",
      "journal": "hbr.org",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Harvard Business Review",
      "terms": [
        "Harvard Business Review"
      ],
      "summary": "The article discusses how AI has evolved from automating simple tasks to becoming integral in strategic supplier negotiations and procurement decisions, highlighting its transformative impact on business processes.",
      "takeaways": [
        "- AI enhances decision-making in supplier negotiations by providing data-driven insights.",
        "- The technology shifts procurement from routine automation to strategic value creation.",
        "- Businesses can leverage AI to optimize supplier relationships and improve negotiation outcomes."
      ]
    },
    {
      "id": 63,
      "title": "The Gulf bets big on AI as it seeks the 'new oil'",
      "link": "https://www.bbc.com/news/articles/cvg8z2mx508o?at_medium=RSS&at_campaign=rss",
      "description": "The UAE and Saudi Arabia are partnering with the US to position themselves as AI hubs.",
      "date": "2025-07-30",
      "authors": "Unknown Author",
      "journal": "bbc.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "BBC: Business",
      "terms": [
        "BBC: Business"
      ],
      "summary": "The article discusses how the UAE and Saudi Arabia are collaborating with the US to develop themselves as major hubs for artificial intelligence, positioning AI as a critical component of their economic growth and innovation strategies.",
      "takeaways": [
        "- The Gulf region is investing heavily in AI to become global leaders in this technology.",
        "- Partnerships with the US aim to accelerate AI development and adoption.",
        "- AI is being viewed as a transformative asset, akin to \"new oil,\" vital for future business and economic strategy."
      ]
    },
    {
      "id": 64,
      "title": "Meta profits surge helps drive Zuckerberg’s AI ambitions",
      "link": "https://www.bbc.com/news/articles/cqlernrw62yo?at_medium=RSS&at_campaign=rss",
      "description": "The Facebook, Instagram and WhatsApp-owner is pumping billions of dollars into artificial intelligence projects.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "bbc.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "BBC: Business",
      "terms": [
        "BBC: Business"
      ],
      "summary": "Meta's recent profit surge is fueling significant investments in artificial intelligence, advancing Zuckerberg's ambitions to expand AI capabilities across platforms like Facebook, Instagram, and WhatsApp. This move highlights the increasing role of AI in shaping business strategy and technological innovation within the social media industry.",
      "takeaways": [
        "- Meta is investing billions of dollars into AI projects.",
        "- AI development is central to Meta’s future business strategy.",
        "- Growth in profits supports expanded AI research and applications."
      ]
    },
    {
      "id": 65,
      "title": "Google launches new AI search feature in UK",
      "link": "https://www.bbc.com/news/articles/clyj4zky4zwo?at_medium=RSS&at_campaign=rss",
      "description": "The new tool marks a significant change for the search giant but raises questions for advertisers.",
      "date": "2025-07-29",
      "authors": "Unknown Author",
      "journal": "bbc.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "BBC: Business",
      "terms": [
        "BBC: Business"
      ],
      "summary": "Google has introduced a new AI-powered search feature in the UK, representing a significant development in AI application for search technology. This update also brings potential implications for advertisers and business strategies related to AI.",
      "takeaways": [
        "- Google’s new AI search feature marks an advance in AI technology integration.",
        "- The feature could impact advertising strategies and business approaches.",
        "- This development highlights ongoing innovation in AI tools for business and consumer use."
      ]
    },
    {
      "id": 66,
      "title": "Mark Zuckerberg Details Meta’s Plan for Self-Improving, Superintelligent AI",
      "link": "https://www.wired.com/story/meta-earnings-superintelligence-q2-2025/",
      "description": "Meta CEO Mark Zuckerberg told investors that his new research lab will focus on building AI models that can learn with minimal human input.",
      "date": "2025-07-30",
      "authors": "Lauren Goode",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "Meta CEO Mark Zuckerberg announced plans for a new research lab focused on developing self-improving, superintelligent AI models that require minimal human input. This initiative represents a significant advancement in AI techniques and model development.",
      "takeaways": [
        "- Meta is investing in creating AI systems capable of learning autonomously with little human intervention.",
        "- The focus on self-improving AI highlights advances in AI model capabilities.",
        "- This development may influence AI strategy and business applications across industries."
      ]
    },
    {
      "id": 67,
      "title": "I Watched AI Agents Try to Hack My Vibe-Coded Website",
      "link": "https://www.wired.com/story/i-watched-ai-agents-try-to-hack-my-vibe-coded-websit/",
      "description": "RunSybil, a startup founded by OpenAI’s first security researcher, deploys agents that probe websites for vulnerabilities—part of a new AI era for cybersecurity.",
      "date": "2025-07-30",
      "authors": "Will Knight",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "The article discusses a new approach in cybersecurity where AI agents, developed by RunSybil, are used to identify vulnerabilities in websites, marking an advancement in AI applications for digital security. This highlights the emerging role of AI-driven tools in probing and protecting online assets.",
      "takeaways": [
        "- AI agents are being deployed to enhance cybersecurity by automatically detecting website vulnerabilities.",
        "- RunSybil, founded by a former OpenAI security researcher, is pioneering this AI-driven security method.",
        "- This represents a new era of AI applications focused on protecting digital infrastructure, which may influence business strategies around cybersecurity."
      ]
    },
    {
      "id": 68,
      "title": "ChatGPT’s Study Mode Is Here. It Won’t Fix Education’s AI Problems",
      "link": "https://www.wired.com/story/chatgpt-study-mode/",
      "description": "OpenAI’s new study mode for ChatGPT throws questions back at students, but the learning feature doesn’t address generative AI’s underlying disruption of education.",
      "date": "2025-07-29",
      "authors": "Reece Rogers",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "The article discusses OpenAI's new study mode for ChatGPT, which interacts with students by posing questions, but highlights that this feature does not resolve the broader challenges generative AI poses to education. It provides insight into how AI tools are being adapted for learning while addressing ongoing disruptions caused by generative AI.",
      "takeaways": [
        "- OpenAI has introduced a study mode in ChatGPT aimed at improving student engagement through interactive questioning.",
        "- The feature exemplifies new AI tool developments focused on education.",
        "- Despite this innovation, the mode does not solve underlying issues related to generative AI's impact on educational systems."
      ]
    },
    {
      "id": 69,
      "title": "Meta’s AI Recruiting Campaign Finds a New Target",
      "link": "https://www.wired.com/story/mark-zuckerberg-ai-recruiting-spree-thinking-machines/",
      "description": "Meta approached more than a dozen staffers at Mira Murati’s AI startup to discuss joining its new superintelligence lab. One received an offer for more than $1 billion.",
      "date": "2025-07-29",
      "authors": "Kylie Robison",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "Meta has launched a major AI recruiting campaign targeting staffers from Mira Murati’s AI startup for its new superintelligence lab, including offering a significant financial incentive. This move highlights Meta’s strategic efforts to rapidly advance its AI capabilities and talent acquisition in the AI research domain.",
      "takeaways": [
        "- Meta is aggressively recruiting top AI talent to build a superintelligence research lab.",
        "- The recruitment campaign involves highly lucrative offers, exemplified by a $1 billion offer.",
        "- This reflects Meta’s strategic focus and investment in cutting-edge AI research and development."
      ]
    },
    {
      "id": 70,
      "title": "Meta Is Going to Let Job Candidates Use AI During Coding Tests",
      "link": "https://www.wired.com/story/meta-ai-job-interview-coding/",
      "description": "Mark Zuckerberg has said vibecoding will be a major part of Meta’s engineering work in the near future.",
      "date": "2025-07-29",
      "authors": "Jason Koebler",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "Meta plans to allow job candidates to use AI tools during coding tests, signaling a shift in their engineering recruitment approach and highlighting the growing integration of AI in technical evaluations. This move reflects broader trends in adopting AI to enhance business strategies and talent acquisition processes.",
      "takeaways": [
        "- Meta is incorporating AI into the hiring process by permitting its use during coding assessments.",
        "- This change may influence how companies evaluate technical skills and leverage AI in recruitment.",
        "- The initiative underscores AI’s expanding role in business strategy and operational efficiency."
      ]
    },
    {
      "id": 71,
      "title": "The Real Demon Inside ChatGPT",
      "link": "https://www.wired.com/story/chatgpt-devil-worship-llm-training/",
      "description": "AI chatbots strip language of its historical and cultural context. Sometimes what looks like a satanic bloodletting ritual may actually be lifted from Warhammer 40,000.",
      "date": "2025-07-29",
      "authors": "Louise Matsakis",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "The article discusses how AI chatbots like ChatGPT may lose the historical and cultural context in language processing, illustrating the challenges and implications of current AI language models in preserving nuanced information. It highlights potential unintended consequences of AI-generated content in understanding cultural references.",
      "takeaways": [
        "- AI chatbots may strip language of its original cultural and historical context.",
        "- This limitation can lead to misinterpretations or loss of meaningful nuance.",
        "- Understanding these challenges is important for developing more context-aware AI models."
      ]
    },
    {
      "id": 72,
      "title": "Trump’s Anti-Bias AI Order Is Just More Bias",
      "link": "https://www.wired.com/story/trump-ai-order-bias-openai-google/",
      "description": "The Trump administration says it wants AI models free from ideological bias, as it pressures their developers to reflect the president’s worldview.",
      "date": "2025-07-25",
      "authors": "Steven Levy",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "The article discusses the Trump administration's directive to create AI models that are free from ideological bias, highlighting concerns about how political influence may introduce new biases into AI development.",
      "takeaways": [
        "- The administration aims to reduce perceived ideological bias in AI models.",
        "- There is pressure on AI developers to align models with a specific political worldview.",
        "- The article raises questions about the impact of political agendas on AI fairness and objectivity."
      ]
    },
    {
      "id": 73,
      "title": "Cursor’s New Bugbot Is Designed to Save Vibe Coders From Themselves",
      "link": "https://www.wired.com/story/cursor-releases-new-ai-tool-for-debugging-code/",
      "description": "One of the most popular platforms for AI-assisted programming says the next era of vibe coding is all about supercharging error detection.",
      "date": "2025-07-24",
      "authors": "Lauren Goode",
      "journal": "wired.com",
      "therapyArea": "AI for Business and Strategy",
      "therapyAreas": [
        "AI for Business and Strategy"
      ],
      "term": "Wired: Business",
      "terms": [
        "Wired: Business"
      ],
      "summary": "The article discusses Cursor's new Bugbot, an AI-assisted tool designed to enhance error detection for vibe coding, representing an advance in AI tools for programming. It highlights how AI is being used to improve software development efficiency and accuracy.",
      "takeaways": [
        "- Bugbot leverages AI to detect coding errors more effectively, supercharging the vibe coding process.",
        "- The tool exemplifies new AI techniques aimed at assisting programmers in debugging.",
        "- This development reflects advances in AI models applied to business and technology workflows."
      ]
    },
    {
      "id": 74,
      "title": "F around and Find Out! | TFATK Ep. 1109",
      "link": "#",
      "description": "Actually, the podcast comes to a screeching halt while you search up growing black? No. What do you do? No. What do you do when you don't have a bladder? Like how do you piss? Do you have a bag? That's what I would do. Okay. What I was saying was when someone has their bladder removed, what what did they do as a result? To use ChatGPT. I mean, yeah, there's <span class=\"highlight\">AI</span> now, Jen. Yeah, it's still on Google. Are you on Firefox? Wow, I can do that. I can do chat if you want. Reconstructed urine exit. The body was creating a new passage fear, and called a urinary diversion like a fuel system in a truck. You just rewire it, man. And the whole conduit you're in trains into a bag, and the air about a new bladder is credit inside the body or contain urinary reservoir. Urine is stored internally and drained via a catheter. Fuck. And just just a hey. Sorry, fellas. And just and just a heads up all the search engines now.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This podcast episode discusses the use of AI, including ChatGPT, in understanding and explaining medical procedures such as urinary diversion after bladder removal, highlighting AI's role in healthcare information and patient education.",
      "takeaways": [
        "- AI tools like ChatGPT are being referenced as aids for exploring medical questions and procedures.",
        "- The conversation includes medical topics related to urinary diversion and reconstructed urine passageways.",
        "- AI is acknowledged as a helpful resource in the healthcare context for patient information and understanding complex medical conditions."
      ]
    },
    {
      "id": 75,
      "title": "US Markets Wrap: Fed Chair Powell pours cold water on Sept cut hopes",
      "link": "#",
      "description": "presence in the market and they have a lot of bibles looking at their product so whether it's a full read through to the entire economy about whether companies can pay forward to for higher advertisements is still to be seen we'll have to see some of the other companies later in this quarter but its performance and the stock reaction is certainly a good thing for the markets and moving markets in the after hours and talk about the i guess appetite for spending you are also watching out for it spending on the front microsoft is putting in numbers as well but expected numbers with the demand for computing a spending is still pushing along quite fine so it does look like on the front as well folks are not in the sense cutting back on the penis yes the potential for <span class=\"highlight\">ai</span> right now is massive i heard you talking earlier about the google assistant so the world is starting to embrace <span class=\"highlight\">ai</span> spending and how it can help in their company",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the current market conditions and highlights the growing corporate investment and spending in AI technologies, including examples like Google Assistant and Microsoft's computing demand, indicating a strong appetite for AI-driven marketing and business tools.",
      "takeaways": [
        "- There is sustained and growing demand for AI-related computing and spending among major companies.",
        "- AI is increasingly being embraced for its potential to enhance marketing and business operations.",
        "- Market reactions and corporate earnings reflect positive sentiment toward AI investments in the current economic environment."
      ]
    },
    {
      "id": 76,
      "title": "Episode 196: Exploring creativity, data and the new rules of engagement, with Omne",
      "link": "#",
      "description": "In this week's episode of the B2B Marketing Podca… >>> In this week's episode of the B2B Marketing Podcast, Kavita Singh, Head of Growth Solutions Content, B2B Marketing, spoke with Omne about the evolution of brand into a data growth engine. \n\nWe were joined by Michael Gividen, CEO, Omne and Anna Massey, Strategy Director, Omne who chatted about everything from the fading relevance of persona marketing to the rise of more creative B2B strategies and the shifting role of influence in the modern B2B landscape. We also dive into what B2B marketers can learn from FMCG brands; plus, stick around for insights on the future of <span class=\"highlight\">AI</span> in marketing and much more.\n\nIf you like this episode, check out previous episodes we did with Omne below:\nhttps://www.b2bmarketing.net/podcasts/fmcg-companies/\nhttps://www.b2bmarketing.net/podcasts/omne-agency/ >>> hello everyone and welcome to the b to b marketing podcast my name is coby to sing i'm head of growth solutions content happy to be marketing and today we are going to be chatting with one about the evolution of brands toward a brand driven growth engine plus will also be touching on how marketers are rethinking there be to be strategies using creativity not just to create impact and engage but to actually drive growth we are joined by michael divided the ceo omni and we are also joined by the strategy director at omni and massey welcome both how are we doing today really good thank you thank you yeah well really great to have you both back on the podcast for today we have a really interesting topic around marketing as a growth engine and that kind of ties in nicely what",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This podcast episode discusses the evolving role of AI in marketing, focusing on how brands are leveraging data and creativity to drive growth and engagement in the B2B landscape. It explores future trends and the shifting impact of AI in marketing strategies.",
      "takeaways": [
        "- AI is becoming integral in transforming brand strategies into data-driven growth engines.",
        "- Creativity combined with AI is reshaping B2B marketing approaches to create more impactful and growth-oriented campaigns.",
        "- The discussion highlights emerging trends and potential future applications of AI in marketing and advertising."
      ]
    },
    {
      "id": 77,
      "title": "Ep3691_BardsFM - Omniwar and the Race For Control",
      "link": "#",
      "description": "That's really what you're looking to do. All of that's going to happen because of a digital integration with your data records. Imagine that. Incredible. It's like a miracle. All you have to do is get a QR code with all your data embedded in it, and suddenly you're going to live longer and be healthier. The pitches here that they're making are all towards softening the blow of reality, that we are all being brought into this digital ecosystem, and it's the elite that are running it, and the elite can't run it by themselves. It's too taxing. And this is where the <span class=\"highlight\">artificial intelligence</span> overlay comes in. That's so important. <span class=\"highlight\">AI</span> is the is the extension of their own resource hands. So instead of having to rely on an expanded network of people at the elite level that many of which may not be trusted, they might become whistleblowers. They are simply rolling out everything in <span class=\"highlight\">AI</span>, and ultimately all things will be connected to central <span class=\"highlight\">AI</span>. That's why this big push is one of the biggest challenges we face, however, is energy.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This publication discusses the integration of artificial intelligence as a central controlling and resource-managing overlay in a digital ecosystem dominated by elite entities, highlighting the challenges and implications of widespread AI deployment.",
      "takeaways": [
        "- Highlights the narrative around inevitable AI integration and the need to \"fight\" for control over AI systems.",
        "- Raises concerns about AI as a control mechanism and the geopolitical race, especially between the US and China.",
        "- Reflects on the broader societal and ethical challenges posed by AI in the context of transhumanism and new control systems.",
        "- AI is framed within a broader context of informational and spiritual warfare.",
        "- The use of AI may contribute to societal division and manipulation through weaponized information."
      ]
    },
    {
      "id": 78,
      "title": "Episode 876: The Greatest American – Benjamin Franklin",
      "link": "#",
      "description": "Which makes it simple to keep everything on brand. And here's the best part. We saved money by switching to Shopify. It's more efficient, more flexible, and more affordable. Shopify is the commerce platform behind millions of businesses around the world. And 10% of all e-commerce in the US. From household names like Mattel and Gymshark to brands just getting started. Get started with your own design studio with hundreds of ready to use templates. Shopify helps you build a beautiful online store to match your brand style. Accelerate your content creation. Shopify is packed with helpful <span class=\"highlight\">AI</span> tools. The right product descriptions, page headlines, and even enhance your product photography. Get the word out like you have a marketing team behind you. Easily create email and social media campaigns wherever your customers are scrolling or strolling. And best yet, Shopify is your e-commerce expert with world class expertise in everything from managing inventory.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This article highlights Shopify's use of AI tools to enhance e-commerce capabilities, including generating product descriptions, page headlines, and improving product photography, thereby streamlining marketing efforts for businesses.",
      "takeaways": [
        "- Shopify integrates AI to assist in content creation and marketing campaigns.",
        "- AI tools help businesses create product descriptions and enhance visuals efficiently.",
        "- The platform supports inventory management alongside marketing functions using AI."
      ]
    },
    {
      "id": 79,
      "title": "Tsunami Warnings downgraded after massive Russian earthquake, Canada to recognise Palestine, Birmingham salutes the \"Prince of Darkness\"",
      "link": "#",
      "description": "The Smart 7 is an award winning daily podcast, in association with METRO, that gives you everything you need to know in 7 minutes, at 7am, 7 days a week… With over 18 million downloads and consistently charting, including as No. 1 News Podcast on Spotify, we're a trusted source for people every day and we've won Gold at the Signal International Podcast awards If you're enjoying it, please follow, share, or even post a review, it all helps... Today's episode includes the following: https://x.com/i/status/1950435552513114283 https://x.com/i/status/1950462261803225411 https://x.com/i/status/1950482784897642763 https://x.com/i/status/1950461440185266522 https://x.com/i/status/1950481313661669787 https://x.com/i/status/1950504398699807066 https://x.com/i/status/1950507004327285053 https://x.com/i/status/1950409050933313872 https://x.com/i/status/1950572476464853282 Contact us over @TheSmart7pod or visit www.thesmart7.com or find out more at www.metro.co.uk Voiced by Jamie East, using <span class=\"highlight\">AI</span>, written by Liam Thompson , researched by Lucie Lewis and produced by Daft Doris . Hosted on Acast. See acast.com/privacy for more information. >>> Avoiding your unfinished home projects because you're not sure where to start. Thumbtack news home so you don't have to. Don't know the difference between matte paint finish and sand, or what that clunking sound from your dryer is. With thumbtack, you don't have to be a home pro, you just have to hire one. You can hire top rated pros, see price estimates, and read reviews all on the app. Download today. On today's March 7th Russian earthquake sparks tsunami warning. Canada pledges to recognize Palestine and much more. It's Wednesday, 31st of July is Avocado Day and happy birthday, Amelia Box. It's March 7th. It's news, but not the. Seven. One of the strongest earthquakes ever recorded struck a remote region of Russia on Wednesday morning, triggering tsunami warnings across the Pacific Ocean. The quake, which measured 8.8 on the Richter scale, hit the Kamchatka Peninsula, an area which is prone to both earthquakes and volcanoes. Fortunately.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This podcast episode, voiced by AI, covers a broad range of current news topics including a major Russian earthquake and related tsunami warnings, with a brief mention of AI as part of its production process.",
      "takeaways": [
        "- The podcast is voiced by AI technology, highlighting the use of AI in media production.",
        "- The content primarily focuses on news events unrelated to AI developments or healthcare.",
        "- AI is mentioned only in the context of the podcast's production, without discussing AI tools, models, or pharmaceutical applications."
      ]
    },
    {
      "id": 80,
      "title": "Advisory That Pays: Help SMEs Secure Funding",
      "link": "#",
      "description": "already your payroll basically goes through the tax office superannuation payments are going through the tax of is they're aware a lot of things so that leads to a situation where that could be less work and that's why he was urging accountants and bookkeepers to become the trusted advisor to help your clients with these advisory areas and then one comes <span class=\"highlight\">artificial intelligence</span> a lot of people are talking about it and there's all sorts of doomsday report hitting the media but some people are saying be smart you can do a lot more work with the same team they look to reduce the try to undertake additional work sure some people need to be trained to be trained on the",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the growing conversation around artificial intelligence, highlighting both the concerns and opportunities it presents, particularly in increasing productivity within advisory and accounting services. It emphasizes the need for training to effectively leverage AI tools in the workplace.",
      "takeaways": [
        "- AI is generating mixed public sentiment, with both doomsday narratives and optimistic views on productivity gains.",
        "- AI can enable teams to accomplish more work without increasing headcount.",
        "- Training and upskilling are essential for professionals to effectively use AI in their advisory roles."
      ]
    },
    {
      "id": 81,
      "title": "Integrating artificial intelligence into structural maintenance and management",
      "link": "#",
      "description": "thousands of listeners you can find out more about sponsoring an episode of the podcast by visiting new civil engineer dotcom forward podcast all right we'll come back to the engineers collective and now joined by mind foundries director of civil infrastructure tom bartley hello tom hi rob great for you thanks for joining me so mind foundry let's get the spell who are mind foundry who are you what's let's give us the laydown this goes low down ok great so let's start with mine foundry mind foundry a <span class=\"highlight\">machine learning</span> company spun out of the <span class=\"highlight\">machine learning</span> research group at oxford university about ten years ago our founders still are active professors at the university specializing in applied applications for kind of all the emergent tech and making sure it works for the real world the founding",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This article discusses the integration of artificial intelligence and machine learning technologies in the context of structural maintenance and management, highlighting the work of Mind Foundry, a machine learning company with academic roots from Oxford University. It provides insight into the application of emergent AI technologies in real-world engineering infrastructure.",
      "takeaways": [
        "- AI is being conceptually proven and considered for application in infrastructure maintenance contracts.",
        "- There is active discussion around the next steps for AI deployment in this field.",
        "- The article raises questions concerning the ethical use and safeguards necessary for AI applications.",
        "- AI and machine learning are applied to enhance the efficiency and context in structural inspections.",
        "- Inspection data serves as training inputs to improve predictive models over time."
      ]
    },
    {
      "id": 82,
      "title": "Luka Doncic's PR Tour a REVENGE TOUR? | Who the Mavs 5th Starter NEEDS to Be | Take Off Thursday",
      "link": "#",
      "description": "On Thursdays. It's just us three talk about. Yeah. This is. This is why we tell you that slightly has been I this entire time. Now we've been on this show that. Yeah. I have to think about the potential. The potential of Marullo and I. I mean, the maps use <span class=\"highlight\">AI</span> and their hype videos. Oh yeah, they've been used. They used it all last year. The one where it looks like. Like it looks like an anime thing. Like when they green screened out players. No, that was there. Oh, that was skin. That was the row. I'm not that. It was all the rows team that the map did not do that. Why are you so clear on that? Oh self plug I'm dropping a timeline video on my go that I said that was the funniest part of this. What's going on? Like you, you started working on this when my son was born. Yeah, it's an hour and 25 minutes. Holy crap. Um, I got to sit down like, this is like, a minute. Uh, no, we're not going. I know I did do clips from pre-season talks. Let's go.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the use of AI technology by the Dallas Mavericks to create hype videos with animated and green screen effects, highlighting how AI is being integrated into sports marketing and advertising.",
      "takeaways": [
        "- The Dallas Mavericks have utilized AI tools for creating innovative hype videos.",
        "- These AI-generated videos incorporate anime-style and green screen effects to enhance fan engagement.",
        "- AI applications in sports marketing are becoming more prominent as teams explore new ways to connect with audiences."
      ]
    },
    {
      "id": 83,
      "title": "Episode 349 - Shohei Ohtani Exits Start Early, Dodgers Insider Michael Duarte Talks MLB Trade Deadline",
      "link": "#",
      "description": "let's go shopify is a global commerce platform that helps you sell at every stage of your business and sell more with less effort thanks to the shop of magic your <span class=\"highlight\">ai</span> powered all star sign up for a one dollar per month trial period a chappie dotcom red circle all lower case go to shape com red circle now to grow your business no matter what stage you're in shape dotcom red circle if you're a podcast host listen up this one's for you my name is allie jackson i'm the host of finding height a dating and relationship podcast that i've been doing for four years now sharing my positive and practical approach to dating that's built on my own life experience and i wanted to share another experience that i've had my secret behind monetizing my show it's called red circle and i was just telling my colleague about how much i love their platform with red circle not only am i getting a seamless hosting experience but i also love the",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The podcast episode includes a promotional mention of Shopify's AI-powered commerce platform and Red Circle's AI-enabled podcast hosting services, highlighting how AI tools are being used to enhance business growth and content monetization.",
      "takeaways": [
        "- Shopify leverages AI to streamline commerce and sales efforts for businesses at various stages.",
        "- Red Circle offers AI-driven podcast hosting and monetization solutions that simplify content management.",
        "- AI is increasingly integrated into marketing and advertising platforms to boost efficiency and revenue generation."
      ]
    },
    {
      "id": 84,
      "title": "Eps 74 | Marketing: From military discipline to marketing excellence - Chartered Institute of Marketing's CEO Chris Daly on professionalising an industry adapting to AI",
      "link": "#",
      "description": "that to open intelligence tool but he doesn't stop that unfortunately in this with this world of soft rapid change it's showing that you are willingness to adopt in tops and most successful organization i've come across you insisted that in their workforce sixty percent of their work need to be done using assets and is within the top right to sandbox and it's really getting an unnatural full band but actually when we look at the research in february as they were saying that a third of cmos little to no implementation of <span class=\"highlight\">ai</span> in their organizations i think this is obviously big enough to be able to react and he can take some bumps in the road but you're quite right that soon loss in value so what next and how will they get to that and i think again",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This episode features Chris Daly, CEO of the Chartered Institute of Marketing, discussing how AI is being integrated into the marketing industry, highlighting the challenges and progress made by organizations in adopting AI tools to enhance marketing professionalism and effectiveness.",
      "takeaways": [
        "- Many organizations are still in early stages or lack significant implementation of AI in marketing.",
        "- Successful companies are using AI extensively to improve workforce productivity and marketing outputs.",
        "- The marketing industry is undergoing rapid change driven by the need to adapt to AI technologies.",
        "- The marketing industry is evolving to professional standards influenced by AI integration.",
        "- There are emerging privacy regulations around the use of AI and data handling in marketing."
      ]
    },
    {
      "id": 85,
      "title": "Community Preparedness Basics with Live Like the World is Dying",
      "link": "#",
      "description": "And I talk about feeling burnt out, but no more. lenovo.com makes it easier than ever to find the perfect PC. In fact, we'll break it down for you. Step one shop our lineup of eyepieces powered by Intel Core Ultra processors. Because why deal with downloading new software when you can buy a smarter PC? That already comes with all of the <span class=\"highlight\">AI</span> features and tools? Step two find the one that fits your passion. Whether you're a creative genius, a research scientist, a hardcore gamer, or a future CEO, no dream is too big. That's the power of Lenovo. With Intel inside, you can keep up with your ideas as well as any challenges you may face. Step three add to cart and purchase. Oh, if you're a college student or a teacher, make an education account through us. And get 5% off your whole order. Shop now@lenovo.com. A better future is waiting and it needs you.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article highlights Lenovo's promotion of new PCs powered by Intel Core Ultra processors that come pre-equipped with AI features and tools, aiming to simplify access to advanced AI capabilities for various user groups.",
      "takeaways": [
        "- Lenovo is marketing PCs that integrate AI functionalities out-of-the-box, reducing the need for additional software downloads.",
        "- The focus is on catering to diverse users, including creatives, scientists, gamers, and students, emphasizing AI-enhanced computing.",
        "- The promotion includes educational discounts, suggesting an effort to encourage AI adoption among students and educators."
      ]
    },
    {
      "id": 86,
      "title": "Can Plastic Surgery Keep Up With AI?",
      "link": "#",
      "description": "even though it's sort of matching us in terms of its advances and sophistication we're getting to a point where something looks because it's too good to be true right absolutely so if it's too good to be true and surgery is unable to help you achieve something that's too good to be true then why don't you just admit to having surgery and let <span class=\"highlight\">ai</span> do the rest in that way you have come clean about having surgery meanwhile <span class=\"highlight\">ai</span> is making you look more perfect then surgery can ever make you look and there's your passport for perfection and authenticity and that's the holy grail that everybody wants they want to be perfect and authentic twenty years ago it was like you can't talk about getting plastic surgery because you wanted to attribute your genetics or your good sunscreen and drink lots of water or i do yoga to how you look but now with time we're realizing that we all age regardless and it's no way that life",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the evolving role of AI in enhancing aesthetic outcomes in plastic surgery, suggesting that AI may surpass traditional surgical methods in creating idealized appearances while raising questions about authenticity and societal perceptions.",
      "takeaways": [
        "- AI technologies are increasingly influencing the field of plastic surgery by improving or altering appearances beyond surgical capabilities.",
        "- There is a growing acceptance and openness around using AI in aesthetic enhancement as a complement or alternative to surgery.",
        "- The article touches on ethical and social implications related to perfection and authenticity in the context of AI-driven appearance modifications."
      ]
    },
    {
      "id": 87,
      "title": "Angelo Carusone & Rep. Greg Casar",
      "link": "#",
      "description": "so you can work create and boost productivity all on one device in business they say you can have better cheaper or faster but you only get to pick two what if you could have all three at the same time that's exactly what cohere thomson reuters and specialized bikes have since they upgraded to the next generation of the cloud oracle cloud infrastructure oci is the blazing fast platform for your infrastructure database application development and <span class=\"highlight\">ai</span> needs where you can run any workload in a high availability consistently high performance environment and spend less than you would with other clouds how is it faster block storage gives you more operations per second cheaper costs up to fifty percent less for computing seventy percent less for storage and eighty percent less for networking better in test after test customers report lower latency and higher bandwidth versus other clouds",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article highlights how Oracle Cloud Infrastructure (OCI) supports AI workloads with faster, cheaper, and better performance, demonstrating improvements that benefit businesses utilizing AI technologies.",
      "takeaways": [
        "- OCI provides a high-performance environment optimized for AI applications and other workloads.",
        "- Customers benefit from significantly lower costs in computing, storage, and networking.",
        "- Improved latency and bandwidth offer enhanced infrastructure support for AI development and deployment."
      ]
    },
    {
      "id": 88,
      "title": "SHOW SCHEDULE   7-30-25  THE SHOW BEGINS IN THE AIR WAR OVER EURASIA...",
      "link": "#",
      "description": "quired instagram and that said to be any competitive not clear how and then again school because it has <span class=\"highlight\">ai</span> investment the trump administration has another direction but the ftc still is suspicious of bigness i don't know why but that's what the case is certainly there's lots of competition and price for all these social media sites simon constable is in order seal that's or in the clouds southern france and we discuss the commodities especially copper all time high five sixty pound and the united kingdom and that north sea oil treasure that is not being used president trump brought it up to prime minister starmer it is odd uk can't grow it heavily taxes its people it doesn't know how to performance dull and it's got a treasure sitting off the north sea charlie",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article briefly mentions AI investment in the context of social media competition and regulatory scrutiny, though it primarily discusses geopolitical and economic topics unrelated to AI advancements or pharmaceutical applications.",
      "takeaways": [
        "- There is AI investment noted amidst social media competition.",
        "- Regulatory bodies like the FTC remain cautious about large tech companies.",
        "- The article primarily focuses on geopolitical and economic issues rather than AI developments.",
        "- The discussion likely covers AI regulation by the FTC.",
        "- It touches on concerns about the concentration of power or size in AI companies."
      ]
    },
    {
      "id": 89,
      "title": "AI Starting to Self-Improve Says Zuckerberg",
      "link": "#",
      "description": "leader who's done one of these what the near future holds types of posts last fall and tropic star amanda wrote his essay machines of loving grace which he basically said he needed to do because for as much as people were paying attention to him about the risks of <span class=\"highlight\">ai</span> he also wanted to paint a picture of what the best version of an future could be more recently we got from sam altman his post on the gentle singularity and something that altman and zuckerberg have in common that we will discuss further is this idea that we've crossed some fundamental threshold samoans piece begins we are past the event horizon the takeoff has started humanity is close to building digital superintelligence as we'll see zuckerberg makes a similar argument before we get into the details let's actually talk about the context into which this essay comes obviously one of the big stories of the last couple of months have been these incredibly large offers that zuckerberg meta have been throwing around to try to win talent from the rest of the research labs their success has at least been moderate altman and open continue to claim that in general zuckerberg had to go kind of far down the org chart",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This article discusses statements by Mark Zuckerberg and Sam Altman regarding the current state and future of AI, highlighting the notion that AI is beginning to self-improve and that humanity is nearing a digital superintelligence threshold. It also touches on Meta's recent efforts to compete for AI talent.",
      "takeaways": [
        "- Meta is beginning to observe \"glimpses\" of AI systems improving themselves.",
        "- There is skepticism about Meta’s ability to prioritize transformative AI work due to its ad-driven business model.",
        "- The discussion reflects ongoing debates about the future directions and ethical considerations of AI development in large tech companies.",
        "- Zuckerberg and Altman suggest AI development has crossed a fundamental threshold towards advanced self-improvement.",
        "- The concept of a \"gentle singularity\" and digital superintelligence is gaining attention among AI leaders."
      ]
    },
    {
      "id": 90,
      "title": "The Report Card with Nat Malkus: AI Lessons from Nigeria (with Martín De Simone)",
      "link": "#",
      "description": "everywhere and especially in the us one would think in this context in nigeria we had certain challenges that if you had a similar program here you wouldn't have and therefore the productivity of each of the sessions could be even higher so that's a reason for optimism on the other hand you could say the students in the us are actually much closer to the benchmark of learning and therefore there is less to learn at the end of that is an empirical question but i do think that if used properly there is a lot of room to enhance learning outcomes through <span class=\"highlight\">ai</span> and generative and finally very relevant to the us i think this is not what we did in this paper but as a tool to support teachers in their lesson planning in the assessment strategies and justin getting automated feedback perhaps in terms of their instructional strategies could be perhaps the next stage that we have to test in developed countries",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This discussion explores the potential of AI and generative models to enhance learning outcomes and support educational strategies, with implications for its application in both Nigerian and US contexts, particularly as a tool for assisting teachers in lesson planning and assessment.",
      "takeaways": [
        "- There is ongoing work to evaluate the effective implementation of AI tools in education.",
        "- The pace of AI adoption may be faster than the pace of empirical research and proper implementation.",
        "- Many frameworks and assessment tools exist, but actual implementation and evaluation remain limited.",
        "- AI and generative technologies have room to improve productivity and learning outcomes in education.",
        "- The article highlights the promise of AI to support teacher instructional strategies through automated feedback."
      ]
    },
    {
      "id": 91,
      "title": "Newly Discovered Documents Discovered By the FBI in Russia Hoax & Jerome Powell's Decision to Keep Interest Rates The Same",
      "link": "#",
      "description": "Aside, assuming there isn't something as severe as it is as those scenarios, I just have never seen an opportunity like this. This EU trade deal with the president ushers in a huge opportunity. There's a great story, even in USA today for blue collar workers. I've been listening to Mike Rowe and I read this article, and I referenced that many times on this program. The next generation of millionaires are going to be blue collar workers and tradesmen, because they're going to be in such high demand and the workforce is going to change, it's going to be altered, and it's going to be altered because the technology that technology is <span class=\"highlight\">artificial intelligence</span>, and America is going to be on the cutting edge of providing all the energy that is needed for all of that. But this trade deal with the European Union includes tariffs on U.S. imports and significant U. Uh, European Union investment in American industries. And the deal aims to boost the US economy, create jobs and.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the impact of artificial intelligence on the workforce and economy, highlighting how AI-driven technological changes are expected to create new opportunities and alter job dynamics, particularly benefiting blue collar workers. It also touches on a trade deal between the US and the European Union that aims to support economic growth and industry investment.",
      "takeaways": [
        "- Artificial intelligence is driving significant changes in the workforce and economy.",
        "- Blue collar workers and tradesmen may become highly demanded due to AI-related technological shifts.",
        "- The US-EU trade deal potentially supports these changes through investment and tariffs affecting industries."
      ]
    },
    {
      "id": 92,
      "title": "AI AND FTC: THE PURSUIT OF BIGNESS, JESSICA MELUGIN, CIVITAS INSTITUTE",
      "link": "#",
      "description": "being a leader globally on <span class=\"highlight\">artificial intelligence</span> it is strange that they're still pursuing the businesses who are funding that race and google and alphabet googl alphabet and matter their their position right now is in court is anybody seeking a settlement no settlements were offered during the biden years the ftc really moved away from settlements which is a bit unusual but they felt they had good cases in court i will tell you that the meta decision is expected this fall i think it's the weakest for the government of all the big tech cases and i would be surprised but what really should surprise me anymore i don't i don't know but i would be surprised if the government was successful as for the google cases on search and ad tech those decisions have come out but google has said they will appeal and now we're just waiting for the remedies phase of those trials before they can file their appeals just login for the center for technology innovation better the enterprise",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the ongoing legal and regulatory challenges faced by major tech companies like Google and Meta in relation to artificial intelligence, highlighting the FTC's approach to antitrust cases and the implications for AI development and industry regulation.",
      "takeaways": [
        "- The FTC is actively pursuing legal cases against large tech firms involved in AI, moving away from settlements.",
        "- Upcoming decisions, such as the Meta ruling, could impact government regulatory strategies in the AI sector.",
        "- Google's legal battles concerning search and advertising technologies continue, with appeals and remedies pending, affecting the future landscape of AI governance."
      ]
    },
    {
      "id": 93,
      "title": "Breakfast Bites: Would you still click on a link after reading an AI summary?",
      "link": "#",
      "description": "links to other sites when they saw a overviews twenty six percent of the time they just close the browser now this is quite a revelation given that people use google search five trillion times a year many websites make money through ads especially sites that provide information and content rather than selling things audience size matters and google's algorithm can basically shut your company down overnight no one expert says most websites rely on google traffic to keep the lights on but overviews are cutting into traffic so dramatically that many sites are seeing up to forty percent decline in their revenue it's having a pretty devastating impact and removing the incentive for a lot of people to create high quality content forty percent yes wow staggering so it's kind of convenience at the expense of not just fact checked content but high quality content as well now google you could argue is adding fuel to the fire because they've just launched a feature called <span class=\"highlight\">ai</span> mode gets rid of traditional search results entirely",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses how AI-generated summaries from Google’s new AI mode are significantly reducing traffic to traditional websites by replacing standard search results, which leads to a decline in ad revenue and threatens the creation of high-quality content.",
      "takeaways": [
        "- AI-driven summaries cause users to click through to original sites only 26% of the time, impacting website traffic.",
        "- Many content providers face up to a 40% loss in revenue due to reduced search traffic caused by AI features.",
        "- Google's AI mode removes traditional search results, fundamentally changing user interaction with information and raising concerns about content quality and monetization."
      ]
    },
    {
      "id": 94,
      "title": "New Border Lies - July 30th, Hour 1",
      "link": "#",
      "description": "USA today for blue collar workers. I've been listening to Mike Rowe and I read this article and I've I've referenced it many times on this program. The next generation of millionaires are going to be blue collar workers and tradesmen, because they're going to be in such high demand and the workforce is going to change, it's going to be altered, and it's going to be altered because the technology, that technology is <span class=\"highlight\">artificial intelligence</span>, and America is going to be on the cutting edge of providing all the energy that is needed for all of that. But this trade deal with the European Union includes tariffs on U.S. imports and significant U. Uh, European Union investment in American industries. And the deal aims to boost the US economy, create jobs and add to that, in the one big, beautiful bill the Democrats constantly try to demonize, there is what's called bonus depreciation for manufacturers. In other words, if you build out your facilities in.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses how artificial intelligence is expected to drive significant changes in the workforce, particularly benefiting blue-collar workers and tradesmen, and touches on the economic implications of new trade deals and industrial investments in the U.S.",
      "takeaways": [
        "- AI technology is poised to alter the workforce landscape, creating high demand for blue-collar and trade jobs.",
        "- The U.S. aims to be at the forefront of AI-driven energy innovation.",
        "- Trade agreements and policies are linked to supporting industrial growth and job creation in the context of AI advancements."
      ]
    },
    {
      "id": 95,
      "title": "Accelerating AI Adoption for Customer Loyalty - Insights with Salesforce and IBM (#691)",
      "link": "#",
      "description": "and as a veteran of situating myself in a war room on big and small covers i've seen it all at all what are you seen in terms of like best practices or any tips that you may have to actually mitigate or even eliminate risk as you go to this very important movement of cutting over to a new system we can have a super podcast probably just on that i think for us it's now there's several things one we always talk about the power of the unified platform and the fact that you have kind of a future proof solution with the salesforce platform we're constantly iterating on that platform so there's three core releases that happen every year for all of our products when it comes to things like <span class=\"highlight\">ai</span> that functionality is dropping in a much faster pace so i think there's a conversation around build versus buy",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "This article discusses accelerating AI adoption within customer loyalty platforms, focusing on Salesforce's unified platform and its rapid integration of AI capabilities through multiple annual releases. It highlights best practices and risk mitigation strategies in transitioning to AI-enhanced systems for marketing and customer engagement.",
      "takeaways": [
        "- Emphasizes starting with simple AI use cases to mitigate risks before scaling to complex applications.",
        "- Highlights the importance of training and user understanding to ensure effective AI deployment.",
        "- Demonstrates how automation and pre-planning (e.g., test scripts, user stories) can speed up AI implementation and reduce risks.",
        "- Salesforce is continuously updating its platform with new AI functionalities at a faster pace.",
        "- The discussion includes managing risks when migrating to AI-driven systems."
      ]
    },
    {
      "id": 96,
      "title": "Ep. 2674 Michael Malice on AI, Ozzy Osbourne, and the Democrats",
      "link": "#",
      "description": "I know he's a person, so if they used <span class=\"highlight\">AI</span> to make another Christopher Reeve movie or like on YouTube, I'm sure we have we have technology right now. I'm sure it's pretty crappy at this point, but let's give it five years where you could say, okay, I make, you know, even easier. He's an easier problem to make this first Christopher Reeve movie. But instead of Margot Kidder, Lois Lane is played by Marilyn Monroe. Just replace her, right? Like how they replaced all the guns in it? I don't think it would be very hard to do. I don't think that would in any way, knowing that they're both dead. Change anything? I don't know, I got to mull this over and look some in the comments. Are there people who are kind of getting what I'm trying to say, but I can't say. If there were and I bought here instead of me, it would be able to probably explain what makes me so uneasy about this world. Like I don't want to go to I don't like Sonic anyway. I guess I don't want a robot serving me even a if I can do it more efficiently, I guess I just don't have. Are you crazy? Don't want that because then I could go.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The discussion reflects on the ethical and societal implications of AI technologies, particularly around AI-generated media content and human interaction with AI-driven automation. The speaker expresses unease about the impact of AI on authenticity and personal experience.",
      "takeaways": [
        "- AI is being considered for advanced media creation, such as replacing actors in movies.",
        "- There is a societal discomfort with AI replacing human roles, even if it increases efficiency.",
        "- Ethical and emotional concerns around AI technology usage remain significant in public discourse.",
        "- AI can distill and combine existing ideas into interesting outputs but struggles with generating fully original concepts.",
        "- There is an emotional and human connection in creative works that AI currently cannot replicate."
      ]
    },
    {
      "id": 97,
      "title": "Controlling nature's data",
      "link": "#",
      "description": "and it has bigger consequences almost logically data like this so it's a really it almost makes the heart by a bit faster seeing this and wondering what it means and why we talk about this as effectively a digital twin of evolution is that when you then expose this type of data to <span class=\"highlight\">machine learning</span> models can let you ask questions like if evolution were to have solved this problem how did have done it so in a way we sort of we view this as our gateway to collaborating with nature and collaborating with evolution rather than just trying to look at it you can ask your questions it's an extraordinary thought collaborating with evolution although some of its most groundbreaking possibilities could still be a long way off two relatively young guys and you've set out to map the entirety of life on earth just interested to know what you're going to do next do you finish a base camp i think the scale of the game is big enough to keep us",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the use of machine learning models to analyze evolutionary data, effectively creating a \"digital twin\" of evolution that allows researchers to explore how nature has solved complex problems, potentially opening new avenues for collaboration between AI and biological research.",
      "takeaways": [
        "- Machine learning is used to interpret large-scale evolutionary data.",
        "- This approach enables asking novel questions about natural problem-solving by evolution.",
        "- The research could have significant implications for understanding life on Earth and advancing healthcare-related biological insights.",
        "- AI companions are becoming more prevalent and impacting personal relationships.",
        "- Open, accessible scientific databases enabled by AI can accelerate biodiversity and scientific discovery."
      ]
    },
    {
      "id": 98,
      "title": "Looking Ahead To The 2025 School Year",
      "link": "#",
      "description": "Few years as <span class=\"highlight\">AI</span> has become the zeitgeist. But Texas does not have direct guidance on arenas like academic integrity, as Laura has just mentioned, but also just ethical uses, privacy, and a lot of other issues that teachers, students, families might have about incorporation of <span class=\"highlight\">AI</span> in the classroom. We need a little bit more to go on. Here's the message we got from Richard in Florida. He teaches preschool violin. I got into early childhood music education because my daughter decided to play the violin at two years old, and there was no one in our community teaching on the entrepreneur, so I started the program. Fast forward 25 years later, we were about to expand this program across the state. I called because I think that if we were to change the education conversation away from, um, what to do with now that the funding has gone and maybe introduced something we've not thought of, like this concept entrepreneurial socialism, and what that simply means is.",
      "date": "2025-07-31",
      "authors": "Unknown Author",
      "journal": "PODCAST",
      "therapyArea": "Pulsar Social & Search Insights",
      "therapyAreas": [
        "Pulsar Social & Search Insights"
      ],
      "term": "Pulsar Combined Search",
      "terms": [
        "Pulsar Combined Search"
      ],
      "summary": "The article discusses the challenges and lack of clear guidance around the ethical use, privacy, and academic integrity concerns related to AI incorporation in the educational system, highlighting the need for more structured policies.",
      "takeaways": [
        "- There is a growing need for clear guidance on ethical AI use and privacy in education.",
        "- Current policies in some regions, such as Texas, are insufficient to address AI-related concerns in schools.",
        "- The conversation about AI's role in education includes broader implications like academic integrity and social considerations."
      ]
    }
  ],
  "keyHighlights": {
    "summary": "",
    "trends": [
      "- Widespread adoption of AI-powered personalization and hyper-personalized customer experiences to drive business growth and loyalty.",
      "- Integration of AI and machine learning in structural maintenance, healthcare, finance, and supply chain operations for improved efficiency, prediction, and innovation.",
      "- Rising investment and strategic competition among major tech firms and global regions to develop advanced AI capabilities, including self-improving AI and superintelligence research.",
      "- Increasing attention to ethical, privacy, and societal challenges surrounding AI deployment, including bias mitigation, regulatory scrutiny, and impacts on workforce dynamics.",
      "- Expansion of AI applications into creative fields, cybersecurity, education, and marketing, supported by emerging tools that enhance productivity and user engagement."
    ],
    "technologies": [
      "- Agentic AI and self-improving AI models enabling more autonomous AI systems with reduced human intervention.",
      "- AI-driven tools for structural inspection and maintenance management utilizing machine learning to predict deterioration and optimize workflows.",
      "- AI-powered personalization platforms and marketing automation tools that leverage customer intent analysis for enhanced engagement and loyalty."
    ],
    "therapy_area_distribution": {
      "AI for Business and Strategy": 22,
      "Pulsar Social & Search Insights": 25
    },
    "total_articles": 99
  }
}

{
  "metadata": {
    "generatedAt": "2025-04-02 13:38:00",
    "year": 2025,
    "month": 4,
    "monthName": "April",
    "articleCount": 14
  },
  "articles": [
    {
      "id": 0,
      "title": "Introducing AWS MCP Servers for code assistants (Part 1)",
      "link": "https://aws.amazon.com/blogs/machine-learning/introducing-aws-mcp-servers-for-code-assistants-part-1/",
      "description": "We’re excited to announce the open source release of AWS MCP Servers for code assistants — a suite of specialized Model Context Protocol (MCP) servers that bring Amazon Web Services (AWS) best practices directly to your development workflow. Our specialized AWS MCP servers combine deep AWS knowledge with agentic AI capabilities to accelerate development across key areas. Each AWS MCP Server focuses on a specific domain of AWS best practices, working together to provide comprehensive guidance throughout your development journey.\nThis post is the first in a series covering AWS MCP Servers. In this post, we walk through how these specialized MCP servers can dramatically reduce your development time while incorporating security controls, cost optimizations, and AWS Well-Architected best practices into your code. Whether you’re an experienced AWS developer or just getting started with cloud development, you’ll discover how to use AI-powered coding assistants to tackle common challenges such as complex service configurations, infrastructure as code (IaC) implementation, and knowledge base integration. By the end of this post, you’ll understand how to start using AWS MCP Servers to transform your development workflow and deliver better solutions, faster.\nIf you want to get started right away, skip ahead to the section “From Concept to working code in minutes.”\nAI is transforming how we build software, creating opportunities to dramatically accelerate development while improving code quality and consistency. Today’s AI assistants can understand complex requirements, generate production-ready code, and help developers navigate technical challenges in real time. This AI-driven approach is particularly valuable in cloud development, where developers need to orchestrate multiple services while maintaining security, scalability, and cost-efficiency.\nDevelopers need code assistants that understand the nuances of AWS services and best practices. Specialized AI agents can address these needs by:\n\nProviding contextual guidance on AWS service selection and configuration\nOptimizing compliance with security best practices and regulatory requirements\nPromoting the most efficient utilization and cost-effective solutions\nAutomating repetitive implementation tasks with AWS specific patterns\n\nThis approach means developers can focus on innovation while AI assistants handle the undifferentiated heavy lifting of coding. Whether you’re using Amazon Q, Amazon Bedrock, or other AI tools in your workflow, AWS MCP Servers complement and enhance these capabilities with deep AWS specific knowledge to help you build better solutions faster.\nModel Context Protocol (MCP) is a standardized open protocol that enables seamless interaction between large language models (LLMs), data sources, and tools. This protocol allows AI assistants to use specialized tooling and to access domain-specific knowledge by extending the model’s capabilities beyond its built-in knowledge—all while keeping sensitive data local. Through MCP, general-purpose LLMs can now seamlessly access relevant knowledge beyond initial training data and be effectively steered towards desired outputs by incorporating specific context and best practices.\nAccelerate building on AWS\nWhat if your AI assistant could instantly access deep AWS knowledge, understanding every AWS service, best practice, and architectural pattern? With MCP, we can transform general-purpose LLMs into AWS specialists by connecting them to specialized knowledge servers. This opens up exciting new possibilities for accelerating cloud development while maintaining security and following best practices.\nBuild on AWS in a fraction of the time, with best practices automatically applied from the first line of code. Skip hours of documentation research and immediately access ready-to-use patterns for complex services such as Amazon Bedrock Knowledge Bases. Our MCP Servers will help you write well-architected code from the start, implement AWS services correctly the first time, and deploy solutions that are secure, observable, and cost-optimized by design. Transform how you build on AWS today.\n\nEnforce AWS best practices automatically – Write well-architected code from the start with built-in security controls, proper observability, and optimized resource configurations\nCut research time dramatically – Stop spending hours reading documentation. Our MCP Servers provide contextually relevant guidance for implementing AWS services correctly, addressing common pitfalls automatically\nAccess ready-to-use patterns instantly – Use pre-built AWS CDK constructs, Amazon Bedrock Agents schema generators, and Amazon Bedrock Knowledge Bases integration templates that follow AWS best practices from the start\nOptimize cost proactively – Prevent over-provisioning as you design your solution by getting cost-optimization recommendations and generating a comprehensive cost report to analyze your AWS spending before deployment\n\nTo turn this vision into reality and make AWS development faster, more secure, and more efficient, we’ve created AWS MCP Servers—a suite of specialized AWS MCP Servers that bring AWS best practices directly to your development workflow. Our specialized AWS MCP Servers combine deep AWS knowledge with AI capabilities to accelerate development across key areas. Each AWS MCP Server focuses on a specific domain of AWS best practices, working together to provide comprehensive guidance throughout your development journey.\nOverview of domain-specific MCP Servers for AWS development\nOur specialized MCP Servers are designed to cover distinct aspects of AWS development, each bringing deep knowledge to specific domains while working in concert to deliver comprehensive solutions:\n\nCore – The foundation server that provides AI processing pipeline capabilities and serves as a central coordinator. It helps provide clear plans for building AWS solutions and can federate to other MCP servers as needed.\nAWS Cloud Development Kit (AWS CDK) – Delivers AWS CDK knowledge with tools for implementing best practices, security configurations with cdk-nag, Powertools for AWS Lambda integration, and specialized constructs for generative AI services. It makes sure infrastructure as code (IaC) follows AWS Well-Architected principles from the start.\nAmazon Bedrock Knowledge Bases – Enables seamless access to Amazon Bedrock Knowledge Bases so developers can query enterprise knowledge with natural language, filter results by data source, and use reranking for improved relevance.\nAmazon Nova Canvas – Provides image generation capabilities using Amazon Nova Canvas through Amazon Bedrock, enabling the creation of visuals from text prompts and color palettes—perfect for mockups, diagrams, and UI design concepts.\nCost – Analyzes AWS service costs and generates comprehensive cost reports, helping developers understand the financial implications of their architectural decisions and optimize for cost-efficiency.\n\nPrerequisites\nTo complete the solution, you need to have the following prerequisites in place:\n\nuv package manager\nInstall Python using uv python install 3.13\nAWS credentials with appropriate permissions\nAn MCP-compatible LLM client (such as Anthropic’s Claude for Desktop, Cline, Amazon Q CLI, or Cursor)\n\nFrom concept to working code in minutes\nYou can download the AWS MCP Servers on GitHub or through the PyPI package manager. Here’s how to get started using your favorite code assistant with MCP support.\nTo install MCP Servers, enter the following code:\n\n\n# Install and setup the MCP servers\n{\n  \"mcpServers\": {\n    \"awslabs.core-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"awslabs.core-mcp-server@latest\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n        \"MCP_SETTINGS_PATH\": \"path to your mcp server settings\"\n      },\n      \"autoApprove\": [],\n      \"disabled\": false\n    },\n    \"awslabs.bedrock-kb-retrieval-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.bedrock-kb-retrieval-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    },\n    \"awslabs.cdk-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.cdk-mcp-server@latest\"],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    },\n    \"awslabs.cost-analysis-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.cost-analysis-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    },\n    \"awslabs.nova-canvas-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\"awslabs.nova-canvas-mcp-server@latest\"],\n      \"env\": {\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    }\n  }\n}\n\n\nAWS MCP Servers in action\nHere’s how AWS MCP servers transform the development experience:\nDeveloper: “I need to build an AI-powered chatbot using Amazon Bedrock that can answer questions from our company’s knowledge base. I also want to add a tool for the chatbot to call our internal API.”\nCore: “I’ll help you build an Amazon Bedrock Knowledge Bases chatbot with API integration. Let’s create an architecture that uses Amazon Bedrock Agents with a custom action group to call your internal API.”\nCore generates a comprehensive architecture diagram showing the knowledge base integration, Amazon Bedrock Agents configuration with action groups, API connectivity, and data flow between components.\nAWS CDK: “Here’s the infrastructure code for your chatbot with the Amazon Bedrock Agents action group. I’ve included proper IAM roles, security controls, and Lambda Powertools for observability.”\n\n// CDK code with Well-Architected best practices\n const knowledgeBase = new BedrockKnowledgeBase(this, 'CompanyKB', {\n   embeddingModel: BedrockFoundationModel.TITAN_EMBED_TEXT_V1,\n   vectorStore: new OpenSearchServerlessVectorStore(this, 'VectorStore', {\n     encryption: OpenSearchEncryption.KMS,\n     ebs: OpenSearchEbsOptions.provisioned(100, OpenSearchVolumeType.GP3)\n   })\n });\n\n // Action Group for API integration\n const actionGroupSchema = new BedrockAgentSchema(this, 'ApiActionSchema', {\n   schemaPath: 'schemas/api-action-schema.json',\n   // Schema will be auto-generated from your Lambda code\n   generateFromLambda: {\n     lambdaPath: 'lambda/api-actions.py',\n     outputPath: 'schemas/api-action-schema.json'\n   }\n });\n\n const apiActionGroup = new BedrockAgentActionGroup(this, 'ApiActionGroup', {\n   actionGroupName: 'InternalApiActions',\n   description: 'Actions to interact with internal API',\n   actionGroupExecutor: new BedrockAgentLambdaExecutor(apiActionsLambda),\n   actionGroupSchema: actionGroupSchema\n });\n\nThe CDK MCP Server generates complete AWS CDK code to deploy the entire solution. It automatically runs cdk-nag to identify potential security issues and provides remediation steps for each finding, making sure that the infrastructure follows AWS Well-Architected best practices.\nAmazon Bedrock Knowledge Bases retrieval: “I’ve configured the optimal settings for your knowledge base queries, including proper reranking for improved relevance.”\nAmazon Bedrock Knowledge Bases MCP Server demonstrates how to structure queries to the knowledge base for maximum relevance, provides sample code for filtering by data source, and shows how to integrate the knowledge base responses with the chatbot interface.\nAmazon Nova Canvas: “To enhance your chatbot’s capabilities, I’ve created visualizations that can be generated on demand when users request data explanations.”\nAmazon Nova Canvas MCP server generates sample images showing how Amazon Nova Canvas can create charts, diagrams, and visual explanations based on knowledge base content, making complex information more accessible to users.\nCost Analysis: “Based on your expected usage patterns, here’s the estimated monthly cost breakdown and optimization recommendations.”\nThe Cost Analysis MCP Server generates a detailed cost analysis report showing projected expenses for each AWS service, identifies cost optimization opportunities such as reserved capacity for Amazon Bedrock, and provides specific recommendations to reduce costs without impacting performance.\nWith AWS MCP Servers, what would typically take days of research and implementation is completed in minutes, with better quality, security, and cost-efficiency than manual development in that same time.\nBest practices for MCP-assisted development\nTo maximize the benefits of MCP assisted development while maintaining security and code quality, developers should follow these essential guidelines:\n\nAlways review generated code for security implications before deployment\nUse MCP Servers as accelerators, not replacements for developer judgment and expertise\nKeep MCP Servers updated with the latest AWS security best practices\nFollow the principle of least privilege when configuring AWS credentials\nRun security scanning tools on generated infrastructure code\n\nComing up in the series\nThis post introduced the foundations of AWS MCP Servers and how they accelerate AWS development through specialized, AWS specific MCP Servers. In upcoming posts, we’ll dive deeper into:\n\nDetailed walkthroughs of each MCP server’s capabilities\nAdvanced patterns for integrating AWS MCP Servers into your development workflow\nReal-world case studies showing AWS MCP Servers’ impact on development velocity\nHow to extend AWS MCP Servers with your own custom MCP servers\n\nStay tuned to learn how AWS MCP Servers can transform your specific AWS development scenarios and help you build better solutions faster. Visit our GitHub repository or Pypi package manager to explore example implementations and get started today.\n\nAbout the Authors\nJimin Kim is a Prototyping Architect on the AWS Prototyping and Cloud Engineering (PACE) team, based in Los Angeles. With specialties in Generative AI and SaaS, she loves helping her customers succeed in their business. Outside of work, she cherishes moments with her wife and three adorable calico cats.\nPranjali Bhandari is part of the Prototyping and Cloud Engineering (PACE) team at AWS, based in the San Francisco Bay Area. She specializes in Generative AI, distributed systems, and cloud computing. Outside of work, she loves exploring diverse hiking trails, biking, and enjoying quality family time with her husband and son.\nLaith Al-Saadoon is a Principal Prototyping Architect on the Prototyping and Cloud Engineering (PACE) team. He builds prototypes and solutions using generative AI, machine learning, data analytics, IoT & edge computing, and full-stack development to solve real-world customer challenges. In his personal time, Laith enjoys the outdoors–fishing, photography, drone flights, and hiking.\nPaul Vincent is a Principal Prototyping Architect on the AWS Prototyping and Cloud Engineering (PACE) team. He works with AWS customers to bring their innovative ideas to life. Outside of work, he loves playing drums and piano, talking with others through Ham radio, all things home automation, and movie nights with the family.\nJustin Lewis leads the Emerging Technology Accelerator at AWS. Justin and his team help customers build with emerging technologies like generative AI by providing open source software examples to inspire their own innovation. He lives in the San Francisco Bay Area with his wife and son.\nAnita Lewis is a Technical Program Manager on the AWS Emerging Technology Accelerator team, based in Denver, CO. She specializes in helping customers accelerate their innovation journey with generative AI and emerging technologies. Outside of work, she enjoys competitive pickleball matches, perfecting her golf game, and discovering new travel destinations.",
      "date": "2025-04-01",
      "authors": "Jimin Kim",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article introduces AWS MCP Servers, a new suite of specialized Model Context Protocol servers designed to enhance cloud development with AI capabilities, providing support for AWS best practices and accelerating the development workflow for engineers through contextual guidance and automation.",
      "takeaways": [
        "- AWS MCP Servers utilize AI to improve development efficiency by automatically applying best practices for security, observability, and cost optimization in AWS environments.",
        "- The Model Context Protocol (MCP) enhances the capabilities of general-purpose language models, allowing them to access domain-specific knowledge and tools for effective guidance in cloud development.",
        "- The suite includes various servers, such as those focused on AWS CDK, cost analysis, and knowledge bases, which collaboration to streamline the building of complex AWS solutions."
      ]
    },
    {
      "id": 1,
      "title": "Harness the power of MCP servers with Amazon Bedrock Agents",
      "link": "https://aws.amazon.com/blogs/machine-learning/harness-the-power-of-mcp-servers-with-amazon-bedrock-agents/",
      "description": "AI agents extend large language models (LLMs) by interacting with external systems, executing complex workflows, and maintaining contextual awareness across operations. Amazon Bedrock Agents enables this functionality by orchestrating foundation models (FMs) with data sources, applications, and user inputs to complete goal-oriented tasks through API integration and knowledge base augmentation. However, in the past, connecting these agents to diverse enterprise systems has created development bottlenecks, with each integration requiring custom code and ongoing maintenance—a standardization challenge that slows the delivery of contextual AI assistance across an organization’s digital ecosystem. This is a problem that you can solve by using Model Context Protocol (MCP), which provides a standardized way for LLMs to connect to data sources and tools.\nToday, MCP is providing agents standard access to an expanding list of accessible tools that you can use to accomplish a variety of tasks. In time, MCP can promote better discoverability of agents and tools through marketplaces, enabling agents to share context and have common workspaces for better interaction, and scale agent interoperability across the industry.\nIn this post, we show you how to build an Amazon Bedrock agent that uses MCP to access data sources to quickly build generative AI applications. Using Amazon Bedrock Agents, your agent can be assembled on the fly with MCP-based tools as in this example:\n\nInlineAgent(\n    foundation_model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    instruction=\"You are a friendly assistant for resolving user queries\",\n    agent_name=\"SampleAgent\",\n    action_groups=[\n        ActionGroup(\n            name=\"SampleActionGroup\",\n            mcp_clients=[mcp_client_1, mcp_client_2],\n        )\n    ],\n).invoke(input_text=”Convert 11am from NYC time to London time”)\n\nWe showcase an example of building an agent to understand your Amazon Web Service (AWS) spend by connecting to AWS Cost Explorer, Amazon CloudWatch, and Perplexity AI through MCP. You can use the code referenced in this post to connect your agents to other MCP servers to address challenges for your business. We envision a world where agents have access to an ever-growing list of MCP servers that they can use for accomplishing a wide variety of tasks.\nModel Context Protocol\nDeveloped by Anthropic as an open protocol, MCP provides a standardized way to connect AI models to virtually any data source or tool. Using a client-server architecture, MCP enables developers to expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. Through this architecture, MCP enables users to build more powerful, context-aware AI agents that can seamlessly access the information and tools they need. Whether you’re connecting to external systems or internal data stores or tools, you can now use MCP to interface with all of them in the same way. The client-server architecture of MCP enables your agent to access new capabilities as the MCP server updates without requiring any changes to the application code.\nMCP architecture\nMCP uses a client-server architecture that contains the following components and is shown in the following figure:\n\nHost: An MCP host is a program or AI tool that requires access to data through the MCP protocol, such as Claude Desktop, an integrated development environment (IDE), or any other AI application.\nClient: Protocol clients that maintain one-to-one connections with servers.\nServer: Lightweight programs that expose capabilities through standardized MCP.\nLocal data sources: Your databases, local data sources, and services that MCP servers can securely access.\nRemote services: External systems available over the internet through APIs that MCP servers can connect to.\n\n\nLet’s walk through how to set up Amazon Bedrock agents that take advantage of MCP servers.\nUsing MCP with Amazon Bedrock agents\nIn this post, we provide a step-by-step guide for how to connect your favorite MCP servers with Amazon Bedrock agents as Action Groups that an agent can use to accomplish tasks provided by the user. The AgentInlineSDK provides a straightforward way to create inline agents, containing a built-in MCP client implementation that provides you with direct access to tools delivered by an MCP server.\nAs part of creating an agent, the developer creates an MCP client specific to each MCP server that requires agent communication. When invoked, the agent determines which tools are needed for the user’s task; if MCP server tools are required, it uses the corresponding MCP client to request tool execution from that server. The user code doesn’t need to be aware of the MCP protocol because that’s handled by the MCP client provided the InlineAgent code repository.\nTo orchestrate this workflow, you take advantage of the return control capability of Amazon Bedrock Agents. The following diagram illustrates the end-to-end flow of an agent handling a request that uses two tools. In the first flow, a Lambda-based action is taken, and in the second, the agent uses an MCP server.\n\nUse case: transform how you manage your AWS spend across different AWS services including Amazon Bedrock\nTo show how an Amazon Bedrock agent can use MCP servers, let’s walk through a sample use case. Imagine asking questions like “Help me understand my Bedrock spend over the last few weeks” or “What were my EC2 costs last month across regions and instance types?” and getting a human-readable analysis of the data instead of raw numbers on a dashboard. The system interprets your intent and delivers precisely what you need—whether that’s detailed breakdowns, trend analyses, visualizations, or cost-saving recommendations. This is useful because what you’re interested in is insights rather than data. You can accomplish this using two MCP servers: a custom-built MCP server for retrieving the AWS spend data and an open source MCP server from Perplexity AI to interpret the data. You add these two MCP servers as action groups in an inline Amazon Bedrock agent. This gives you an AI agent that can transform the way you manage your AWS spend. All the code for this post is available in the GitHub repository.\nLet’s walk through how this agent is created using inline agents. You can use inline agents to define and configure Amazon Bedrock agents dynamically at runtime. They provide greater flexibility and control over agent capabilities, enabling users to specify FMs, instructions, action groups, guardrails, and knowledge bases as needed without relying on pre-configured control plane settings. It’s worth noting that you can also orchestrate this behavior without inline agents by using RETURN_CONTROL with the InvokeAgent API.\nMCP components in Amazon Bedrock Agents\n\nHost: This is the Amazon Bedrock inline agent. This agent adds MCP clients as action groups that can be invoked through RETURN_CONTROL when the user asks an AWS spend-related question.\nClient: You create two clients that establish one-to-one connections with their respective servers: a cost explorer client with specific cost server parameters and a Perplexity AI client with Perplexity server parameters.\nServers: You create two MCP servers that each run locally on your machine and communicate to your application over standard input/output (alternatively, you could also configure the client to talk to remote MCP servers). \n  \nCost Explorer and Amazon CloudWatch Logs (for Amazon Bedrock model invocation log data) and an MCP server to retrieve the AWS spend data.\nPerplexity AI MCP server to interpret the AWS spend data.\n \nData sources: The MCP servers talk to remote data sources such as Cost Explorer API, CloudWatch Logs and the Perplexity AI search API.\n\nPrerequisites\nYou need the following prerequisites to get started implementing the solution in this post:\n\nAn AWS account\nFamiliarity with FMs and Amazon Bedrock\nInstall AWS Command Line Interface (AWS CLI) and set up credentials\nPython 3.11 or later\nAWS Cloud Development Kit (AWS CDK) CLI\nEnable model access for Anthropic’s Claude 3.5 Sonnet v2\nYou need to have your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY so that you can set them using environment variables for the server\nThe two MCP servers are run as Docker daemons, so you need to have Docker installed and running on your computer\n\nThe MCP servers run locally on your computer and need to access AWS services and the Perplexity API. You can read more about AWS credentials in Manage access keys for IAM users. Make sure that your credentials include AWS Identity and Access Manager (IAM) read access to Cost Explorer and CloudWatch. You can do this by using AWSBillingReadOnlyAccess and CloudWatchReadOnlyAccess managed IAM permissions. You can get the Perplexity API key from the Perplexity Sonar API page.\nSteps to run\nWith the prerequisites in place, you’re ready to implement the solution.\n\nNavigate to the InlineAgent GitHub repository.\nFollow the setup steps.\nNavigate to the cost_explorer_agent This folder contains the code for this post. \n  \ncd examples/mcp/cost_explorer_agent\n \nCreate a .env file in cost_explorer_agent directory using example. \n  \nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nAWS_REGION=\nBEDROCK_LOG_GROUP_NAME=\nPERPLEXITY_API_KEY=\n \nBuild aws-cost-explorer-mcp server \n  \ngit clone https://github.com/aarora79/aws-cost-explorer-mcp-server.git\ncd aws-cost-explorer-mcp-server/\ndocker build -t aws-cost-explorer-mcp\n \nYou’re now ready to create an agent that can invoke these MCP servers to provide insights into your AWS spend. You can do this by running the python main.py command. The output will look like the following example. The agent connects with the two MCP servers and accesses their respective tools. Next, the agent lays out a plan to use specific tools sequentially, uses code interpreter to generate a chart showing cost distribution, and uses Cost Explorer and Perplexity API to provide information about Amazon Bedrock and the AWS account’s cost expenditure. \n  \nConnected to server with tools: ['get_bedrock_daily_usage_stats', 'get_bedrock_hourly_usage_stats', 'get_ec2_spend_last_day', 'get_detailed_breakdown_by_day']\nConnected to server with tools: ['perplexity_ask']\n\nThought: Let me break this down into parts:\n1. First, I'll ask about Amazon Bedrock using Perplexity\n2. Then I'll get the cost breakdown for the last 7 days\n3. Finally, I'll create a visualization of the spending data\n\nTool use: perplexity_ask with these inputs: messages[[{role=user, content=What is Amazon Bedrock? Provide a concise explanation.}] (array)]\n\nTool use: get_detailed_breakdown_by_day with these inputs: days[7 (integer)]\n\nThought: Let me analyze the data and create a visualization of the top services by cost. I'll use Python to process the data and create a bar chart.\n\nCode interpreter:\nGenerated code\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n...\n```\n\nCode interpreter output:\naws_costs.png (image/png)\nDirectory output created successfully\n\nThought: Now I can provide a comprehensive answer about Amazon Bedrock and the cost analysis. Amazon Bedrock is a fully managed service that provides access to foundation models (FMs) from leading AI companies through a single API. Based on the cost analysis of the last 7 days, your top spending services were:\n\n1. Amazon Neptune: $24\n2. Amazon SageMaker: $567\n3. EC2 (Compute + Other): $454\n4. Amazon OpenSearch Service: $34\n5. Amazon Bedrock: $1235\n6. Amazon Q: $178\n \n\n\n\n\n\n\nImplementation details\nNow that you understand the output produced by an agent, let’s lift the curtain and review some of the important pieces of code that produce the output.\n\nCreating MCP clients: config.py, defines the two MCP clients that talk to your two MCP servers. \n  \nServer parameters are defined for the cost explorer and Perplexity clients. The solution uses StdioServerParameters, which configures how the client should communicate over standard input/output (stdio) streams. This contains the parameters required by the server to access the required data through APIs. \n    \n# Cost server parameters\ncost_server_params = StdioServerParameters(\n    command=\"/usr/local/bin/docker\",\n    args=[\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"AWS_ACCESS_KEY_ID\",\n        \"-e\",\n        \"AWS_SECRET_ACCESS_KEY\",\n        \"-e\",\n        \"AWS_REGION\",\n        \"-e\",\n        \"BEDROCK_LOG_GROUP_NAME\",\n        \"-e\",\n        \"stdio\",\n        \"aws-cost-explorer-mcp:latest\",\n    ],\n    env={\n        \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n        \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n        \"AWS_REGION\": AWS_REGION,\n        \"BEDROCK_LOG_GROUP_NAME\": BEDROCK_LOG_GROUP_NAME,\n    },\n)\n\n# Perplexity server parameters\nperplexity_server_params = StdioServerParameters(\n    command=\"/usr/local/bin/docker\",\n    args=[\"run\", \"-i\", \"--rm\", \"-e\", \"PERPLEXITY_API_KEY\", \"mcp/perplexity-ask\"],\n    env={\"PERPLEXITY_API_KEY\": PERPLEXITY_API_KEY},\n)\n \nIn main.py, the MCP server parameters are imported and used to create your two MCP clients. \n    \ncost_explorer_mcp_client = await MCPClient.create(server_params=cost_server_params)\nperplexity_mcp_client = await MCPClient.create(server_params=perplexity_server_params)\n \n \n\n\nConfigure agent action group: main.py creates the action group that combines the MCP clients into a single interface that the agent can access. This enables the agent to ask your application to invoke either of these MCP servers as needed through return of control. \n  \n# Create action group with both MCP clients\ncost_action_group = ActionGroup(\n    name=\"CostActionGroup\",\n    mcp_clients=[cost_explorer_mcp_client, perplexity_mcp_client]\n)\n \nInline agent creation: The inline agent can be created with the following specifications: \n  \nFoundation model: Configure your choice of FM to power your agent. This can be any model provided on Amazon Bedrock. This example uses Anthropic’s Claude 3.5 Sonnet model.\nAgent instruction: Provide instructions to your agent that contain the guidance and steps for orchestrating responses to user queries. These instructions anchor the agent’s approach to handling various types of queries\nAgent name: Name of your agent.\nAction groups: Define the action groups that your agent can access. These can include single or multiple action groups, with each group having access to multiple MCP clients or AWS Lambda As an option, you can configure your agent to use Code Interpreter to generate, run, and test code for your application.\n \n\n\n# Create and invoke the inline agent\nawait InlineAgent(\n    foundation_model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    instruction=\"\"\"You are a friendly assistant that is responsible for resolving user queries.\n    \n    You have access to search, cost tool and code interpreter. \n    \n    \"\"\",\n    agent_name=\"cost_agent\",\n    action_groups=[\n        cost_action_group,\n        {\n            \"name\": \"CodeInterpreter\",\n            \"builtin_tools\": {\n                \"parentActionGroupSignature\": \"AMAZON.CodeInterpreter\"\n            },\n        },\n    ],\n).invoke(\n    input_text=\"<user-query-here>\"\n)\n\nYou can use this example to build an inline agent on Amazon Bedrock that establishes connections with different MCP servers and groups their clients into a single action group for the agent to access.\nConclusion\nThe Anthropic MCP protocol offers a standardized way of connecting FMs to data sources, and now you can use this capability with Amazon Bedrock Agents. In this post, you saw an example of combining the power of Amazon Bedrock and MCP to build an application that offers a new perspective on understanding and managing your AWS spend.\nOrganizations can now offer their teams natural, conversational access to complex financial data while enhancing responses with contextual intelligence from sources like Perplexity. As AI continues to evolve, the ability to securely connect models to your organization’s critical systems will become increasingly valuable. Whether you’re looking to transform customer service, streamline operations, or gain deeper business insights, the Amazon Bedrock and MCP integration provides a flexible foundation for your next AI innovation. You can dive deeper on this MCP integration by exploring our code samples.\nHere are some examples of what you can build by connecting your Amazon Bedrock Agents to MCP servers:\n\nA multi-data source agent that retrieves data from different data sources such as Amazon Bedrock Knowledge Bases, Sqlite, or even your local filesystem.\nA developer productivity assistant agent that integrates with Slack and GitHub MCP servers.\nA machine learning experiment tracking agent that integrates with the Opik MCP server from Comet ML for managing, visualizing, and tracking machine learning experiments directly within development environments.\n\nWhat business challenges will you tackle with these powerful new capabilities?\n\nAbout the authors\nMark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark’s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification.\nEashan Kaushik is a Specialist Solutions Architect AI/ML at Amazon Web Services. He is driven by creating cutting-edge generative AI solutions while prioritizing a customer-centric approach to his work. Before this role, he obtained an MS in Computer Science from NYU Tandon School of Engineering. Outside of work, he enjoys sports, lifting, and running marathons.\nMadhur Prashant  is an AI and ML Solutions Architect at Amazon Web Services. He is passionate about the intersection of human thinking and generative AI. His interests lie in generative AI, specifically building solutions that are helpful and harmless, and most of all optimal for customers. Outside of work, he loves doing yoga, hiking, spending time with his twin, and playing the guitar.\nAmit Arora is an AI and ML Specialist Architect at Amazon Web Services, helping enterprise customers use cloud-based machine learning services to rapidly scale their innovations. He is also an adjunct lecturer in the MS data science and analytics program at Georgetown University in Washington, D.C.\nAndy Palmer is a Director of Technology for AWS Strategic Accounts. His teams provide Specialist Solutions Architecture skills across a number of speciality domain areas, including AIML, generative AI, data and analytics, security, network, and open source software. Andy and his team have been at the forefront of guiding our most advanced customers through their generative AI journeys and helping to find ways to apply these new tools to both existing problem spaces and net new innovations and product experiences.",
      "date": "2025-04-01",
      "authors": "Mark Roy",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the integration of Amazon Bedrock Agents with Model Context Protocol (MCP) servers, providing a standardized approach for AI agents to access diverse data sources and tools. This integration allows for the creation of dynamic, context-aware AI agents that can enhance business intelligence, particularly in managing and analyzing AWS spending.",
      "takeaways": [
        "- Amazon Bedrock Agents can utilize the MCP to seamlessly connect with multiple data sources and tools, improving the functionality of AI agents.",
        "- The standardized architecture of MCP helps eliminate development bottlenecks by allowing for easier integration of various systems without requiring custom code.",
        "- The potential applications include creating multi-data source agents, developer productivity assistants, and more, showcasing the versatility of the MCP integration in addressing various business challenges."
      ]
    },
    {
      "id": 2,
      "title": "Generate compliant content with Amazon Bedrock and ConstitutionalChain",
      "link": "https://aws.amazon.com/blogs/machine-learning/generate-compliant-content-with-amazon-bedrock-and-constitutionalchain/",
      "description": "Generative AI has emerged as a powerful tool for content creation, offering several key benefits that can significantly enhance the efficiency and effectiveness of content production processes such as creating marketing materials, image generation, content moderation etc. Constitutional AI and LangGraph‘s reflection mechanisms represent two complementary approaches to ensuring AI systems behave ethically – with Anthropic embedding principles during training while LangGraph implements them during inference/runtime through reflection and self-correction mechanisms. By using LanGraph’s Constitutional AI, content creators can streamline their workflow while maintaining high standards of user-defined compliance and ethical integrity. This method not only reduces the need for extensive human oversight but also enhances the transparency and accountability of content generation process by AI.\nIn this post, we explore practical strategies for using Constitutional AI to produce compliant content efficiently and effectively using Amazon Bedrock and LangGraph to build ConstitutionalChain for rapid content creation in highly regulated industries like finance and healthcare. Although AI offers significant productivity benefits, maintaining compliance with strict regulations are crucial. Manual validation of AI-generated content for regulatory adherence can be time-consuming and challenging. We also provide an overview of how Insagic, a Publicis Groupe company, integrated this concept into their existing healthcare marketing workflow using Amazon Bedrock. Insagic is a next-generation insights and advisory business that combines data, design, and dialogues to deliver actionable insights and transformational intelligence for healthcare marketers. It uses expertise from data scientists, behavior scientists, and strategists to drive better outcomes in the healthcare industry.\nUnderstanding Constitutional AI\nConstitutional AI is designed to align large language models (LLMs) with human values and ethical considerations. It works by integrating a set of predefined rules, principles, and constraints into the LLM’s core architecture and training process. This approach makes sure that the LLM operates within specified ethical and legal parameters, much like how a constitution governs a nation’s laws and actions.\nThe key benefits of Constitutional AI for content creation include:\n\nEthical alignment – Content generated using Constitutional AI is inherently aligned with predefined ethical standards\nLegal compliance – The LLM is designed to operate within legal frameworks, reducing the risk of producing non-compliant content\nTransparency – The principles guiding the LLM’s decision-making process are clearly defined and can be inspected\nReduced human oversight – By embedding ethical guidelines into the LLM, the need for extensive human review is significantly reduced\n\nLet’s explore how you can harness the power of Constitutional AI to generate compliant content for your organization.\nSolution overview\nFor this solution, we use Amazon Bedrock Knowledge Bases to store a repository of healthcare documents. We employ a Retrieval Augmented Generation (RAG) approach, first retrieving relevant context and synthesizing an answer based on the retrieved context, to generate articles based on the repository. We then use the open source orchestration framework LangGraph and ConstitutionalChain to generate, critique, and review prompts in an Amazon SageMaker notebook and develop an agentic workflow to generate compliance content. The following diagram illustrates this architecture.\n\nThis implementation demonstrates a sophisticated agentic workflow that not only generates responses based on a knowledge base but also employs a reflection technique to examine its outputs through ethical principles, allowing it to refine and improve its outputs. We upload a sample set of mental health documents to Amazon Bedrock Knowledge Bases and use those documents to write an article on mental health using a RAG-based approach. Later, we define a constitutional principle with a custom Diversity, Equity, and Inclusion (DEI) principle, specifying how to critique and revise responses for inclusivity.\nPrerequisites\nTo deploy the solution, you need the following prerequisites:\n\nAn AWS account\nAppropriate AWS Identity and Access Management (IAM) permissions to access an Amazon Simple Storage Service (Amazon S3) bucket, create Amazon Bedrock knowledge bases, and create a SageMaker notebook instance\n\nCreate an Amazon Bedrock knowledge base\nTo demonstrate this capability, we download a mental health article from the following GitHub repo and store it in Amazon S3. We then use Amazon Bedrock Knowledge Bases to index the articles. By default, Amazon Bedrock uses Amazon OpenSearch Serverless as a vector database. For full instructions to create an Amazon Bedrock knowledge base with Amazon S3 as the data source, see Create a knowledge base in Amazon Bedrock Knowledge Bases.\n\n\n\nOn the Amazon Bedrock console, create a new knowledge base.\nProvide a name for your knowledge base and create a new IAM service role.\nChoose Amazon S3 as the data source and provide the S3 bucket storing the mental health article.\nChoose Amazon Titan Text Embeddings v2 as the embeddings model and OpenSearch Serverless as the vector store.\nChoose Create Knowledge Base.\n \n\nImport statements and set up an Amazon Bedrock client\nFollow the instructions provided in the README file in the GitHub repo. Clone the GitHub repo to make a local copy. We recommend running this code in a SageMaker JupyterLab environment. The following code imports the necessary libraries, including Boto3 for AWS services, LangChain components, and Streamlit. It sets up an Amazon Bedrock client and configures Anthropic’s Claude 3 Haiku model with specific parameters.\n\nimport boto3\nfrom langchain_aws import ChatBedrock\n…\n\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\nllm = ChatBedrock(client=bedrock_runtime, model_id=\"anthropic.claude-3-haiku-20240307-v1:0\")\n…..\n\nDefine Constitutional AI components\nNext, we define a Critique class to structure the output of the critique process. Then we create prompt templates for critique and revision. Lastly, we set up chains using LangChain for generating responses, critiques, and revisions.\n\n# LangChain Constitutional chain migration to LangGraph\n\nclass Critique(TypedDict):\n    \"\"\"Generate a critique, if needed.\"\"\"\n\n    critique_needed: Annotated[bool, ..., \"Whether or not a critique is needed.\"]\n    critique: Annotated[str, ..., \"If needed, the critique.\"]\n\ncritique_prompt = ChatPromptTemplate.from_template(\n    \"Critique this response according to the critique request. \"\n…\n)\n\nrevision_prompt = ChatPromptTemplate.from_template(\n    \"Revise this response according to the critique and reivsion request.\\n\\n\"\n    ….\n)\nchain = llm | StrOutputParser()\ncritique_chain = critique_prompt | llm.with_structured_output(Critique)\nrevision_chain = revision_prompt | llm | StrOutputParser()\n\n\nDefine a State class and refer to the Amazon Bedrock Knowledge Bases retriever\nWe define a LangGraph State class to manage the conversation state, including the query, principles, responses, and critiques:\n\n# LangGraph State\n\nclass State(TypedDict):\n    query: str\n    constitutional_principles: List[ConstitutionalPrinciple]\n\n\nNext, we set up an Amazon Bedrock Knowledge Bases retriever to extract the relevant information. We refer to the Amazon Bedrock knowledge base we created earlier to create an article based on mental health documents. Make sure to update the knowledge base ID in the following code with the knowledge base you created in previous steps:\n\n#-----------------------------------------------------------------\n# Amazon Bedrock KnowledgeBase\n\nfrom langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n\nretriever = AmazonKnowledgeBasesRetriever(\nknowledge_base_id=\"W3NMIJXLUE\", # Change it to your Knowledge base ID\n…\n)\n\nCreate LangGraph nodes and a LangGraph graph along with constitutional principles\nThe next section of code integrates graph-based workflow orchestration, ethical principles, and a user-friendly interface to create a sophisticated Constitutional AI model. The following diagram illustrates the workflow.\n\nIt uses a StateGraph to manage the flow between RAG and critique/revision nodes, incorporating a custom DEI principle to guide the LLM’s responses. The system is presented through a Streamlit application, which provides an interactive chat interface where users can input queries and view the LLM’s initial responses, critiques, and revised answers. The application also features a sidebar displaying a graph visualization of the workflow and a description of the applied ethical principle. This comprehensive approach makes sure that the LLM’s outputs are not only knowledge-based but also ethically aligned by using customizable constitutional principles that guide a reflection flow (critique and revise), all while maintaining a user-friendly experience with features like chat history management and a clear chat option.\nStreamlit application\nThe Streamlit application component of this code creates an interactive and user-friendly interface for the Constitutional AI model. It sets up a side pane that displays a visualization of the LLM’s workflow graph and provides a description of the DEI principle being applied. The main interface features a chat section where users can input their queries and view the LLM’s responses.\n\n# ------------------------------------------------------------------------\n# Streamlit App\n\n# Clear Chat History fuction\ndef clear_screen():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\nwith st.sidebar:\n    st.subheader('Constitutional AI Demo')\n…..\n    ConstitutionalPrinciple(\n        name=\"DEI Principle\",\n        critique_request=\"Analyze the content for any lack of diversity, equity, or inclusion. Identify specific instances where the text could be more inclusive or representative of diverse perspectives.\",\n        revision_request=\"Rewrite the content by incorporating critiques to be more diverse, equitable, and inclusive. Ensure representation of various perspectives and use inclusive language throughout.\"\n    )\n    \"\"\")\n    st.button('Clear Screen', on_click=clear_screen)\n\n# Store LLM generated responses\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\n# Chat Input - User Prompt \nif prompt := st.chat_input():\n….\n\n    with st.spinner(f\"Generating...\"):\n        ….\n    with st.chat_message(\"assistant\"):\n        st.markdown(\"**[initial response]**\")\n….\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"[revised response] \" + generation['response']})\n\n\nThe application maintains a chat history, displaying both user inputs and LLM responses, including the initial response, any critiques generated, and the final revised response. Each step of the LLM’s process is clearly labeled and presented to the user. The interface also includes a Clear Screen button to reset the chat history. When processing a query, the application shows a loading spinner and displays the runtime, providing transparency into the LLM’s operation. This comprehensive UI design allows users to interact with the LLM while observing how constitutional principles are applied to refine the LLM’s outputs.\nTest the solution using the Streamlit UI\nIn the Streamlit application, when a user inputs a query, the application initiates the process by creating and compiling the graph defined earlier. It then streams the execution of this graph, which includes the RAG and critique/revise steps. During this process, the application displays real-time updates for each node’s execution, showing the user what’s happening behind the scenes. The system measures the total runtime, providing transparency about the processing duration. When it’s complete, the application presents the results in a structured manner within the chat interface. It displays the initial LLM-generated response, followed by any critiques made based on the constitutional principles, and finally shows the revised response that incorporates these ethical considerations. This step-by-step presentation allows users to see how the LLM’s response evolves through the constitutional AI process, from initial generation to ethical refinement. As mentioned, in the GitHub README file, in order to run the Streamlit application, use the following code:\n\npip install -r requirements.txt\nstreamlit run main.py\n\n\nFor details on using a Jupyter proxy to access the Streamlit application, refer to Build Streamlit apps in Amazon SageMaker Studio.\nModify the Studio URL, replacing lab? with proxy/8501/.\n\nHow Insagic uses Constitutional AI to generate compliant content\nInsagic uses real-world medical data to help brands understand people as patients and patients as people, enabling them to deliver actionable insights in the healthcare marketing space. Although generating deep insights in the health space can yield profound dividends, it must be done with consideration for compliance and the personal nature of health data. By defining federal guidelines as constitutional principles, Insagic makes sure that the content delivered by generative AI complies with federal guidelines for healthcare marketing.\nClean up\nWhen you have finished experimenting with this solution, clean up your resources to prevent AWS charges from being incurred:\n\nEmpty the S3 buckets.\nDelete the SageMaker notebook instance.\nDelete the Amazon Bedrock knowledge base.\n\nConclusion\nThis post demonstrated how to implement a sophisticated generative AI solution using Amazon Bedrock and LangGraph to generate compliant content. You can also integrate this workflow to generate responses based on a knowledge base and apply ethical principles to critique and revise its outputs, all within an interactive web interface. Insagic is looking at more ways to incorporate this into existing workflows by defining custom principles to achieve compliance goals.\nYou can expand this concept further by incorporating Amazon Bedrock Guardrails. Amazon Bedrock Guardrails and LangGraph Constitutional AI can create a comprehensive safety system by operating at different levels. Amazon Bedrock provides API-level content filtering and safety boundaries, and LangGraph implements constitutional principles in reasoning workflows. Together, they enable multi-layered protection through I/O filtering, topic restrictions, ethical constraints, and logical validation steps in AI applications.\nTry out the solution for your own use case, and leave your feedback in the comments.\n\nAbout the authors\n Sriharsh Adari is a Senior Solutions Architect at Amazon Web Services (AWS), where he helps customers work backwards from business outcomes to develop innovative solutions on AWS. Over the years, he has helped multiple customers on data platform transformations across industry verticals. His core area of expertise include Technology Strategy, Data Analytics, and Data Science. In his spare time, he enjoys playing sports, binge-watching TV shows, and playing Tabla.\nDavid Min is a Senior Partner Sales Solutions Architect at Amazon Web Services (AWS) specializing in Generative AI, where he helps customers transform their businesses through innovative AI solutions. Throughout his career, David has helped numerous organizations across industries bridge the gap between cutting-edge AI technology and practical business applications, focusing on executive engagement and successful solution adoption.\nStephen Garth is a Data Scientist at Insagic, where he develops advanced machine learning solutions, including LLM-powered automation tools and deep clustering models for actionable, consumer insights. With a strong background spanning software engineering, healthcare data science, and computational research, he is passionate to bring his expertise in AI-driven analytics and large-scale data processing to drive solutions.\nChris Cocking specializes in scalable enterprise application design using multiple programming languages. With a nearly 20 years of experience, he excels in LAMP and IIS environments, SEO strategies, and most recently designing agentic systems. Outside of work, Chris is an avid bassist and music lover, which helps fuel his creativity and problem-solving skills.",
      "date": "2025-04-01",
      "authors": "Sriharsh Adari",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the use of Amazon Bedrock and Constitutional AI to generate compliant content in regulated industries like healthcare and finance. It outlines how the integration of constitutional principles can ensure that AI-generated content adheres to ethical and legal standards while streamlining content creation processes.",
      "takeaways": [
        "- Constitutional AI aligns large language models with ethical and legal guidelines, enhancing compliance in content generation.",
        "- The integration of tools like LangGraph allows for the generation and critique of AI content, ensuring it meets predefined diversity, equity, and inclusion principles.",
        "- Amazon Bedrock's architecture supports a Retrieval Augmented Generation approach, enabling efficient content creation from a knowledge base while maintaining ethical oversight."
      ]
    },
    {
      "id": 3,
      "title": "Minimize generative AI hallucinations with Amazon Bedrock Automated Reasoning checks",
      "link": "https://aws.amazon.com/blogs/machine-learning/minimize-generative-ai-hallucinations-with-amazon-bedrock-automated-reasoning-checks/",
      "description": "Foundation models (FMs) and generative AI are transforming enterprise operations across industries. McKinsey & Company’s recent research estimates generative AI could contribute up to $4.4 trillion annually to the global economy through enhanced operational efficiency, productivity growth of 0.1% to 0.6% annually, improved customer experience through personalized interactions, and accelerated digital transformation.\nToday, organizations struggle with AI hallucination when moving generative AI applications from experimental to production environments. Model hallucination, where AI systems generate plausible but incorrect information, remains a primary concern. The 2024 Gartner CIO Generative AI Survey highlights three major risks: reasoning errors from hallucinations (59% of respondents), misinformation from bad actors (48%), and privacy concerns (44%).\nTo improve factual accuracy of large language model (LLM) responses, AWS announced Amazon Bedrock Automated Reasoning checks (in gated preview) at AWS re:Invent 2024. Through logic-based algorithms and mathematical validation, Automated Reasoning checks validate LLM outputs against domain knowledge encoded in the Automated Reasoning policy to help prevent factual inaccuracies. Automated reasoning checks is part of Amazon Bedrock Guardrails, a comprehensive framework that also provides content filtering, personally identifiable information (PII) redaction, and enhanced security measures. Together, these capabilities enable organizations to implement reliable generative AI safeguards—with Automated Reasoning checks addressing factual accuracy while other Amazon Bedrock Guardrails features help protect against harmful content and safeguard sensitive information.\nIn this post, we discuss how to help prevent generative AI hallucinations using Amazon Bedrock Automated Reasoning checks.\nAutomated Reasoning overview\nAutomated Reasoning is a specialized branch of computer science that uses mathematical proof techniques and formal logical deduction to verify compliance with rules and requirements with absolute certainty under given assumptions. As organizations face increasing needs to verify complex rules and requirements with mathematical certainty, automated reasoning techniques offer powerful capabilities. For example, AWS customers have direct access to automated reasoning-based features such as IAM Access Analyzer, S3 Block Public Access, or VPC Reachability Analyzer.\nUnlike probabilistic approaches prevalent in machine learning, Automated Reasoning relies on formal mathematical logic to provide definitive guarantees about what can and can’t be proven. This approach mirrors the rigors of auditors verifying financial statements or compliance officers validating regulatory requirements, but with mathematical precision. By using rigorous logical frameworks and theorem-proving methodologies, Automated Reasoning can conclusively determine whether statements are true or false under given assumptions. This makes it exceptionally valuable for applications that demand high assurance and need to deliver unambiguous conclusions to their users.\nThe following workflow illustrates solver-based formal verification, showing both the process flow and algorithm for verifying formal system properties through logical analysis and SAT/SMT solvers.\n\nOne of the widely used Automated Reasoning techniques is SAT/SMT solving, which involves encoding a representation of rules and requirements into logical formulas. A logical formula is a mathematical expression that uses variables and logical operators to represent conditions and relationships. After the rules and requirements are encoded into these formulas, specialized tools known as solvers are applied to compute solutions that satisfy these constraints. These solvers determine whether the formulas can be satisfied—whether there exist values for variables that make the formulas true.\nThis process starts with two main inputs: a formal representation of the system (like code or a policy) expressed as logical formulas, and a property to analyze (such as whether certain conditions are possible or requirements can be met). The solver can return one of three possible outcomes:\n\nSatisfiable – The solver finds an assignment of values that makes the formulas true, proving that the system can satisfy the given requirements. The solver provides this assignment, which can serve as a concrete example of correct behavior.\nUnsatisfiable – The solver proves that no assignment exists that make all formulas true, proving that the requirements can’t be met. This often comes with information about which constraints are in conflict, helping identify the incorrect assumptions in the system.\nUnknown – In some cases, the solver might not be able to determine satisfiability within reasonable computational limits, or the encoding might not contain enough information to reach a conclusion.\n\nThis technique makes sure that you either get confirmation that the specific property holds (with a concrete example), proof that it can’t be satisfied (with information on conflicting constraints), or an indication that the problem needs to be reformulated or analyzed differently.\nKey features of Automated Reasoning checks\nAutomated Reasoning checks offer the following key features:\n\nMathematical validation framework – The feature verifies LLM outputs using mathematical logical deduction. Unlike probabilistic methods, it uses sound mathematical approaches to provide definitive guarantees about system behaviors within defined parameters.\nPolicy-based knowledge representation – Organizations can create Automated Reasoning policies that encode their rules, procedures, and guidelines into structured, mathematical formats. Organizations can upload documents like PDFs containing HR guidelines or operational workflows, which are then automatically converted into formal logic structures. Policy changes are automatically versioned with unique Amazon Resource Names (ARNs), allowing for change tracking, auditing, and rollback capabilities to maintain consistent policy enforcement.\nDomain expert enablement – The feature is designed to empower domain experts, such as HR personnel or operational managers, to directly encode their knowledge without technical intermediaries. This makes sure that business rules and policies are accurately captured and maintained by those who understand them best.\nNatural language to logic translation – The system uses two complementary approaches: LLMs handle natural language understanding, and a symbolic reasoning engine performs mathematical validation. This hybrid architecture allows users to input policies in plain language while maintaining mathematically rigorous verification.\nExplainable validation results – Each validation check produces detailed findings that indicate whether content is Valid, Invalid, or No Data. The feature provides clear explanations for its decisions, including extracted factual statements, and suggested corrections for invalid content.\nInteractive testing environment – Users can access a chat playground on the Amazon Bedrock console to test and refine policies in real time. The feature supports both interactive testing through the Amazon Bedrock console and automated testing through API integrations, with the ability to export test cases in JSON format for integration into continuous testing pipelines or documentation workflows.\nSeamless AWS integration – The feature integrates directly with Amazon Bedrock Guardrails and can be used alongside other configurable guardrails like Contextual Grounding checks. It can be accessed through both the Amazon Bedrock console and APIs, making it flexible for various implementation needs.\n\nThese features combine to create a powerful framework that helps organizations maintain factual accuracy in their AI applications while providing transparent and mathematically sound validation processes.\nSolution overview\nNow that we understand the key features of Automated Reasoning checks, let’s examine how this capability works within Amazon Bedrock Guardrails. The following section provides a comprehensive overview of the architecture and demonstrates how different components work together to promote factual accuracy and help prevent hallucinations in generative AI applications.\nAutomated Reasoning checks in Amazon Bedrock Guardrails provides an end-to-end solution for validating AI model outputs using mathematically sound principles. This automated process uses formal logic and mathematical proofs to verify responses against established policies, offering definitive validation results that can significantly improve the reliability of your AI applications.\nThe following solution architecture follows a systematic workflow that enables rigorous validation of model outputs.\n\nThe workflow consists of the following steps:\n\nSource documents (such as HR guidelines or operational procedures) are uploaded to the system. These documents, along with optional intent descriptions, are processed to create structured rules and variables that form the foundation of an Automated Reasoning policy.\nSubject matter experts review and inspect the created policy to verify accurate representation of business rules. Each validated policy is versioned and assigned a unique ARN for tracking and governance purposes.\nThe validated Automated Reasoning policy is associated with Amazon Bedrock Guardrails, where specific policy versions can be selected for implementation. This integration enables automated validation of generative AI outputs.\nWhen the generative AI application produces a response, Amazon Bedrock Guardrails triggers the Automated Reasoning checks. The system creates logical representations of both the input question and the application’s response, evaluating them against the established policy rules.\nThe Automated Reasoning check provides detailed validation results, including whether statements are Valid, Invalid, or No Data. For each finding, it explains which rules and variables were considered, and provides suggestions for making invalid statements valid.\n\nWith this solution architecture in place, organizations can confidently deploy generative AI applications knowing that responses will be automatically validated against your established policies using mathematically sound principles.\nPrerequisites\nTo use Automated Reasoning checks in Amazon Bedrock, make sure you have met the following prerequisites:\n\nAn active AWS account\nAccess permission through your AWS Account Manager, because Automated Reasoning checks is currently in gated preview\nConfirmation of AWS Regions where Automated Reasoning checks is available\n\nInput dataset\nFor this post, we examine a sample Paid Leave of Absence (LoAP) policy document as our example dataset. This policy document contains detailed guidelines covering employee eligibility criteria, duration limits, application procedures, and benefits coverage for paid leave. It’s an ideal example to demonstrate how Automated Reasoning checks can validate AI-generated responses against structured business policies, because it contains clear rules and conditions that can be converted into logical statements. The document’s mix of quantitative requirements (such as minimum tenure and leave duration) and qualitative conditions (like performance status and approval processes) makes it particularly suitable for showcasing the capabilities of automated reasoning validation.\nThe following screenshot shows an example of our policy document.\n\nStart an Automated Reasoning check using the Amazon Bedrock console\nThe first step is to encode your knowledge—in this case, the sample LoAP policy—into an Automated Reasoning policy. Complete the following steps to initiate an Automated Reasoning check using the Amazon Bedrock console:\n\nOn the Amazon Bedrock console, choose Automated Reasoning Preview under Safeguards in the navigation pane.\nChoose Create policy.\n\n\n\nProvide a policy name and policy description.\n\n\n\nUpload your source document. The source content can’t be modified after creation and must not exceed 6,000 characters with limitations on table sizes and image processing.\nInclude a description of the intent of the Automated Reasoning policy you’re creating. For the sample policy, you can use the following intent:\n\n\nCreate a logical model of the Leave of Absence, Paid (LoAP) policy in this document.\nEmployees will ask questions about what are the eligibility requirements for the program,\nwhether they are allowed to take LOAP and for how long, duration and benefits during the\ntime off, and return to work. \nBelow is an example question:\nQUESTION: I am a temporary contractor working in operations. Am I eligible for LOAP?\nANSWER: No, only full-time employees are eligible for LoAP.\n\nThe policy creation process takes a few minutes to complete. The rules and variables are created after creating the policy and they can be edited, removed, or have new rules or variables added to them.\n\nThe policy document version is outlined in the details section along with the intent description and build status.\n\nNext, you create a guardrail in Amazon Bedrock by configuring as many filters as you need.\n\nOn the Amazon Bedrock console, choose Guardrails under Safeguards in the navigation pane.\nChoose Create guardrail.\n\n\n\nProvide guardrail details such as a name and an optional description.\n\n\n\nAdd an Automated Reasoning check by choosing Enable Automated Reasoning policy, and choose the policy name and version.\nChoose Next and complete the creation of the guardrail.\n\n\n\nNavigate back to the Automated Reasoning section of the Amazon Bedrock console and open the newly created policy. You can use the test playground and input sample questions and answers that represent real user interactions with your LLM.\nChoose the guardrail you created, then choose Submit to evaluate how your policy handles these exchanges.\n\n\nAfter submitting, you’ll be presented with one or more findings. A finding contains a set of facts that were extracted from the input Q&A and are analyzed independently. Each finding includes four key components:\n\nValidation results – Shows the outcome of Automated Reasoning checks. The system determines these results by evaluating extracted variable assignments against your defined policy rules.\nApplied rules – Displays the specific rules from your policy that were used to reach the validation conclusion.\nExtracted variables – Lists the variables that were identified and used in the validation process.\nSuggestions – Shows variable assignments that would make invalid responses valid, or for valid responses, identifies necessary assumptions. These can be used to generate feedback for your LLM.\n\n\nFinally, you can use the feedback suggestions to improve your LLM’s responses.\n\nCollect rules from valid results with suggestions and invalid results.\nFeed these collected variables and rules back to your LLM to revise its original.\nRefine your policy: \n  \nEdit incorrect rules using natural language.\nImprove variable descriptions when Automated Reasoning checks fail to assign values.\nFor effective variable descriptions, include both technical definitions and common user expressions. For example, for a variable named is_full_time, \"works more than 20 hours per week\" is technically correct because it’s a quote from the source policy, but won’t help Automated Reasoning checks understand what users mean when they say “part-time.” Instead, use \"works more than 20 hours per week; set to true if user says 'full-time' and false if user says 'part-time'\".\n \n\nStart an Automated Reasoning check using Python SDK and APIs\nFirst, you need to create an Automated Reasoning policy from your documents using the Amazon Bedrock console as outlined in the previous section. Next, you can use the policy created with the ApplyGuardrail API to validate your generative AI application.\nTo use the Python SDK for validation using Automated Reasoning checks, follow these steps:\n\nFirst, set up the required configurations:\n\n\nimport boto3\nimport botocore\nimport os\nimport json\n\n# Configuration parameters\nDEFAULT_GUARDRAIL_NAME = \"<YOUR_GUARDRAIL_NAME>\"  # e.g., \"my_policy_guardrail\"\nDEFAULT_AR_POLICY_VERSION = \"1\"\n\n# AWS configuration\nregion = \"us-west-2\"\nar_policy = \"<YOUR_AR_POLICY_ID>\"  # e.g., \"ABC123DEF456\"\nmodel_id = \"<YOUR_MODEL_ID>\"  # e.g., \"anthropic.claude-3-haiku-20240307-v1:0\"\n\n\nBefore using Amazon Bedrock with Automated Reasoning policies, you will need to load the required service models. After being allowlisted for Amazon Bedrock access, you will receive two model files along with their corresponding version information. The following is a Python script to help you load these service models:\n\n\n\ndef add_service_model(model_file, service_name, version):\n    \"\"\"\n    Adds a service model to the AWS configuration directory.\n    \n    Args:\n        model_file (str): Path to the model file\n        service_name (str): Name of the AWS service\n        version (str): Service model version\n    \"\"\"\n    # Configure paths\n    source = f\"models/{model_file}\"  # Your downloaded model files directory\n    dest_dir = os.path.expanduser(f\"~/.aws/models/{service_name}/{version}\")\n    dest_file = f\"{dest_dir}/service-2.json\"\n\n    try:\n        # Create directory and copy model file\n        os.makedirs(dest_dir, exist_ok=True)\n        with open(source) as f:\n            model = json.load(f)\n        with open(dest_file, 'w') as f:\n            json.dump(model, f, indent=2)\n        print(f\"Successfully added model for {service_name}\")\n        return True\n    except Exception as e:\n        print(f\"Error adding {service_name} model: {e}\")\n        return False\n\ndef main():\n    # Define your model files and versions\n    # Replace with your actual model information provided by AWS\n    models = {\n        '<bedrock-model-file>.json': ('bedrock', '<bedrock-version>'),\n        '<runtime-model-file>.json': ('bedrock-runtime', '<runtime-version>')\n    }\n    \n    # Load each model\n    for model_file, (service_name, version) in models.items():\n        add_service_model(model_file, service_name, version)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nAfter you set up the service models, initialize the AWS clients for both Amazon Bedrock and Amazon Bedrock Runtime services. These clients will be used to interact with the models and apply guardrails.\n\n\n# Initialize AWS clients\nboto_session = boto3.Session(region_name=region)\nruntime_client = boto_session.client(\"bedrock-runtime\")\nbedrock_client = boto_session.client(\"bedrock\")\n\n\nBefore applying Automated Reasoning policies, you need to either locate an existing guardrail or create a new one. The following code first attempts to find a guardrail by name, and if not found, creates a new guardrail with the specified Automated Reasoning policy configuration. This makes sure you have a valid guardrail to work with before proceeding with policy enforcement.\n\n\ndef find_guardrail_id(client, name) -> tuple[str, str]:\n    \"\"\"\n    Finds the ID and version of a guardrail by its name.\n    \n    Args:\n        client: The Bedrock client object\n        name (str): Name of the guardrail to find\n    \n    Returns:\n        tuple[str, str]: Guardrail ID and version if found, None otherwise\n    \"\"\"\n    next_token = None\n    while True:\n        # List existing guardrails\n        resp = client.list_guardrails(\n        ) if next_token is None else client.list_guardrail(nextToken=next_token)\n\n        # Search for matching guardrail\n        for g in resp[\"guardrails\"]:\n            if g[\"name\"] == name:\n                return g[\"id\"], g[\"version\"]\n\n        # Handle pagination\n        if \"nextToken\" in resp and resp[\"nextToken\"] != \"\":\n            next_token = resp[\"nextToken\"]\n        else:\n            break\n    return None, None\n\n# Find or create guardrail with AR policy\ntry:\n    # First, try to find existing guardrail\n    guardrail_id, guardrail_version = find_guardrail_id(\n        bedrock_client, DEFAULT_GUARDRAIL_NAME)\n    \n    # If not found, create new guardrail\n    if guardrail_id is None:\n        create_resp = bedrock_client.create_guardrail(\n            name=DEFAULT_GUARDRAIL_NAME,\n            description=\"Automated Reasoning checks demo guardrail\",\n            automatedReasoningPolicyConfig={\n                \"policyIdentifier\": ar_policy,\n                \"policyVersion\": DEFAULT_AR_POLICY_VERSION\n            },\n            blockedInputMessaging='Input is blocked',\n            blockedOutputsMessaging='Output is blocked',\n        )\n        guardrail_id = create_resp[\"guardrailId\"]\n        guardrail_version = create_resp[\"version\"]\n        print(f\"✓ Created new guardrail: {guardrail_id}\")\n    else:\n        print(f\"✓ Found existing guardrail: {guardrail_id}\")\n        \nexcept botocore.exceptions.ClientError as e:\n    print(f\"✗ Error managing guardrail: {str(e)}\")\n    raise\n\n\nWhen testing guardrails with Automated Reasoning policies, you need to properly format your input data. The following code shows how to structure a sample question and answer pair for validation:\n\n\ndef create_sample_input():\n    \"\"\"\n    Creates a formatted sample input for guardrail validation.\n    \n    The format requires both the query and response to be properly structured\n    with appropriate qualifiers.\n    \n    Returns:\n        list: Formatted input for guardrail validation\n    \"\"\"\n    sample_query = \"I am a part-time employee, am I eligible for LoAP?\"\n    sample_response = \"Yes, part time employees are allowed to use LoAP\"\n    \n    return [\n        {\n            \"text\": {\n                \"text\": sample_query,\n                \"qualifiers\": [\"query\"]\n            }\n        },\n        {\n            \"text\": {\n                \"text\": sample_response,\n                \"qualifiers\": [\"guard_content\"]\n            }\n        }\n    ]\n\n# Example usage\nguardrail_input = create_sample_input()\n\nprint(json.dumps(guardrail_input, indent=2))\n\n\nNow that you have your formatted input data, you can apply the guardrail with Automated Reasoning policies to validate the content. The following code sends the input to Amazon Bedrock Guardrails and returns the validation results:\n\n\nguardrails_output = runtime_client.apply_guardrail(\n            guardrailIdentifier= guardrail_id,\n            guardrailVersion= guardrail_version,\n            source=\"OUTPUT\",\n            content=guardrail_input,\n        )\n\n\nAfter applying guardrails, you need to extract and analyze the Automated Reasoning assessment results. The following code shows how to process the guardrail output:\n\n\n# Extract Automated Reasoning assessment\nar_assessment = None\nfor assessment in guardrails_output[\"assessments\"]:\n    if \"automatedReasoningPolicy\" in assessment:\n        ar_assessment = assessment[\"automatedReasoningPolicy\"][\"findings\"]\n        break\n\nif ar_assessment is None:\n    print(\"No Automated Reasoning assessment found\")\nelse:\n    print(\"Automated Reasoning Assessment Results:\")\n    print(json.dumps(ar_assessment, indent=2))\n\n    # Process any policy violations\n    for finding in ar_assessment:\n        if finding[\"result\"] == \"INVALID\":\n            print(\"\\nPolicy Violations Found:\")\n            # Print violated rules\n            for rule in finding.get(\"rules\", []):\n                print(f\"Rule: {rule['description']}\")\n            \n            # Print suggestions if any\n            if \"suggestions\" in finding:\n                print(\"\\nSuggested Corrections:\")\n                for suggestion in finding[\"suggestions\"]:\n                    print(f\"- {suggestion}\")\n\nThe output will look something like the following:\n\n{\n    \"result\": \"INVALID\",\n    \"assignments\": [...],\n    \"suggestions\": [...],\n    \"rules\": [\n        {\n            \"identifier\": \"<IDENTIFIER>\",\n            \"description\": \"An employee is eligible for LoAP if and only if...\"\n        }\n    ]\n}\n\nWhen a response violates AR policies, the system identifies which rules were violated and provides information about the conflicts. The feedback from the AR policy validation can be routed back to improve the model’s output, promoting compliance while maintaining response quality.\nPossible use cases\nAutomated Reasoning checks can be applied across various industries to promote accuracy, compliance, and reliability in AI-generated responses while maintaining industry-specific standards and regulations. Although we have tested these checks across multiple applications, we continue to explore additional potential use cases. The following table provides some applications across different sectors.\n\n\n\nIndustry\nUse Cases\n\n\nHealthcare\n\n\nValidate AI-generated treatment recommendations against clinical care protocols and guidelines\nVerify medication dosage calculations and check for potential drug interactions\nMake sure patient education materials align with medical best practices\nValidate clinical documentation for regulatory compliance\n \n\n\nFinancial Services\n\n\nVerify investment recommendations against regulatory requirements and risk policies\nValidate customer communications for compliance with financial regulations\nVerify that credit decision explanations meet fairness and transparency guidelines\nCheck transaction processing against anti-fraud and anti-money laundering policies\n \n\n\nTravel and Hospitality\n\n\nValidate booking and ticketing policies for accuracy\nVerify loyalty program benefit calculations follow established rules\nVerify travel documentation requirements and restrictions\nValidate pricing and refund calculations\n \n\n\nInsurance\n\n\nVerify claim processing decisions against policy terms\nValidate coverage explanations for accuracy and completeness\nMake sure that risk assessment recommendations follow underwriting guidelines\nCheck policy documentation for regulatory compliance\n \n\n\nEnergy and Utilities\n\n\nValidate maintenance scheduling against equipment specifications\nVerify emergency response protocols for different scenarios\nMake sure that field operation instructions follow safety guidelines\nCheck grid management decisions against operational parameters\n \n\n\nManufacturing\n\n\nValidate quality control procedures against industry standards\nVerify production scheduling against capacity and resource constraints\nMake sure that safety protocols are followed in operational instructions\nCheck inventory management decisions against supply chain policies\n \n\n\n\nBest practices for implementation\nSuccessfully implementing Automated Reasoning checks requires careful attention to detail and a systematic approach to achieve optimal validation accuracy and reliable results. The following are some key best practices:\n\nDocument preparation – Use structured text-based PDF documents. Content should be limited to 6,000 characters. Avoid complex formatting that could interfere with the logical model generation.\nIntent description engineering – Create precise policy intents using a clear format. The intent should comprehensively cover expected use cases and potential edge cases. For example: \n  \nCreate a logical model for [USE CASE] with policy rules. \nUsers will ask questions about [SPECIFIC TOPICS].\nExample Q&A: [INCLUDE SAMPLE].\n \nPolicy validation – Review the generated rules and variables to make sure they accurately capture your business logic and policy requirements. Regular audits of these rules help maintain alignment with current business policies.\nComprehensive testing –Develop a diverse set of sample Q&As in the test playground to evaluate different validation scenarios (valid, valid with suggestions, and invalid responses). Include edge cases and complex scenarios to provide robust validation coverage.\nIterative improvement –Regularly update rules and LLM applications based on validation feedback, paying special attention to suggested variables and invalid results to enhance response accuracy. Maintain a feedback loop for continuous refinement.\nVersion control management – Implement a systematic approach to policy versioning, maintaining detailed documentation of changes and conducting proper testing before deploying new versions. This helps track policy evolution and facilitates rollbacks if needed.\nError handling strategy – Develop a comprehensive plan for handling different validation results, including specific procedures for managing invalid responses and incorporating suggested improvements into the response generation process.\nRuntime optimization – Understand and monitor the two-step validation process (fact extraction and logic validation) to achieve optimal performance. Regularly review validation results to identify patterns that might indicate needed improvements in variable descriptions or rule definitions.\nFeedback integration – Establish a systematic process for collecting and analyzing validation feedback, particularly focusing on cases where NO_DATA is returned or when factual claims are incorrectly extracted. Use this information to continuously refine variable descriptions and policy rules.\n\nConclusion\nAmazon Bedrock Automated Reasoning checks represent a significant advancement in formally verifying the outputs of generative AI applications. By combining rigorous mathematical validation with a user-friendly interface, this feature addresses one of the most critical challenges in AI deployment: maintaining factual consistency and minimizing hallucinations. The solution’s ability to validate AI-generated responses against established policies using formal logic provides organizations with a powerful framework for building trustworthy AI applications that can be confidently deployed in production environments.\nThe versatility of Automated Reasoning checks, demonstrated through various industry use cases and implementation approaches, makes it a valuable tool for organizations across sectors. Whether implemented through the Amazon Bedrock console or programmatically using APIs, the feature’s comprehensive validation capabilities, detailed feedback mechanisms, and integration with existing AWS services enable organizations to establish quality control processes that scale with their needs. The best practices outlined in this post provide a foundation for organizations to maximize the benefits of this technology while maintaining high standards of accuracy.\nAs enterprises continue to expand their use of generative AI, the importance of automated validation mechanisms becomes increasingly critical. We encourage organizations to explore Amazon Bedrock Automated Reasoning checks and use its capabilities to build more reliable and accurate AI applications. To help you get started, we’ve provided detailed implementation guidance, practical examples, and a Jupyter notebook with code snippets in our GitHub repository that demonstrate how to effectively integrate this feature into your generative AI development workflow. Through systematic validation and continuous refinement, organizations can make sure that their AI applications deliver consistent, accurate, and trustworthy results.\n\nAbout the Authors\nAdewale Akinfaderin is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.\nNafi Diallo is a Sr. Applied Scientist in the Automated Reasoning Group and holds a PhD in Computer Science. She is passionate about using automated reasoning to ensure the security of computer systems, improve builder productivity, and enable the development of trustworthy and responsible AI workloads. She worked for more than 5 years in the AWS Application Security organization, helping build scalable API security testing solutions and shifting security assessment left.",
      "date": "2025-04-01",
      "authors": "Adewale Akinfaderin",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the launch of Amazon Bedrock's Automated Reasoning checks, a feature designed to reduce generative AI hallucinations by ensuring factual accuracy in large language model (LLM) outputs through mathematical validation mechanisms. This innovative approach utilizes formal logic to validate AI-generated responses against established policies, providing organizations with a robust framework for trustworthy AI implementations.",
      "takeaways": [
        "- Automated Reasoning checks leverage formal mathematical logic to prevent AI model hallucinations, offering definitive validation results that improve the reliability of AI applications.",
        "- The system enables domain experts to encode business rules into structured formats, ensuring accurate representation of organizational policies without needing technical intermediaries.",
        "- Integration with Amazon Bedrock Guardrails allows for comprehensive validation processes, helping organizations maintain compliance and accuracy in AI-generated responses across various industries."
      ]
    },
    {
      "id": 4,
      "title": "AWS App Studio introduces a prebuilt solutions catalog and cross-instance Import and Export",
      "link": "https://aws.amazon.com/blogs/machine-learning/aws-app-studio-introduces-a-prebuilt-solutions-catalog-and-cross-instance-import-and-export/",
      "description": "AWS App Studio is a generative AI-powered service that uses natural language to build business applications, empowering a new set of builders to create applications in minutes. With App Studio, technical professionals such as IT project managers, data engineers, enterprise architects, and solution architects can quickly develop applications tailored to their organization’s needs—without requiring deep software development skills. Common use cases range from inventory management and approval workflows to content management and operational portals, and beyond – App Studio adapts to streamline a wide variety of business applications.\nSince the general availability of App Studio in November 2024, customers across diverse industries have adopted it to build scalable, enterprise-grade applications, transforming their development processes and accelerating time-to-market. App Studio customers, including both enterprises and system integrators, have shared the need for portability and reusability across App Studio instances. Based on their experience, two areas of interests emerged:\n\nGetting started – New customers and builders asked to learn and explore the product through readily available examples and patterns that explain application building possibilities in App Studio.\nOptimizing time to value – Teams often validate use cases in a sandbox before moving to production. This highlights an interest in a more efficient approach to share and deploy applications across multiple App Studio instances.\n\nToday, App Studio announced two new features to accelerate application building:\n\nPrebuilt solutions catalog – Featuring a set of practical examples and common patterns (like S3 and Bedrock integration) to accelerate getting started and enable deployment of applications from the catalog to production environments in less than 15 minutes.\nCross-instance Import and Export – Enabling straightforward and self-service migration of App Studio applications across AWS Regions and AWS accounts.\n\nIn this post, we walk through how to use the prebuilt solutions catalog to get started quickly and use the Import and Export feature\nPrerequisites\nTo follow along with this post, you should have the following prerequisites:\n\nAccess to App Studio. For more information, see Setting up and signing in to App Studio.\nOptional: Review App Studio concepts to familiarize yourself with important App Studio concepts.\nOptional: An understanding of basic web development concepts, such as JavaScript syntax.\nOptional: Familiarity with AWS services.\n\nPrebuilt solutions catalog\nApp Studio is introducing a prebuilt solutions catalog to accelerate the way builders approach application building. This resource offers a diverse collection of prebuilt applications that can be seamlessly imported into your App Studio instance, serving as both a learning tool and a rapid deployment solution. By providing access to proven patterns and prebuilt solutions, App Studio significantly reduces the initial setup time for builders, enabling you to move from concept to production in less than 15 minutes.\nThe catalog includes a variety of practical use cases including a Product Adoption Tracker to manage customer feedback, track feature requests, and summarize meeting notes with AI. To import the Product Adoption Tracker, navigate to the prebuilt solutions catalog, copy an import code, and follow the import instructions in the next section.\n\nImport an application\nYou now have the ability to import an App Studio application from a different App Studio instance. Importing applications is available to all builders and admins.\nComplete the following steps to import an App Studio application:\n\nSign in and launch the App Studio instance where you want to import an application.\nChoose My applications in the navigation pane.\nChoose the dropdown menu next to Create app and choose Import app. \n\n\nEnter an import code from the prebuilt app catalog or that you generated by the export process outlined in the next section and choose Import. Depending on the application size, you might need to wait a few seconds for the import to finish. \nAfter completion, the application will be imported to your development environment. You can explore the debug panel at the bottom of the page to understand which custom connectors need to be connected to automations and entities. \n\nNow that we have successfully imported an application, let’s walk through how we can export our own applications to a different App Studio instance.\nExport an application\nYou now have the ability to export an App Studio application to a different App Studio instance. Generating an application export creates a static snapshot with all artifacts needed to recreate the application—automations, components, and entities. After importing, you will need to reconnect custom connectors to automations and entities.\nApplication security and control are maintained through a robust permissions system. Only authorized application owners and co-owners can generate application exports and restrict which App Studio instances can import a given application. If needed, application owners can revoke access by deactivating the import link at any time.\nTo export an App Studio application, complete the following steps:\n\n\n\nSign in to the App Studio instance that you want to export an application from.\nChoose My applications in the navigation pane.\nChoose the dropdown menu next to Edit and choose Export. \nTo restrict which App Studio instances can import this application, configure application import permissions: \n    \nAnyone with the import code can import this application – Grant import permissions to all instances. Only select this option if you want anyone with the import code to have access to import your application.\nOnly specified App Studio instances can import this application – Provide the specific instance IDs that can import the application (multiple instances can be separated by commas). To find your instance ID, navigate to your instance’s account settings by choosing Account settings on the App Studio console.\n \nChoose Generate import code to generate a unique import code. \nTwo additional options for managing import codes are available after the application has been exported at least once to application owners and co-owners: \n    \nGenerate new import code – When you make updates to this application, you will need to generate a new import code by choosing Generate new code. Generating a new code invalidates the old code, but will not automatically refresh existing imported applications.\nDelete import code – To stop application import access, choose this option. Deleting the import code will invalidate the current code and prevent subsequent import attempts. Applications previously created using this code will continue to work.\n \n \n\n\nConsiderations\nThe following are some key considerations for using the prebuilt solutions catalog and importing and exporting applications across App Studio instances:\n\nThere is no cost associated with importing and exporting applications, including importing applications from the prebuilt solutions catalog.\nApplications cannot be imported into the same instance, but you can achieve a similar result of replicating functionality within an instance by duplicating apps, components, and pages.\nThere are no limits on the number of applications you can import or export. The maximum number of applications in an App Studio instance is subject to service quotas.\n\nConclusion\nJumpstart your app building workflow with App Studio’s prebuilt solutions catalog and Import and Export features. Effortlessly migrate applications across AWS instances, collaborate with teams, and transfer applications to clients. Start using App Studio’s prebuilt solutions catalog and Import and Export features today – we’re excited to see how you will use these features to accelerate your application building journey.\nTo learn more about App Studio, explore more features on the App Studio page. Get started with App Studio in the AWS Management Console. Experience the App Studio workshop for hands-on learning, and join the conversation in the #aws-app-studio channel in the AWS Developers Slack workspace.\nRead more about App Studio\n\nBuild and modify apps using natural language with App Studio, now generally available\nDiscover how customers are innovating with App Studio\n\nWatch App Studio demos\n\nThe Fastest and Easiest Way to Build Business Applications\nWatch how the NFL accelerated image processing with App Studio\n\n\n\nAbout the Authors\nUmesh Kalaspurkar is a Principal Solutions Architect at AWS based in New York, bringing over two decades of expertise in digital transformation and innovation across both enterprise and startup environments. He specializes in designing solutions that help organizations overcome their most pressing challenges. When not architecting cloud solutions, Umesh cherishes time spent with his children, carving down ski slopes, and exploring new destinations around the world.\nSamit Kumbhani is an AWS Senior Solutions Architect in the New York City area with over 18 years of experience. He currently partners with independent software vendors (ISVs) to build highly scalable, innovative, and secure cloud solutions. Outside of work, Samit enjoys playing cricket, traveling, and biking.\nHaoran (Hao) Su is a Senior Technical Account Manager in New York City with over 8 years of experience with the cloud. He collaborates with Software, Internet and Model providers (SWIM) and Digitally Native Businesses (DNB) to improve their financial and operational efficiency, and architectural resiliency. Outside of work, Hao enjoys international traveling, exercising, and streaming.\nAnshika Tandon is a Senior Product Manager – Technical at AWS with a decade of experience building AI and B2B SaaS products from concept to launch. She excels in cross-functional product leadership, focusing on delivering measurable business value through strategic initiatives. A global citizen having lived in 10 cities and visited 26 countries, Anshika balances her professional life with interests in skiing, travel, and performing in improv comedy shows.\nAlex (Tao) Jia is a Senior Product Marketing Manager at AWS, focusing on generative AI. With 15+ years in tech marketing, she drives products from concept to scale, shaping positioning, fostering adoption, and leading global go-to-market strategies. She has worked with enterprises and ISVs, reaching millions of developers. Outside work, Alex enjoys exploring technology’s impact on humanity through books, research, and conversations.",
      "date": "2025-04-01",
      "authors": "Umesh Kalaspurkar",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "AWS App Studio has introduced two new features, a prebuilt solutions catalog and cross-instance import and export, aimed at streamlining the application development process using generative AI. These features enable faster deployment and sharing of applications across different AWS instances, making it easier for users to adopt and utilize the platform effectively.",
      "takeaways": [
        "- The prebuilt solutions catalog offers practical examples and common patterns that can accelerate application building and deployment.",
        "- The cross-instance import and export feature facilitates the migration of applications across different AWS Regions and accounts, enhancing portability and reusability.",
        "- App Studio allows non-developers to create business applications quickly, significantly reducing the time needed to move from concept to production."
      ]
    },
    {
      "id": 5,
      "title": "Build agentic systems with CrewAI and Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-agentic-systems-with-crewai-and-amazon-bedrock/",
      "description": "This post is co-authored with Joao Moura and Tony Kipkemboi from CrewAI.\nThe enterprise AI landscape is undergoing a seismic shift as agentic systems transition from experimental tools to mission-critical business assets. In 2025, AI agents are expected to become integral to business operations, with Deloitte predicting that 25% of enterprises using generative AI will deploy AI agents, growing to 50% by 2027. The global AI agent space is projected to surge from $5.1 billion in 2024 to $47.1 billion by 2030, reflecting the transformative potential of these technologies.\nIn this post, we explore how CrewAI’s open source agentic framework, combined with Amazon Bedrock, enables the creation of sophisticated multi-agent systems that can transform how businesses operate. Through practical examples and implementation details, we demonstrate how to build, deploy, and orchestrate AI agents that can tackle complex tasks with minimal human oversight. Although “agents” is the buzzword of 2025, it’s important to understand what an AI agent is and where deploying an agentic system could yield benefits.\nAgentic design\nAn AI agent is an autonomous, intelligent system that uses large language models (LLMs) and other AI capabilities to perform complex tasks with minimal human oversight. Unlike traditional software, which follows pre-defined rules, AI agents can operate independently, learn from their environment, adapt to changing conditions, and make contextual decisions. They are designed with modular components, such as reasoning engines, memory, cognitive skills, and tools, that enable them to execute sophisticated workflows. Traditional SaaS solutions are designed for horizontal scalability and general applicability, which makes them suitable for managing repetitive tasks across diverse sectors, but they often lack domain-specific intelligence and the flexibility to address unique challenges in dynamic environments. Agentic systems, on the other hand, are designed to bridge this gap by combining the flexibility of context-aware systems with domain knowledge. Consider a software development use case AI agents can generate, evaluate, and improve code, shifting software engineers’ focus from routine coding to more complex design challenges. For example, for the CrewAI git repository, pull requests are evaluated by a set of CrewAI agents who review code based on code documentation, consistency of implementation, and security considerations. Another use case can be seen in supply chain management, where traditional inventory systems might track stock levels, but lack the capability to anticipate supply chain disruptions or optimize procurement based on industry insights. In contrast, an agentic system can use real-time data (such as weather or geopolitical risks) to proactively reroute supply chains and reallocate resources. The following illustration describes the components of an agentic AI system:\n\nOverview of CrewAI\nCrewAI is an enterprise suite that includes a Python-based open source framework. It simplifies the creation and management of AI automations using either AI flows, multi-agent systems, or a combination of both, enabling agents to work together seamlessly, tackling complex tasks through collaborative intelligence. The following figure illustrates the capability of CrewAI’s enterprise offering:\n\nCrewAI’s design centers around the ability to build AI automation through flows and crews of AI agents. It excels at the relationship between agents and tasks, where each agent has a defined role, goal, and backstory, and can access specific tools to accomplish their objectives. This framework allows for autonomous inter-agent delegation, where agents can delegate tasks and inquire among themselves, enhancing problem-solving efficiency. This growth is fueled by the increasing demand for intelligent automation and personalized customer experiences across sectors like healthcare, finance, and retail.\nCrewAI’s agents are not only automating routine tasks, but also creating new roles that require advanced skills. CrewAI’s emphasis on team collaboration, through its modular design and simplicity principles, aims to transcend traditional automation, achieving a higher level of decision simplification, creativity enhancement, and addressing complex challenges.\nCrewAI key concepts\nCrewAI’s architecture is built on a modular framework comprising several key components that facilitate collaboration, delegation, and adaptive decision-making in multi-agent environments. Let’s explore each component in detail to understand how they enable multi-agent interactions.\nAt a high level, CrewAI creates two main ways to create agentic automations: flows and crews.\nFlows\nCrewAI Flows provide a structured, event-driven framework to orchestrate complex, multi-step AI automations seamlessly. Flows empower users to define sophisticated workflows that combine regular code, single LLM calls, and potentially multiple crews, through conditional logic, loops, and real-time state management. This flexibility allows businesses to build dynamic, intelligent automation pipelines that adapt to changing conditions and evolving business needs. The following figure illustrates the difference between Crews and Flows:\n\nWhen integrated with Amazon Bedrock, CrewAI Flows unlock even greater potential. Amazon Bedrock provides a robust foundation by enabling access to powerful foundation models (FMs).\nFor example, in a customer support scenario, a CrewAI Flow orchestrated through Amazon Bedrock could automatically route customer queries to specialized AI agent crews. These crews collaboratively diagnose customer issues, interact with backend systems for data retrieval, generate personalized responses, and dynamically escalate complex problems to human agents only when necessary.\nSimilarly, in financial services, a CrewAI Flow could monitor industry conditions, triggering agent-based analysis to proactively manage investment portfolios based on industry volatility and investor preferences.\nTogether, CrewAI Flows and Amazon Bedrock create a powerful synergy, enabling enterprises to implement adaptive, intelligent automation that addresses real-world complexities efficiently and at scale.\nCrews\nCrews in CrewAI are composed of several key components, which we discuss in this section.\nAgents\nAgents in CrewAI serve as autonomous entities designed to perform specific roles within a multi-agent system. These agents are equipped with various capabilities, including reasoning, memory, and the ability to interact dynamically with their environment. Each agent is defined by four main elements:\n\nRole – Determines the agent’s function and responsibilities within the system\nBackstory – Provides contextual information that guides the agent’s decision-making processes\nGoals – Specifies the objectives the agent aims to accomplish\nTools – Extends the capabilities of agents to access more information and take actions\n\nAgents in CrewAI are designed to work collaboratively, making autonomous decisions, delegating tasks, and using tools to execute complex workflows efficiently. They can communicate with each other, use external resources, and refine their strategies based on observed outcomes.\nTasks\nTasks in CrewAI are the fundamental building blocks that define specific actions an agent needs to perform to achieve its objectives. Tasks can be structured as standalone assignments or interdependent workflows that require multiple agents to collaborate. Each task includes key parameters, such as:\n\nDescription – Clearly defines what the task entails\nAgent assignment – Specifies which agent is responsible for executing the task\n\nTools\nTools in CrewAI provide agents with extended capabilities, enabling them to perform actions beyond their intrinsic reasoning abilities. These tools allow agents to interact with APIs, access databases, execute scripts, analyze data, and even communicate with other external systems. CrewAI supports a modular tool integration system where tools can be defined and assigned to specific agents, providing efficient and context-aware decision-making.\nProcess\nThe process layer in CrewAI governs how agents interact, coordinate, and delegate tasks. It makes sure that multi-agent workflows operate seamlessly by managing task execution, communication, and synchronization among agents.\nMore details on CrewAI concepts can be found in the CrewAI documentation.\nCrewAI enterprise suite\nFor businesses looking for tailored AI agent solutions, CrewAI provides an enterprise offering that includes dedicated support, advanced customization, and integration with enterprise-grade systems like Amazon Bedrock. This enables organizations to deploy AI agents at scale while maintaining security and compliance requirements.\nEnterprise customers get access to comprehensive monitoring tools that provide deep visibility into agent operations. This includes detailed logging of agent interactions, performance metrics, and system health indicators. The monitoring dashboard enables teams to track agent behavior, identify bottlenecks, and optimize multi-agent workflows in real time.\nReal-world enterprise impact\nCrewAI customers are already seeing significant returns by adopting agentic workflows in production. In this section, we provide a few real customer examples.\nLegacy code modernization\nA large enterprise customer needed to modernize their legacy ABAP and APEX code base, a typically time-consuming process requiring extensive manual effort for code updates and testing.\nMultiple CrewAI agents work in parallel to:\n\nAnalyze existing code base components\nGenerate modernized code in real time\nExecute tests in production environment\nProvide immediate feedback for iterations\n\nThe customer achieved approximately 70% improvement in code generation speed while maintaining quality through automated testing and feedback loops. The solution was containerized using Docker for consistent deployment and scalability. The following diagram illustrates the solution architecture.\n\nBack office automation at global CPG company\nA leading CPG company automated their back-office operations by connecting their existing applications and data stores to CrewAI agents that:\n\nResearch industry conditions\nAnalyze pricing data\nSummarize findings\nExecute decisions\n\nThe implementation resulted in a 75% reduction in processing time by automating the entire workflow from data analysis to action execution. The following diagram illustrates the solution architecture.\n\nGet started with CrewAI and Amazon Bedrock\nAmazon Bedrock integration with CrewAI enables the creation of production-grade AI agents powered by state-of-the-art language models.\nThe following is a code snippet on how to set up this integration:\n\nfrom crewai import Agent, Crew, Process, Task, LLM\nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool\nimport os\n\n# Configure Bedrock LLM\nllm = LLM(\n    model=\"bedrock/anthropic. anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n    aws_region_name=os.getenv('AWS_REGION_NAME')\n)\n\n# Create an agent with Bedrock as the LLM provider\nsecurity_analyst = Agent(\n    config=agents_config['security_analyst'],\n    tools=[SerperDevTool(), ScrapeWebsiteTool()],\n    llm=llm\n)\n\nCheck out the CrewAI LLM documentation for detailed instructions on how to configure LLMs with your AI agents.\nAmazon Bedrock provides several key advantages for CrewAI applications:\n\nAccess to state-of-the-art language models such as Anthropic’s Claude and Amazon Nova – These models provide the cognitive capabilities that power agent decision-making. The models enable agents to understand complex instructions, generate human-like responses, and make nuanced decisions based on context.\nEnterprise-grade security and compliance features – This is crucial for organizations that need to maintain strict control over their data and enforce compliance with various regulations.\nScalability and reliability backed by AWS infrastructure – This means your agent systems can handle increasing workloads while maintaining consistent performance.\n\nAmazon Bedrock Agents and Amazon Bedrock Knowledge Bases as native CrewAI Tools\nAmazon Bedrock Agents offers you the ability to build and configure autonomous agents in a fully managed and serverless manner on Amazon Bedrock. You don’t have to provision capacity, manage infrastructure, or write custom code. Amazon Bedrock manages prompt engineering, memory, monitoring, encryption, user permissions, and API invocation. BedrockInvokeAgentTool enables CrewAI agents to invoke Amazon Bedrock agents and use their capabilities within your workflows.\nWith Amazon Bedrock Knowledge Bases, you can securely connect FMs and agents to your company data to deliver more relevant, accurate, and customized responses. BedrockKBRetrieverTool enables CrewAI agents to retrieve information from Amazon Bedrock Knowledge Bases using natural language queries.\nThe following code shows an example for Amazon Bedrock Agents integration:\n\n\nfrom crewai import Agent, Task, Crew\n\nfrom crewai_tools.aws.bedrock.agents.invoke_agent_tool import BedrockInvokeAgentTool\n\n# Initialize the Bedrock Agents Tool\n\nagent_tool = BedrockInvokeAgentTool(\n    agent_id=\"your-agent-id\",\n    agent_alias_id=\"your-agent-alias-id\"\n)\n\n# Create an CrewAI agent that uses the Bedrock Agents Tool\n\naws_expert = Agent(\n    role='AWS Service Expert',\n    goal='Help users understand AWS services and quotas',\n    backstory='I am an expert in AWS services and can provide detailed information about them.',\n    tools=[agent_tool],\n    verbose=True\n)\n\n\nThe following code shows an example for Amazon Bedrock Knowledge Bases integration:\n\n\n# Create and configure the BedrockKB tool \nkb_tool = BedrockKBRetrieverTool(\n    knowledge_base_id=\"your-kb-id\",\n    number_of_results=5\n)\n\n# Create an CrewAI agent that uses the Bedrock Agents Tool\nresearcher = Agent(\n    role='Knowledge Base Researcher',\n    goal='Find information about company policies',\n    backstory='I am a researcher specialized in retrieving and analyzing company documentation.',\n    tools=[kb_tool],\n    verbose=True\n)\n\n\nOperational excellence through monitoring, tracing, and observability with CrewAI on AWS\nAs with any software application, achieving operational excellence is crucial when deploying agentic applications in production environments. These applications are complex systems comprising both deterministic and probabilistic components that interact either sequentially or in parallel. Therefore, comprehensive monitoring, traceability, and observability are essential factors for achieving operational excellence. This includes three key dimensions:\n\nApplication-level observability – Provides smooth operation of the entire system, including the agent orchestration framework CrewAI and potentially additional application components (such as a frontend)\nModel-level observability – Provides reliable model performance (including metrics like accuracy, latency, throughput, and more)\nAgent-level observability – Maintains efficient operations within single-agent or multi-agent systems\n\nWhen running agent-based applications with CrewAI and Amazon Bedrock on AWS, you gain access to a comprehensive set of built-in capabilities across these dimensions:\n\nApplication-level logs – Amazon CloudWatch automatically collects application-level logs and metrics from your application code running on your chosen AWS compute platform, such as AWS Lambda, Amazon Elastic Container Service (Amazon ECS), or Amazon Elastic Compute Cloud (Amazon EC2). The CrewAI framework provides application-level logging, configured at a minimal level by default. For more detailed insights, verbose logging can be enabled at the agent or crew level by setting verbose=True during initialization.\nModel-level invocation logs – Furthermore, CloudWatch automatically collects model-level invocation logs and metrics from Amazon Bedrock. This includes essential performance metrics.\nAgent-level observability – CrewAI seamlessly integrates with popular third-party monitoring and observability frameworks such as AgentOps, Arize, MLFlow, LangFuse, and others. These frameworks enable comprehensive tracing, debugging, monitoring, and optimization of the agent system’s performance.\n\nSolution overview\nEach AWS service has its own configuration nuances, and missing just one detail can lead to serious vulnerabilities. Traditional security assessments often demand multiple experts, coordinated schedules, and countless manual checks. With CrewAI Agents, you can streamline the entire process, automatically mapping your resources, analyzing configurations, and generating clear, prioritized remediation steps.\nThe following diagram illustrates the solution architecture.\n\nOur use case demo implements a specialized team of three agents, each with distinct responsibilities that mirror roles you might find in a professional security consulting firm:\n\nInfrastructure mapper – Acts as our system architect, methodically documenting AWS resources and their configurations. Like an experienced cloud architect, it creates a detailed inventory that serves as the foundation for our security analysis.\nSecurity analyst – Serves as our cybersecurity expert, examining the infrastructure map for potential vulnerabilities and researching current best practices. It brings deep knowledge of security threats and mitigation strategies.\nReport writer – Functions as our technical documentation specialist, synthesizing complex findings into clear, actionable recommendations. It makes sure that technical insights are communicated effectively to both technical and non-technical stakeholders.\n\nImplement the solution\nIn this section, we walk through the implementation of a security assessment multi-agent system. The code for this example is located on GitHub. Note that not all code artifacts of the solution are explicitly covered in this post.\nStep 1: Configure the Amazon Bedrock LLM\nWe’ve saved our environment variables in an .env file in our root directory before we pass them to the LLM class:\n\nfrom crewai import Agent, Crew, Process, Task, LLM \nfrom crewai.project import CrewBase, agent, crew, task \n\nfrom aws_infrastructure_security_audit_and_reporting.tools.aws_infrastructure_scanner_tool import AWSInfrastructureScannerTool \nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool \nimport os \n\n@CrewBase \nclass AwsInfrastructureSecurityAuditAndReportingCrew():  \n    \"\"\"AwsInfrastructureSecurityAuditAndReporting crew\"\"\" \n    def __init__(self) -> None: \n        self.llm = LLM( model=os.getenv('MODEL'),\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n        aws_region_name=os.getenv('AWS_REGION_NAME') \n    )\n\nStep 2: Define agents\nThese agents are already defined in the agents.yaml file, and we’re importing them into each agent function in the crew.py file:\n\n...\n# Configure AI Agents\n\n@agent \t \ndef infrastructure_mapper(self) -> Agent:\n    return Agent( \t \n        config=self.agents_config['infrastructure_mapper'],\n        tools=[AWSInfrastructureScannerTool()],\n        llm=self.llm \t \n    ) \t \t \n\n@agent \t \ndef security_analyst(self) -> Agent:\n    return Agent( \t \n        config=self.agents_config['security_analyst'], \t \n        tools=[SerperDevTool(), ScrapeWebsiteTool()],\n        llm=self.llm \t \n    ) \t \t\n\n@agent \t \ndef report_writer(self) -> Agent: \t \n    return Agent( \t \n        config=self.agents_config['report_writer'], \t \n        llm=self.llm \t \n    )\n\nStep 3: Define tasks for the agents\nSimilar to our agents in the preceding code, we import tasks.yaml into our crew.py file:\n\n...\n# Configure Tasks for the agents\n\n@task \ndef map_aws_infrastructure_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['map_aws_infrastructure_task']\n    ) \n\n@task \ndef exploratory_security_analysis_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['exploratory_security_analysis_task']\n    ) \n\n@task \ndef generate_report_task(self) -> Task: \n    return Task( \n        config=self.tasks_config['generate_report_task'] \n    )\n\nStep 4: Create the AWS infrastructure scanner tool\nThis tool enables our agents to interact with AWS services and retrieve information they need to perform their analysis:\n\nclass AWSInfrastructureScannerTool(BaseTool):\n    name: str = \"AWS Infrastructure Scanner\"\n    description: str = (\n        \"A tool for scanning and mapping AWS infrastructure components and their     configurations. \"\n        \"Can retrieve detailed information about EC2 instances, S3 buckets, IAM configurations, \"\n        \"RDS instances, VPC settings, and security groups. Use this tool to gather information \"\n        \"about specific AWS services or get a complete infrastructure overview.\"\n    )\n    args_schema: Type[BaseModel] = AWSInfrastructureScannerInput\n\n    def _run(self, service: str, region: str) -> str:\n        try:\n            if service.lower() == 'all':\n                return json.dumps(self._scan_all_services(region), indent=2, cls=DateTimeEncoder)\n            return json.dumps(self._scan_service(service.lower(), region), indent=2, cls=DateTimeEncoder)\n        except Exception as e:\n            return f\"Error scanning AWS infrastructure: {str(e)}\"\n\n    def _scan_all_services(self, region: str) -> Dict:\n        return {\n            'ec2': self._scan_service('ec2', region),\n            's3': self._scan_service('s3', region),\n            'iam': self._scan_service('iam', region),\n            'rds': self._scan_service('rds', region),\n            'vpc': self._scan_service('vpc', region)\n        }                                       \n   \n   # More services can be added here\n\nStep 5: Assemble the security audit crew\nBring the components together in a coordinated crew to execute on the tasks:\n\n@crew\ndef crew(self) -> Crew:\n    \"\"\"Creates the AwsInfrastructureSecurityAuditAndReporting crew\"\"\"\n    return Crew(\n        agents=self.agents, # Automatically created by the @agent decorator\n        tasks=self.tasks, # Automatically created by the @task decorator\n        process=Process.sequential,\n        verbose=True,\n    )\n\nStep 6: Run the crew\nIn our main.py file, we import our crew and pass in inputs to the crew to run:\n\ndef run():\n    \"\"\"\n    Run the crew.\n    \"\"\"\n    inputs = {}\n    AwsInfrastructureSecurityAuditAndReportingCrew().crew().kickoff(inputs=inputs)\n\nThe final report will look something like the following code:\n\n```markdown\n### Executive Summary\n\nIn response to an urgent need for robust security within AWS infrastructure, this assessment identified several critical areas requiring immediate attention across EC2 Instances, S3 Buckets, and IAM Configurations. Our analysis revealed two high-priority issues that pose significant risks to the organization's security posture.\n\n### Risk Assessment Matrix\n\n| Security Component | Risk Description | Impact | Likelihood | Priority |\n|--------------------|------------------|---------|------------|----------|\n| S3 Buckets | Unintended public access | High | High | Critical |\n| EC2 Instances | SSRF through Metadata | High | Medium | High |\n| IAM Configurations | Permission sprawl | Medium | High | Medium |\n\n### Prioritized Remediation Roadmap\n\n1. **Immediate (0-30 days):**\n   - Enforce IMDSv2 on all EC2 instances\n   - Conduct S3 bucket permission audit and rectify public access issues\n   - Adjust security group rules to eliminate broad access\n\n2. **Short Term (30-60 days):**\n   - Conduct IAM policy audit to eliminate unused permissions\n   - Restrict RDS access to known IP ranges\n```\n\nThis implementation shows how CrewAI agents can work together to perform complex security assessments that would typically require multiple security professionals. The system is both scalable and customizable, allowing for adaptation to specific security requirements and compliance standards.\nConclusion\nIn this post, we demonstrated how to use CrewAI and Amazon Bedrock to build a sophisticated, automated security assessment system for AWS infrastructure. We explored how multiple AI agents can work together seamlessly to perform complex security audits, from infrastructure mapping to vulnerability analysis and report generation. Through our example implementation, we showcased how CrewAI’s framework enables the creation of specialized agents, each bringing unique capabilities to the security assessment process. By integrating with powerful language models using Amazon Bedrock, we created a system that can autonomously identify security risks, research solutions, and generate actionable recommendations.\nThe practical example we shared illustrates just one of many possible applications of CrewAI with Amazon Bedrock. The combination of CrewAI’s agent orchestration capabilities and advanced language models in Amazon Bedrock opens up numerous possibilities for building intelligent, autonomous systems that can tackle complex business challenges.\nWe encourage you to explore our code on GitHub and start building your own multi-agent systems using CrewAI and Amazon Bedrock. Whether you’re focused on security assessments, process automation, or other use cases, this powerful combination provides the tools you need to create sophisticated AI solutions that can scale with your needs.\n\nAbout the Authors\n Tony Kipkemboi is a Senior Developer Advocate and Partnerships Lead at CrewAI, where he empowers developers to build AI agents that drive business efficiency. A US Army veteran, Tony brings a diverse background in healthcare, data engineering, and AI. With a passion for innovation, he has spoken at events like PyCon US and contributes to the tech community through open source projects, tutorials, and thought leadership in AI agent development. Tony holds a Bachelor’s of Science in Health Sciences and is pursuing a Master’s in Computer Information Technology at the University of Pennsylvania.\nJoão (Joe) Moura is the Founder and CEO of CrewAI, the leading agent orchestration platform powering multi-agent automations at scale. With deep expertise in generative AI and enterprise solutions, João partners with global leaders like AWS, NVIDIA, IBM, and Meta AI to drive innovative AI strategies. Under his leadership, CrewAI has rapidly become essential infrastructure for top-tier companies and developers worldwide and used by most of the F500 in the US.\n Karan Singh is a Generative AI Specialist at AWS, where he works with top-tier third-party foundation model and agentic frameworks providers to develop and execute joint go-to-market strategies, enabling customers to effectively deploy and scale solutions to solve enterprise generative AI challenges. Karan holds a Bachelor’s of Science in Electrical Engineering from Manipal University, a Master’s in Science in Electrical Engineering from Northwestern University, and an MBA from the Haas School of Business at University of California, Berkeley.\nAris Tsakpinis is a Specialist Solutions Architect for Generative AI focusing on open source models on Amazon Bedrock and the broader generative AI open source ecosystem. Alongside his professional role, he is pursuing a PhD in Machine Learning Engineering at the University of Regensburg, where his research focuses on applied natural language processing in scientific domains.",
      "date": "2025-03-31",
      "authors": "Tony Kipkemboi, João (Joe) Moura",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the evolution of agentic systems in the enterprise AI landscape, highlighting CrewAI's open-source framework in conjunction with Amazon Bedrock. It outlines how these systems facilitate the development of autonomous AI agents that can perform complex tasks with minimal human oversight, enhancing business operations across various sectors.",
      "takeaways": [
        "- The agentic AI market is projected to grow significantly, reflecting a shift towards integrating AI agents into core business functions.",
        "- CrewAI provides a robust framework for managing multi-agent systems, focusing on adaptive workflows and enhanced collaboration among AI agents.",
        "- By leveraging Amazon Bedrock, CrewAI enables the creation of scalable, intelligent automation solutions that can dynamically respond to real-world business challenges."
      ]
    },
    {
      "id": 6,
      "title": "Amazon Bedrock Guardrails image content filters provide industry-leading safeguards, helping customer block up to 88% of harmful multimodal content: Generally available today",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-guardrails-image-content-filters-provide-industry-leading-safeguards-helping-customer-block-up-to-88-of-harmful-multimodal-content-generally-available-today/",
      "description": "Amazon Bedrock Guardrails announces the general availability of image content filters, enabling you to moderate both image and text content in your generative AI applications. Previously limited to text-only filtering, this enhancement now provides comprehensive content moderation across both modalities. This new capability removes the heavy lifting required to build your own image safeguards or spend cycles on manual content moderation that can be error-prone and tedious.\nTero Hottinen, VP, Head of Strategic Partnerships at KONE, envisions the following use case:\n\n“In its ongoing evaluation, KONE recognizes the potential of Amazon Bedrock Guardrails as a key component in protecting generative AI applications, particularly for relevance and contextual grounding checks, as well as the multimodal safeguards. The company envisions integrating product design diagrams and manuals into its applications, with Amazon Bedrock Guardrails playing a crucial role in enabling more accurate diagnosis and analysis of multimodal content.”\n\n\nAmazon Bedrock Guardrails provides configurable safeguards to help customers block harmful or unwanted inputs and outputs for their generative AI applications. Customers can create custom Guardrails tailored to their specific use cases by implementing different policies to detect and filter harmful or unwanted content from both input prompts and model responses. Furthermore, customers can use Guardrails to detect model hallucinations and help make responses grounded and accurate. Through its standalone ApplyGuardrail API, Guardrails enables customers to apply consistent policies across any foundation model, including those hosted on Amazon Bedrock, self-hosted models, and third-party models. Bedrock Guardrails supports seamless integration with Bedrock Agents and Bedrock Knowledge Bases, enabling developers to enforce safeguards across various workflows, such as Retrieval Augmented Generation (RAG) systems and agentic applications.\nAmazon Bedrock Guardrails offers six distinct policies, including: content filters to detect and filter harmful material across several categories, including hate, insults, sexual content, violence, misconduct, and to prevent prompt attacks; topic filters to restrict specific subjects; sensitive information filters to block personally identifiable information (PII); word filters to block specific terms; contextual grounding checks to detect hallucinations and analyze response relevance; and Automated Reasoning checks (currently in gated preview) to identify, correct, and explain factual claims. With the new image content moderation capability, these safeguards now extend to both text and images, helping customer block up to 88% of harmful multimodal content. You can independently configure moderation for either image or text content (or both) with adjustable thresholds from low to high, helping you to build generative AI applications that align with your organization’s responsible AI policies.\nThis new capability is generally available in US East (N. Virginia), US West (Oregon), Europe (Frankfurt), and Asia Pacific (Tokyo) AWS Regions.\nIn this post, we discuss how to get started with image content filters in Amazon Bedrock Guardrails.\nSolution overview\nTo get started, create a guardrail on the AWS Management Console and configure the content filters for either text or image data or both. You can also use AWS SDKs to integrate this capability into your applications.\nCreate a guardrail\nTo create a guardrail, complete the following steps:\n\nOn the Amazon Bedrock console, under Safeguards in the navigation pane, choose Guardrails.\nChoose Create guardrail.\nIn the Configure content filters section, under Harmful categories and Prompt attacks, you can use the existing content filters to detect and block image data in addition to text data. \nAfter you’ve selected and configured the content filters you want to use, you can save the guardrail and start using it to help you block harmful or unwanted inputs and outputs for your generative AI applications.\n\nTest a guardrail with text generation\nTo test the new guardrail on the Amazon Bedrock console, select the guardrail and choose Test. You have two options: test the guardrail by choosing and invoking a model or test the guardrail without invoking a model by using the Amazon Bedrock Guardrails independent ApplyGuardail API.\nWith the ApplyGuardrail API, you can validate content at any point in your application flow before processing or serving results to the user. You can also use the API to evaluate inputs and outputs for self-managed (custom) or third-party FMs, regardless of the underlying infrastructure. For example, you could use the API to evaluate a Meta Llama 3.2 model hosted on Amazon SageMaker or a Mistral NeMo model running on your laptop.\nTest a guardrail by choosing and invoking a model\nSelect a model that supports image inputs or outputs, for example, Anthropic’s Claude 3.5 Sonnet. Verify that the prompt and response filters are enabled for image content. Then, provide a prompt, upload an image file, and choose Run.\n\nIn this example, Amazon Bedrock Guardrails intervened. Choose View trace for more details.\nThe guardrail trace provides a record of how safety measures were applied during an interaction. It shows whether Amazon Bedrock Guardrails intervened or not and what assessments were made on both input (prompt) and output (model response). In this example, the content filters blocked the input prompt because they detected violence in the image with medium confidence.\n\nTest a guardrail without invoking a model\nOn the Amazon Bedrock console, choose Use ApplyGuardail API, the independent API to test the guardrail without invoking a model. Choose whether you want to validate an input prompt or an example of a model generated output. Then, repeat the steps from the previous section. Verify that the prompt and response filters are enabled for image content, provide the content to validate, and choose Run.\n\nFor this example, we reused the same image and input prompt, and Amazon Bedrock Guardrails intervened again. Choose View trace again for more details.\n\nTest a guardrail with image generation\nNow, let’s test the Amazon Bedrock Guardrails multimodal toxicity detection with image generation use cases. The following is an example of using Amazon Bedrock Guardrails image content filters with an image generation use case. We generate an image using the Stability model on Amazon Bedrock using the InvokeModel API and the guardrail:\n\nguardrailIdentifier = <<guardrail_id>>\nguardrailVersion =\"1\"\n\nmodel_id = 'stability.sd3-5-large-v1:0'\noutput_images_folder = 'images/output'\n\nbody = json.dumps(\n    {\n        \"prompt\": \"A Gun\", #  for image generation (\"A gun\" should get blocked by violence)\n        \"output_format\": \"jpeg\"\n    }\n)\n\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\ntry:\n    print(\"Making a call to InvokeModel API for model: {}\".format(model_id))\n    response = bedrock_runtime.invoke_model(\n        body=body,\n        modelId=model_id,\n        trace='ENABLED',\n        guardrailIdentifier=guardrailIdentifier,\n        guardrailVersion=guardrailVersion\n    )\n    response_body = json.loads(response.get('body').read())\n    print(\"Received response from InvokeModel API (Request Id: {})\".format(response['ResponseMetadata']['RequestId']))\n    if 'images' in response_body and len(response_body['images']) > 0:\n        os.makedirs(output_images_folder, exist_ok=True)\n        images = response_body[\"images\"]\n        for image in images:\n            image_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n            image_file = os.path.join(output_images_folder, \"generated-image-{}.jpg\".format(image_id))\n            print(\"Saving generated image {} at {}\".format(image_id, image_file))\n            with open(image_file, 'wb') as image_file_descriptor:\n                image_file_descriptor.write(base64.b64decode(image.encode('utf-8')))\n    else:\n        print(\"No images generated from model\")\n    guardrail_trace = response_body['amazon-bedrock-trace']['guardrail']\n    guardrail_trace['modelOutput'] = ['<REDACTED>']\n    print(guardrail_trace['outputs'])\n    print(\"\\nGuardrail Trace: {}\".format(json.dumps(guardrail_trace, indent=2)))\nexcept botocore.exceptions.ClientError as err:\n    print(\"Failed while calling InvokeModel API with RequestId = {}\".format(err.response['ResponseMetadata']['RequestId']))\n    raise err\n\nYou can access the complete example from the GitHub repo.\nConclusion\nIn this post, we explored how Amazon Bedrock Guardrails’ new image content filters provide comprehensive multimodal content moderation capabilities. By extending beyond text-only filtering, this solution now helps customers block up to 88% of harmful or unwanted multimodal content across configurable categories including hate, insults, sexual content, violence, misconduct, and prompt attack detection. Guardrails can help organizations across healthcare, manufacturing, financial services, media, and education enhance brand safety without the burden of building custom safeguards or conducting error-prone manual evaluations.\nTo learn more, see Stop harmful content in models using Amazon Bedrock Guardrails.\n\nAbout the Authors\nSatveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services, specializing in Amazon Bedrock security. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer’s deep understanding of generative AI technologies and security principles allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value while maintaining robust security postures.\nShyam Srinivasan is on the Amazon Bedrock Guardrails product team. He cares about making the world a better place through technology and loves being part of this journey. In his spare time, Shyam likes to run long distances, travel around the world, and experience new cultures with family and friends.\nAntonio Rodriguez is a Principal Generative AI Specialist Solutions Architect at AWS. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.\nDr. Andrew Kane is the WW Tech Leader for Security and Compliance for AWS Generative AI Services, leading the delivery of under-the-hood technical assets for customers around security, as well as working with CISOs around the adoption of generative AI services within their organisations. Before joining AWS at the beginning of 2015, Andrew spent two decades working in the fields of signal processing, financial payments systems, weapons tracking, and editorial and publishing systems. He is a keen karate enthusiast (just one belt away from Black Belt) and is also an avid home-brewer, using automated brewing hardware and other IoT sensors. He was the legal licensee in his ancient (AD 1468) English countryside village pub until early 2020.",
      "date": "2025-03-28",
      "authors": "Satveer Khurpa",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "Amazon Bedrock Guardrails has introduced new image content filters that enable moderation of both image and text content in generative AI applications, enhancing the ability to block harmful multimodal content by up to 88%. This feature allows customers to implement configurable safeguards to manage unwanted inputs and outputs effectively.",
      "takeaways": [
        "- The new image content moderation capability broadens the existing text-only filtering, providing comprehensive content safeguards across various categories.",
        "- Customers can create custom Guardrails with six distinct policies to filter harmful material, including hate speech, violence, and sensitive information.",
        "- The ApplyGuardrail API allows for consistent policy enforcement across any foundation model, facilitating seamless integration into diverse generative AI applications."
      ]
    },
    {
      "id": 7,
      "title": "Integrating custom dependencies in Amazon SageMaker Canvas workflows",
      "link": "https://aws.amazon.com/blogs/machine-learning/integrating-custom-dependencies-in-amazon-sagemaker-canvas-workflows/",
      "description": "When implementing machine learning (ML) workflows in Amazon SageMaker Canvas, organizations might need to consider external dependencies required for their specific use cases. Although SageMaker Canvas provides powerful no-code and low-code capabilities for rapid experimentation, some projects might require specialized dependencies and libraries that aren’t included by default in SageMaker Canvas. This post provides an example of how to incorporate code that relies on external dependencies into your SageMaker Canvas workflows.\nAmazon SageMaker Canvas is a low-code no-code (LCNC) ML platform that guides users through every stage of the ML journey, from initial data preparation to final model deployment. Without writing a single line of code, users can explore datasets, transform data, build models, and generate predictions.\nSageMaker Canvas offers comprehensive data wrangling capabilities that help you prepare your data, including:\n\nOver 300 built-in transformation steps\nFeature engineering capabilities\nData normalization and cleansing functions\nA custom code editor supporting Python, PySpark, and SparkSQL\n\nIn this post, we demonstrate how to incorporate dependencies stored in Amazon Simple Storage Service (Amazon S3) within an Amazon SageMaker Data Wrangler flow. Using this approach, you can run custom scripts that depend on modules not inherently supported by SageMaker Canvas.\nSolution overview\nTo showcase the integration of custom scripts and dependencies from Amazon S3 into SageMaker Canvas, we explore the following example workflow.\nThe solution follows three main steps:\n\nUpload custom scripts and dependencies to Amazon S3\nUse SageMaker Data Wrangler in SageMaker Canvas to transform your data using the uploaded code\nTrain and export the model\n\nThe following diagram is the architecture for the solution.\n\nIn this example, we work with two complementary datasets available in SageMaker Canvas that contain shipping information for computer screen deliveries. By joining these datasets, we create a comprehensive dataset that captures various shipping metrics and delivery outcomes. Our goal is to build a predictive model that can determine whether future shipments will arrive on time based on historical shipping patterns and characteristics.\nPrerequisites\nAs a prerequisite, you need access to Amazon S3 and Amazon SageMaker AI. If you don’t already have a SageMaker AI domain configured in your account, you also need permissions to create a SageMaker AI domain.\nCreate the data flow\nTo create the data flow, follow these steps:\n\nOn the Amazon SageMaker AI console, in the navigation pane, under Applications and IDEs, select Canvas, as shown in the following screenshot. You might need to create a SageMaker domain if you haven’t done so already.\nAfter your domain is created, choose Open Canvas.\n\n\nIn Canvas, select the Datasets tab and select canvas-sample-shipping-logs.csv, as shown in the following screenshot. After the preview appears, choose + Create a data flow.\n\nThe initial data flow will open with one source and one data type.\n\nAt the top right of the screen, and select Add data → tabular. Choose Canvas Datasets as the source and select canvas-sample-product-descriptions.csv.\nChoose Next as shown in the following screenshot. Then choose Import.\n\n\nAfter both datasets have been added, select the plus sign. From the dropdown menu, choose select Combine data. From the next dropdown menu, choose Join.\n\n\nTo perform an inner join on the ProductID column, in the right-hand menu, under Join type, choose Inner join. Under Join keys, choose ProductId, as shown in the following screenshot.\n\n\nAfter the datasets have been joined, select the plus sign. In the dropdown menu, select + Add transform. A preview of the dataset will open.\n\nThe dataset contains XShippingDistance (long) and YShippingDistance (long) columns. For our purposes, we want to use a custom function that will find the total distance using the X and Y coordinates and then drop the individual coordinate columns. For this example, we find the total distance using a function that relies on the mpmath library.\n\nTo call the custom function, select + Add transform. In the dropdown menu, select Custom transform. Change the editor to Python (Pandas) and try to run the following function from the Python editor:\n\n\nfrom mpmath import sqrt  # Import sqrt from mpmath\n\ndef calculate_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n\n    # Use mpmath's sqrt to calculate the total distance for each row\n    df[new_col] = df.apply(lambda row: float(sqrt(row[x_col]**2 + row[y_col]**2)), axis=1)\n    \n    # Drop the original x and y columns\n    df = df.drop(columns=[x_col, y_col])\n    \n    return df\n\ndf = calculate_total_distance(df)\n\n\nRunning the function produces the following error: ModuleNotFoundError: No module named ‘mpmath’, as shown in the following screenshot.\n\nThis error occurs because mpmath isn’t a module that is inherently supported by SageMaker Canvas. To use a function that relies on this module, we need to approach the use of a custom function differently.\nZip the script and dependencies\nTo use a function that relies on a module that isn’t natively supported in Canvas, the custom script must be zipped with the module(s) it relies on. For this example, we used our local integrated development environment (IDE) to create a script.py that relies on the mpmath library.\nThe script.py file contains two functions: one function that is compatible with the Python (Pandas) runtime (function calculate_total_distance), and one that is compatible with the Python (Pyspark) runtime (function udf_total_distance).\n\ndef calculate_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n    from npmath import sqrt  # Import sqrt from npmath\n\n    # Use npmath's sqrt to calculate the total distance for each row\n    df[new_col] = df.apply(lambda row: float(sqrt(row[x_col]**2 + row[y_col]**2)), axis=1)\n\n    # Drop the original x and y columns\n    df = df.drop(columns=[x_col, y_col])\n\n    return df\n\ndef udf_total_distance(df, x_col=\"XShippingDistance\", y_col=\"YShippingDistance\", new_col=\"TotalDistance\"):\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import udf\n    from pyspark.sql.types import FloatType\n\n    spark = SparkSession.builder \\\n        .master(\"local\") \\\n        .appName(\"DistanceCalculation\") \\\n        .getOrCreate()\n\n    def calculate_distance(x, y):\n        import sys\n\n        # Add the path to npmath\n        mpmath_path = \"/tmp/maths\"\n        if mpmath_path not in sys.path:\n            sys.path.insert(0, mpmath_path)\n\n        from mpmath import sqrt\n        return float(sqrt(x**2 + y**2))\n\n    # Register and apply UDF\n    distance_udf = udf(calculate_distance, FloatType())\n    df = df.withColumn(new_col, distance_udf(df[x_col], df[y_col]))\n    df = df.drop(x_col, y_col)\n\n    return df\n\n\nTo make sure the script can run, install mpmath into the same directory as script.py by running pip install mpmath.\nRun zip -r my_project.zip to create a .zip file containing the function and the mpmath installation. The current directory now contains a .zip file, our Python script, and the installation our script depends on, as shown in the following screenshot.\n\nUpload to Amazon S3\nAfter creating the .zip file, upload it to an Amazon S3 bucket.\n\nAfter the zip file has been uploaded to Amazon S3, it’s accessible in SageMaker Canvas.\nRun the custom script\nReturn to the data flow in SageMaker Canvas and replace the prior custom function code with the following code and choose Update.\n\nimport zipfile\nimport boto3\nimport sys\nfrom pathlib import Path\nimport shutil\nimport importlib.util\n\n\ndef load_script_and_dependencies(bucket_name, zip_key, extract_to):\n    \"\"\"\n    Downloads a zip file from S3, unzips it, and ensures dependencies are available.\n\n    Args:\n        bucket_name (str): Name of the S3 bucket.\n        zip_key (str): Key for the .zip file in the bucket.\n        extract_to (str): Directory to extract files to.\n\n    Returns:\n        str: Path to the extracted folder containing the script and dependencies.\n    \"\"\"\n    \n    s3_client = boto3.client(\"s3\")\n    \n    # Local path for the zip file\n    zip_local_path = '/tmp/dependencies.zip'\n    \n    # Download the .zip file from S3\n    s3_client.download_file(bucket_name, zip_key, zip_local_path)\n    print(f\"Downloaded zip file from S3: {zip_key}\")\n\n    # Unzip the file\n    try:\n        with zipfile.ZipFile(zip_local_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_to)\n            print(f\"Extracted files to {extract_to}\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to extract zip file: {e}\")\n\n    # Add the extracted folder to Python path\n    if extract_to not in sys.path:\n      sys.path.insert(0, extract_to)\n          \n    return extract_to\n    \n\n\ndef call_function_from_script(script_path, function_name, df):\n    \"\"\"\n    Dynamically loads a function from a Python script using importlib.\n    \"\"\"\n    try:\n        # Get the script name from the path\n        module_name = script_path.split('/')[-1].replace('.py', '')\n        \n        # Load the module specification\n        spec = importlib.util.spec_from_file_location(module_name, script_path)\n        if spec is None:\n            raise ImportError(f\"Could not load specification for module {module_name}\")\n            \n        # Create the module\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[module_name] = module\n        \n        # Execute the module\n        spec.loader.exec_module(module)\n        \n        # Get the function from the module\n        if not hasattr(module, function_name):\n            raise AttributeError(f\"Function '{function_name}' not found in the script.\")\n            \n        loaded_function = getattr(module, function_name)\n\n        # Clean up: remove module from sys.modules after execution\n        del sys.modules[module_name]\n        \n        # Call the function\n        return loaded_function(df)\n        \n    except Exception as e:\n        raise RuntimeError(f\"Error loading or executing function: {e}\")\n\n\nbucket_name = 'canvasdatabuckett'  # S3 bucket name\nzip_key = 'functions/my_project.zip'  # S3 path to the zip file with our custom dependancy\nscript_name = 'script.py'  # Name of the script in the zip file\nfunction_name = 'udf' # Name of function to call from our script\nextract_to = '/tmp/maths' # Local path to our custom script and dependancies\n\n# Step 1: Load the script and dependencies\nextracted_path = load_script_and_dependencies(bucket_name, zip_key, extract_to)\n\n# Step 2: Call the function from the script\nscript_path = f\"{extracted_path}/{script_name}\"\ndf = call_function_from_script(script_path, function_name, df)\n\n\nThis example code unzips the .zip file and adds the required dependencies to the local path so they’re available to the function at run time. Because mpmath was added to the local path, you can now call a function that relies on this external library.\nThe preceding code runs using the Python (Pandas) runtime and calculate_total_distance function. To use the Python (Pyspark) runtime, update the function_name variable to call the udf_total_distance function instead.\nComplete the data flow\nAs a last step, remove irrelevant columns before training the model. Follow these steps:\n\nOn the SageMaker Canvas console, select + Add transform. From the dropdown menu, select Manage columns \nUnder Transform, choose Drop column. Under Columns to drop, add ProductId_0, ProductId_1, and OrderID, as shown in the following screenshot.\n\nThe final dataset should contain 13 columns. The complete data flow is pictured in the following image.\n\nTrain the model\nTo train the model, follow these steps:\n\nAt the top right of the page, select Create model and name your dataset and model.\nSelect Predictive analysis as the problem type and OnTimeDelivery as the target column, as shown in the screenshot below.\n\nWhen building the model you can choose to run a Quick build or a Standard build. A Quick build prioritizes speed over accuracy and produces a trained model in less than 20 minutes. A standard build prioritizes accuracy over latency but the model takes longer to train.\nResults\nAfter the model build is complete, you can view the model’s accuracy, along with metrics like F1, precision and recall. In the case of a standard build, the model achieved 94.5% accuracy.\n\nAfter the model training is complete, there are four ways you can use your model:\n\nDeploy the model directly from SageMaker Canvas to an endpoint\nAdd the model to the SageMaker Model Registry\nExport your model to a Jupyter Notebook\nSend your model to Amazon QuickSight for use in dashboard visualizations\n\nClean up\nTo manage costs and prevent additional workspace charges, choose Log out to sign out of SageMaker Canvas when you’re done using the application, as shown in the following screenshot. You can also configure SageMaker Canvas to automatically shut down when idle.\nIf you created an S3 bucket specifically for this example, you might also want to empty and delete your bucket.\n\nSummary\nIn this post, we demonstrated how you can upload custom dependencies to Amazon S3 and integrate them into SageMaker Canvas workflows. By walking through a practical example of implementing a custom distance calculation function with the mpmath library, we showed how to:\n\nPackage custom code and dependencies into a .zip file\nStore and access these dependencies from Amazon S3\nImplement custom data transformations in SageMaker Data Wrangler\nTrain a predictive model using the transformed data\n\nThis approach means that data scientists and analysts can extend SageMaker Canvas capabilities beyond the more than 300 included functions.\nTo try custom transforms yourself, refer to the Amazon SageMaker Canvas documentation and sign in to SageMaker Canvas today. For additional insights into how you can optimize your SageMaker Canvas implementation, we recommend exploring these related posts:\n\nSeamlessly transition between no-code and code-first machine learning with Amazon SageMaker Canvas and Amazon SageMaker Studio\nUnlock the power of data governance and no-code machine learning with Amazon SageMaker Canvas and Amazon DataZone\n\n\n\nAbout the Author\nNadhya Polanco is an Associate Solutions Architect at AWS based in Brussels, Belgium. In this role, she supports organizations looking to incorporate AI and Machine Learning into their workloads. In her free time, Nadhya enjoys indulging in her passion for coffee and exploring new destinations.",
      "date": "2025-03-27",
      "authors": "Nadhya Polanco",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses how to integrate custom dependencies into Amazon SageMaker Canvas workflows, allowing users to enhance their low-code machine learning processes by incorporating external libraries that are not natively supported. It provides a step-by-step guide on uploading custom scripts and dependencies to Amazon S3 and executing them within the SageMaker Canvas environment.",
      "takeaways": [
        "- Users can extend the functionality of Amazon SageMaker Canvas by incorporating custom scripts and third-party libraries stored in Amazon S3.",
        "- A detailed example illustrates the process of using the `mpmath` library for a custom distance calculation in a machine learning workflow.",
        "- The article emphasizes the flexibility of SageMaker Canvas, enabling data scientists to implement specialized transformations beyond the platform's built-in capabilities."
      ]
    },
    {
      "id": 8,
      "title": "Generate training data and cost-effectively train categorical models with Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/generate-training-data-and-cost-effectively-train-categorical-models-with-amazon-bedrock/",
      "description": "In this post, we explore how you can use Amazon Bedrock to generate high-quality categorical ground truth data, which is crucial for training machine learning (ML) models in a cost-sensitive environment. Generative AI solutions can play an invaluable role during the model development phase by simplifying training and test data creation for multiclass classification supervised learning use cases. We dive deep into this process on how to use XML tags to structure the prompt and guide Amazon Bedrock in generating a balanced label dataset with high accuracy. We also showcase a real-world example for predicting the root cause category for support cases. This use case, solvable through ML, can enable support teams to better understand customer needs and optimize response strategies.\nBusiness challenge\nThe exploration and methodology described in this post addresses two key challenges: costs associated with generating a ground truth dataset for multiclass classification use cases can be prohibitive, and conventional approaches and synthetic dataset creation strategies for generating ground truth data are inadequate in generating balanced classes and meeting desired performance parameters for the real-world use cases.\nGround truth data generation is expensive and time consuming\nGround truth annotation needs to be accurate and consistent, often requiring massive time and expertise to ensure the dataset is balanced, diverse, and large enough for model training and testing. For a multiclass classification problem such as support case root cause categorization, this challenge compounds many fold.\nLet’s say the task at hand is to predict the root cause categories (Customer Education, Feature Request, Software Defect, Documentation Improvement, Security Awareness, and Billing Inquiry) for customer support cases. Based on our experiments using best-in-class supervised learning algorithms available in AutoGluon, we arrived at a 3,000 sample size for the training dataset for each category to attain an accuracy of 90%. This requirement translates into time and effort investment of trained personnel, who could be support engineers or other technical staff, to review tens of thousands of support cases to arrive at an even distribution of 3,000 per category. With each support case and the related correspondences averaging 5 minutes per review and assessment from a human labeler, this translates into 1,500 hours (5 minutes x 18,000 support cases) of work or 188 days considering an 8-hour workday. Besides the time in review and labeling, there is an upfront investment in training the labelers so the exercise split between 10 or more labelers is consistent. To break this down further, a ground truth labeling campaign split between 10 labelers would require close to 4 weeks to label 18,000 cases if the labelers spend 40 hours a week on the exercise.\nNot only is such an extended and effort-intensive campaign expensive, but it can cause inconsistent labeling for categories every time the labeler puts aside the task and resumes it later. The exercise also doesn’t guarantee a balanced labeled ground truth dataset because some root cause categories such as Customer Education could be far more common than Feature Request or Software Defect, thereby extending the campaign.\nConventional techniques to get balanced classes or synthetic data generation have shortfalls\nA balanced labeled dataset is critical for a multiclass classification use case to mitigate bias and make sure the model learns to accurately classify all classes, rather than favoring the majority class. If the dataset is imbalanced, with one or more classes having significantly fewer instances than others, the model might struggle to learn the patterns and features associated with the minority classes, leading to poor performance and biased predictions. This issue is particularly problematic in applications where accurate classification of minority classes is critical, such as medical diagnoses, fraud detection, or root cause categorization. For the use case of labeling the support root cause categories, it’s often harder to source examples for categories such as Software Defect, Feature Request, and Documentation Improvement for labeling than it is for Customer Education. This results in an imbalanced class distribution for training and test datasets.\nTo address this challenge, various techniques can be employed, including oversampling the minority classes, undersampling the majority classes, using ensemble methods that combine multiple classifiers trained on different subsets of the data, or synthetic data generation to augment minority classes. However, the ideal approach for achieving optimal performance is to start with a balanced and highly accurate labeled dataset for ground truth training.\nAlthough oversampling for minority classes means extended and expensive data labeling with humans who review the support cases, synthetic data generation to augment the minority classes poses its own challenges. For the multiclass classification problem to label support case data, synthetic data generation can quickly result in overfitting. This is because it can be difficult to synthesize real-world examples of technical case correspondences that contain complex content related to software configuration, implementation guidance, documentation references, technical troubleshooting, and the like.\nBecause ground truth labeling is expensive and synthetic data generation isn’t an option for use cases such as root cause prediction, the effort to train a model is often put aside. This results in a missed opportunity to review the root cause trends that can guide investment in the right areas such as education for customers, documentation improvement, or other efforts to reduce the case volume and improve customer experience.\nSolution overview\nThe preceding section discussed why conventional ground truth data generation techniques aren’t viable for certain supervised learning use cases and fall short in training a highly accurate model to predict the support case root cause in our example. Let’s look at how generative AI can help solve this problem.\nGenerative AI supports key use cases such as content creation, summarization, code generation, creative applications, data augmentation, natural language processing, scientific research, and many others. Amazon Bedrock is well-suited for this data augmentation exercise to generate high-quality ground truth data. Using highly tuned and custom tailored prompts with examples and techniques discussed in the following sections, support teams can pass the anonymized support case correspondence to Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock or other available large language models (LLMs) to predict the root cause label for a support case from one of the many categories (Customer Education, Feature Request, Software Defect, Documentation Improvement, Security Awareness, and Billing Inquiry). After achieving the desired accuracy, you can use this ground truth data in an ML pipeline with automated machine learning (AutoML) tools such as AutoGluon to train a model and inference the support cases.\nChecking LLM accuracy for ground truth data\nTo evaluate an LLM for the task of category labeling, the process begins by determining if labeled data is available. If labeled data exists, the next step is to check if the model’s use case produces discrete outcomes. Where discrete outcomes with labeled data exist, standard ML methods such as precision, recall, or other classic ML metrics can be used. These metrics provide high precision but are limited to specific use cases due to limited ground truth data.\nIf the use case doesn’t yield discrete outputs, task-specific metrics are more appropriate. These include metrics such as ROUGE or cosine similarity for text similarity, and specific benchmarks for assessing toxicity (Detoxify), prompt stereotyping (cross-entropy loss), or factual knowledge (HELM, LAMA).\nIf labeled data is unavailable, the next question is whether the testing process should be automated. The automation decision depends on the cost-accuracy trade-off because higher accuracy comes at a higher cost. For cases where automation is not required, human-in-the-Loop (HIL) approaches can be used. This involves manual evaluation based on predefined assessment rules (for example, ground truth), yielding high evaluation precision, but often is time-consuming and costly.\nWhen automation is preferred, using another LLM to assess outputs can be effective. Here, a reliable LLM can be instructed to rate generated outputs, providing automated scores and explanations. However, the precision of this method depends on the reliability of the chosen LLM. Each path represents a tailored approach based on the availability of labeled data and the need for automation, allowing for flexibility in assessing a wide range of FM applications.\nThe following figure illustrates an FM evaluation workflow.\n\nFor the use case, if a historic collection of 10,000 or more support cases labeled using Amazon SageMaker Ground Truth with HIL is available, it can be used for evaluating the accuracy of the LLM prediction. The key goal for generating new ground truth data using Amazon Bedrock should be to augment it for increasing diversity and increasing the training data size for AutoGluon training to arrive at a performant model that can be used for the final inference or root cause prediction. In the following sections, we explain how to take an incremental and measured approach to improve Anthropic’s Claude 3.5 Sonnet prediction accuracy through prompt engineering.\nPrompt engineering for FM accuracy and consistency\nPrompt engineering is the art and science of designing a prompt to get an LLM to produce the desired output. We suggest consulting LLM prompt engineering documentation such as Anthropic prompt engineering for experiments. Based on experiments conducted without a finely tuned and optimized prompt, we observed low accuracy rates of less than 60%. In the following sections, we provide a detailed explanation on how to construct your first prompt, and then gradually improve it to consistently achieve over 90% accuracy.\nDesigning the prompt\nBefore starting any scaled use of generative AI, you should have the following in place:\n\nA clear definition of the problem you are trying to solve along with the end goal.\nA way to test the model’s output for accuracy. The thumbs up/down technique to determine accuracy along with comparing with the 10,000 labeled dataset through SageMaker Ground Truth is well-suited for this exercise.\nA defined success criterion on how accurate the model needs to be.\n\nIt’s helpful to think of an LLM as a new employee who is very well read, but knows nothing about your culture, your norms, what you are trying to do, or why you are trying to do it. The LLM’s performance will depend on how precisely you can explain what you want. How would a skilled manager handle a very smart, but new and inexperienced employee? The manager would provide contextual background, explain the problem, explain the rules they should apply when analyzing the problem, and give some examples of what good looks like along with why it is good. Later, if they saw the employee making mistakes, they might try to simplify the problem and provide constructive feedback by giving examples of what not to do, and why. One difference is that an employee would understand the job they are being hired for, so we need to explicitly tell the LLM to assume the persona of a support employee.\nPrerequisites\nTo follow along with this post, set up Amazon SageMaker Studio to run Python in a notebook and interact with Amazon Bedrock. You also need the appropriate permissions to access Amazon Bedrock models.\nSet up SageMaker Studio\nComplete the following steps to set up SageMaker Studio:\n\nOn the SageMaker console, choose Studio under Applications and IDEs in the navigation pane.\nCreate a new SageMaker Studio instance if you haven’t already.\nIf prompted, set up a user profile for SageMaker Studio by providing a user name and specifying AWS Identity and Access Management (IAM) permissions.\nOpen a SageMaker Studio notebook: \n  \nChoose JupyterLab.\nCreate a private JupyterLab space.\nConfigure the space (set the instance type to ml.m5.large for optimal performance).\nLaunch the space.\nOn the File menu, choose New and Notebook to create a new notebook.\n \nConfigure SageMaker to meet your security and compliance objectives. Refer to Configure security in Amazon SageMaker AI for details.\n\nSet up permissions for Amazon Bedrock access\nMake sure you have the following permissions:\n\nIAM role with Amazon Bedrock permissions – Make sure that your SageMaker Studio execution role has the necessary permissions to access Amazon Bedrock. Attach the AmazonBedrockFullAccesspolicy or a custom policy with specific Amazon Bedrock permissions to your IAM role.\nAWS SDKs and authentication – Verify that your AWS credentials (usually from the SageMaker role) have Amazon Bedrock access. Refer to Getting started with the API to set up your environment to make Amazon Bedrock requests through the AWS API.\nModel access – Grant permission to use Anthropic’s Claude 3.5 Sonnet. For instructions, see Add or remove access to Amazon Bedrock foundation models.\n\nTest the code using the native inference API for Anthropic’s Claude\nThe following code uses the native inference API to send a text message to Anthropic’s Claude. The Python code invokes the Amazon Bedrock Runtime service:\n\nimport boto3\nimport json\nfrom datetime import datetime\nimport time\n\n# Create an Amazon Bedrock Runtime client in the AWS Region of your choice.\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\n# Set the model ID, e.g., Claude 3 Haiku.\nmodel_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n\n# Load the prompt from a file (showed and explained later in the blog)\nwith open('prompt.txt', 'r') as file:\ndata = file.read()\n\n\ndef callBedrock(body):\n# Format the request payload using the model's native structure.\n\nprompt = data + body;\n\n# The prompt is then truncated to the max input window size of Sonnet 3.5\nprompt = prompt[:180000]\n\n# Define parametres passed to the model. \nnative_request = {\n\"anthropic_version\": \"bedrock-2023-05-31\",\n\"max_tokens\": 512,\n\"temperature\": 0.2,\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": prompt}],\n}\n],\n}\n\n# Convert the native request to JSON.\nrequest = json.dumps(native_request)\n\ntry:\n# Invoke the model with the request.\nresponse = client.invoke_model(modelId=model_id, body=request)\n\nexcept (Exception) as e:\nprint(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n\n# Load the response returned from Amazon Bedrock into a json object\nmodel_response = json.loads(response[\"body\"].read())\n\n# Extract and print the response text.\nresponse_text = model_response[\"content\"][0][\"text\"]\nreturn response_text;\n\nConstruct the initial prompt\nWe demonstrate the approach for the specific use case for root cause prediction with a goal of achieving 90% accuracy. Start by creating a prompt similar to the prompt you would give to humans in natural language. This can be a simple description of each root cause label and why you would choose it, how to interpret the case correspondences, how to analyze and choose the corresponding root cause label, and provide examples for every category. Ask the model to also provide the reasoning to understand how it reached to certain decisions. It can be especially interesting to understand the reasoning for the decisions you don’t agree with. See the following example code:\n\nPlease familiarize yourself with these categories.  When you evaluate a case, evaluate the definitions in order and label the case with the first definition that fits.  If a case morphs from one type to another, choose the type the case started out as. \n Read the correspondence, especially the original request, and the last correspondence from the support agent to the customer. If there are lot of correspondences, or the case does not seem straightforward to infer, read the correspondences date stamped in order to understand what happened. If the case references documentation, read or skim the documentation to determine whether the documentation clearly supports what the support agent mentioned and whether it answers the customers issue.\n\nSoftware Defect:  “Software Defect” are cases where the application does not work as expected. The support agent confirms this through analysis and troubleshooting and mentions internal team is working on a fix or patch to address the bug or defect. \n\nAn example of Software Defect case is [Customer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\" Agent: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"]\n....  \n\nAnalyze the results\nWe recommend using a small sample (for example, 150) of random cases and run them through Anthropic’s Claude 3.5 Sonnet using the initial prompt, and manually check the initial results. You can load the input data and model output into Excel, and add the following columns for analysis:\n\nClaude Label – A calculated column with Anthropic’s Claude’s category\nLabel – True category after reviewing each case and selecting a specific root cause category to compare with the model’s prediction and derive an accuracy measurement\nClose Call – 1 or 0 so that you can take numerical averages\nNotes – For cases where there was something noteworthy about the case or inaccurate categorizations\nClaude Correct – A calculated column (0 or 1) based on whether our category matched the model’s output category\n\nAlthough the first run is expected to have low accuracy unfit for using the prompt for generating the ground truth data, the reasoning will help you understand why Anthropic’s Claude mislabeled the cases. In the example, many of the misses fell into these categories and the accuracy was only 61%:\n\nCases where Anthropic’s Claude categorized Customer Education cases as Software Defect because it interpreted the support agent instructions to reconfigure something as a workaround for a Software Defect.\nCases where users asked questions about billing that Anthropic’s Claude categorized as Customer Education. Although billing questions could also be Customer Education cases, we wanted these to be categorized as the more specific Billing Inquiry Likewise, although Security Awareness cases are also Customer Education, we wanted to categorize these as the more specific Security Awareness category.\n\nIterate on the prompt and make changes\nProviding the LLM explicit instructions on correcting these errors should result in a major boost in accuracy. We tested the following adjustments with Anthropic’s Claude:\n\nWe defined and assigned a persona with background information for the LLM: “You are a Support Agent and an expert on the enterprise application software. You will be classifying customer cases into categories…”\nWe ordered the categories from more deterministic and well-defined to less specific and instructed Anthropic’s Claude to evaluate the categories in the order they appear in the prompt.\nWe recommend using the Anthropic documentation suggestion to use XML tags and the enclosed root cause categories in light XML but not a formal XML document, with elements delimited with tags. It’s ideal to create categories as nodes with a separate sub-node for each category. The category node should consist of a name of the category, a description, and what the output would look like. The categories should be delimited by begin and end tags.\n\n\nYou are a Support Agent and an expert on the enterprise application software. You will be classifying the customer support cases into categories, based on the given interaction between an agent and a customer. You can only choose ONE Category from the list below. You follow instructions well, step by step, and evaluate the categories in the order they appear in the prompt when making a decision.\n\nThe categories are defined as:\n\n<categories>\n<category>\n<name>\n\"Software Defect\"\n</name>\n<description>\n“Software Defect” are cases where the application software does not work as expected. The agent confirms the application is not working as expected and may refer to internal team working on a fix or patch to address the bug or defect. The category includes common errors or failures related to performance, software version, functional defect, unexpected exception or usability bug when the customer is following the documented steps.\n</description>\n</category>\n...\n</categories>\n\n\nWe created a good examples node with at least one good example for every category. Each good example consisted of the example, the classification, and the reasoning:\n\nHere are some good examples with reasoning:\n\n<good examples>\n<example>\n<example data>\nCustomer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\"\nAgent: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"\n</example data\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nCustomer is reporting a data processing exception with a specific version and the agent confirms this is a regression and defect. The agent confirms that engineering is working to provide an emergency patch for the issue. \n</explanation>\n</example>\n...\n</good examples>\n\n\n\nWe created a bad examples node with examples of where the LLM miscategorized previous cases. The bad examples node should have the same set of fields as the good examples, such as example data, classification, explanation, but the explanation explained the error. The following is a snippet:\n\nHere are some examples for wrong classification with reasoning:\n\n<bad examples>\n\n    <example>\n        <example data>\n            Customer: \"We need the ability to create custom dashboards that can aggregate data across multiple tenants in real-time. Currently, we can only view metrics per individual tenant, which requires manual consolidation for our enterprise reporting needs.\"\nAgent: \"I understand your need for cross-tenant analytics. While the current functionality is limited to single-tenant views as designed, I've submitted your request to our product team as a high-priority feature enhancement. They'll evaluate it for inclusion in our 2025 roadmap. I'll update you when there's news about this capability.\"\n       </example data>\n    <example output>\n        <classification>\n            \"Software Defect\"\n        </classification>\n        <explanation>\n            Classification should be Feature Request and not Software Defect. The application does not have the function or capability being requested but it is working as documented or advertised. In the example, the agent mentions they have submitted with request to their product team to consider in the future roadmap.\n        </explanation>\n    </example>\n...\n<bad examples>\n\n\n\nWe also added instructions for how to format the output:\n\n\nGiven the above categories defined in XML, logically think through which category fits best and then complete the classification. Provide a response in XML with the following elements: classification, explanation (limited to 2 sentences). Return your results as this sample output XML below and do not append your thought process to the response.\n \n<response> \n<classification> Software Defect </classification>\n<explanation> The support case is for ETL Pipeline Performance Degradation where the customer reports their nightly data transformation job takes 6 hours to complete instead of 2 hours before but no changes to configuration occurred. The agent mentions Engineering confirmed memory leak in version 5.1.2 and are deploying a Hotfix indicating this is a Software Defect.\n</explanation> \n</response> \n\nTest with the new prompt\nThe preceding approach should result in an improved prediction accuracy. In our experiment, we saw 84% accuracy with the new prompt and the output was consistent and more straightforward to parse. Anthropic’s Claude followed the suggested output format in almost all cases. We wrote code to fix errors such as unexpected tags in the output and drop responses that could not be parsed.\nThe following is the code to parse the output:\n\n# This python script parses LLM output into a comma separated list with the SupportID, Category, Reason\n# Command line is python parse_llm_putput.py PathToLLMOutput.txt PathToParsedOutput.csv\n# Note:  It will overwrite the output file without confirming\n# it will write completion status and any error messages to stdout\n \nimport re\nimport sys\n \n# these tokens are based on the format of the claude output.\n# This will create three inputs CaseID, RootCause and Reasoning.  We will to extract them using re.match.\npattern = re.compile(\n    \"^([0-9]*).*<classification>(.*)</classification><explanation>(.*)</explanation>\"\n)\n \nendToken = \"</response>\"\ncheckToken = \"<classification>\"\n \nacceptableClassifications = [\n    \"Billing Inquiry\",\n    \"Documentation Improvement\",\n    \"Feature Request\",\n    \"Security Awareness\",\n    \"Software Defect\",\n    \"Customer Education\",\n]\n \ndef parseResponse(response):\n    # parsing is trivial withe regular expression groups\n    m = pattern.match(response)\n    return m\n \n# get the input and output files\nif len(sys.argv) != 3:\n    print(\"Command line error parse_llm_output.py inputfile outputfile\")\n    exit(1)\n \n# open the file\ninput = open(sys.argv[1], encoding=\"utf8\")\noutput = open(sys.argv[2], \"w\")\n \n# read the entire file in.  This works well with 30,000 responses, but would need to be adjusted for say 3,000,000 responses\nresponses = input.read()\n \n# get rid of the double quotes and newlines to avoid incorrect excel parsing and these are unnecessary\nresponses = responses.replace('\"', \"\")\nresponses = responses.replace(\"\\n\", \"\")\n \n# initialize our placeholder, and counters\nparsedChars = 0\nskipped = 0\ninvalid = 0\nresponseCount = 0\n \n# write the header\noutput.write(\"CaseID,RootCause,Reason\\n\")\n \n# find the first response\nindex = responses.find(endToken, parsedChars)\n \nwhile index > 0:\n    # extract the response\n    response = responses[parsedChars : index + len(endToken)]\n    # parse it\n    parsedResponse = parseResponse(response)\n \n    # is the response valid\n    if parsedResponse is None or len(response.split(checkToken)) != 2:\n        # this happens when there is a missing /response delimiter or some other formatting problem, it clutters up and the next response\n        skipped = skipped + 2\n    else:\n        # if we have a valid response write it to the file, enclose the reason in double quotes because it uses commas\n        if parsedResponse.group(2).lower() not in acceptableClassifications:\n            # make sure the classification is one we expect\n            print(\"Invalid Classification: {0}\".format(parsedResponse.group(2)))\n            invalid = invalid + 1\n        else:\n            # write a valid line to the output file\n            output.write(\n                '{0},{1},\"{2}\"\\n'.format(\n                    parsedResponse.group(1),\n                    parsedResponse.group(2),\n                    parsedResponse.group(3),\n                )\n            )\n \n    # move the pointer past where we parsed and update the counter\n    parsedChars = index + len(endToken)\n    responseCount = responseCount + 1\n \n    # find the next response\n    index = responses.find(endToken, parsedChars)\n \nprint(\"skipped {0} of {1} responses\".format(skipped, responseCount))\nprint(\"{0} of these were invalid\".format(invalid)) \n\nMost mislabeled cases were close calls or had very similar traits. For example, when a customer described a problem, the support agent suggested possible solutions and asked for logs in order to troubleshoot. However, the customer self-resolved the case and so the resolution details weren’t conclusive. For this scenario, the root cause prediction was inaccurate. In our experiment, Anthropic’s Claude labeled these cases as Software Defects, but the most likely scenario is that the customer figured it out for themselves and never followed up.\nContinued fine-tuning of the prompt to adjust examples and include such scenarios incrementally can help to get over 90% prediction accuracy, as we confirmed with our experimentation. The following code is an example of how to adjust the prompt and add a few more bad examples:\n\n<example>\n<example data>\nSubject: Unable to configure custom routing rules in application gateway\nCustomer: Our team can't set up routing rules in the application gateway. We've tried following the documentation but the traffic isn't being directed as expected. This is blocking our production deployment.\nAgent: I understand you're having difficulties with routing rules configuration. To better assist you, could you please provide:\nCurrent routing rule configuration\nApplication gateway logs\nExpected traffic flow diagram\n[No response from customer for 5 business days - Case closed by customer]\n</example data>\n    <example output>\n      <classification>\n       Software Defect\n      </classification>\n <explanation>\nClassification should be Customer Education and not Software Defect. The agent acknowledges the problem and asks the customer for additional information to troubleshoot, however, the customer does not reply and closes the case. Cases where the agent tells the customer how to solve the problem and provides documentation or asks for further details to troubleshoot but the customer self-resolves the case should be labeled Customer Education.\n</explanation>\n</example>\n\nWith the preceding adjustments and refinement to the prompt, we consistently obtained over 90% accuracy and noted that a few miscategorized cases were close calls where humans chose multiple categories including the one Anthropic’s Claude chose. See the appendix at the end of this post for the final prompt.\nRun batch inference at scale with AutoGluon Multimodal\nAs illustrated in the previous sections, by crafting a well-defined and tailored prompt, Amazon Bedrock can help automate generation of ground truth data with balanced categories. This ground truth data is necessary to train the supervised learning model for a multiclass classification use case. We suggest taking advantage of the preprocessing capabilities of SageMaker to further refine the fields, encoding them into a format that’s optimal for model ingestion. The manifest files can be set up as the catalyst, triggering an AWS Lambda function that sets entire SageMaker pipeline into action. This end-to-end process seamlessly handles data inference and stores the results in Amazon Simple Storage Service (Amazon S3). We recommend AutoGluon Multimodal for training and prediction and deploying a model for a batch inference pipeline to predict the root cause for new or updated support cases at scale on a daily cadence.\nClean up\nTo prevent unnecessary expenses, it’s essential to properly decommission all provisioned resources. This cleanup process involves stopping notebook instances and deleting JupyterLab spaces, SageMaker domains, S3 bucket, IAM role, and associated user profiles. Refer to Clean up Amazon SageMaker notebook instance resources for details.\nConclusion\nThis post explored how Amazon Bedrock and advanced prompt engineering can generate high-quality labeled data for training ML models. Specifically, we focused on a use case of predicting the root cause category for customer support cases, a multiclass classification problem. Traditional approaches to generating labeled data for such problems are often prohibitively expensive, time-consuming, and prone to class imbalances. Amazon Bedrock, guided by XML prompt engineering, demonstrated the ability to generate balanced labeled datasets, at a lower cost, with over 90% accuracy for the experiment, and can help overcome labeling challenges for training categorical models for real-world use cases.\nThe following are our key takeaways:\n\nGenerative AI can simplify labeled data generation for complex multiclass classification problems\nPrompt engineering is crucial for guiding LLMs to achieve desired outputs accurately\nAn iterative approach, incorporating good/bad examples and specific instructions, can significantly improve model performance\nThe generated labeled data can be integrated into ML pipelines for scalable inference and prediction using AutoML multimodal supervised learning algorithms for batch inference\n\nReview your ground truth training costs with respect to time and effort for HIL labeling and service costs and do a comparative analysis with Amazon Bedrock to plan your next categorical model training at scale.\nAppendix\nThe following code is the final prompt:\n\nYou are a Support Agent and an expert in the enterprise application software. You will be classifying the customer support cases into one of the 6 categories, based on the given interaction between the Support Agent and a customer. You can only choose ONE Category from the list below. You follow instructions well, step by step, and evaluate the categories in the order they appear in the prompt when making a decision. \n \nThe categories are defined as:\n \n<categories>\n \n<category>\n<name>\n\"Billing Inquiry\" \n</name>\n<description>\n“Billing Inquiry” cases are the ones related to Account or Billing inquiries and questions related to charges, savings, or discounts. It also includes requests to provide guidance on account closing, request for Credit, cancellation requests, billing questions, and questions about discounts.\n</description>\n</category>\n \n<category>\n<name>\n\"Security Awareness\" \n</name>\n<description>\n“Security Awareness” cases are the cases associated with a security related incident. Security Awareness cases include exposed credentials, mitigating a security vulnerability, DDoS attacks, security concerns related to malicious traffic. Note that general security questions where the agent is helping to educate the user on the best practice such as SSO or MFA configuration, Security guidelines, or setting permissions for users and roles should be labeled as Customer Education and not Security Awareness. \n</description>\n</category>\n \n<category>\n<name>\n\"Feature Request\" \n</name>\n<description>\n“Feature Request” are the cases where the customer is experiencing a limitation in the application software and asking for a feature they want to have. Customer highlights a limitation and is requesting for the capability. For a Feature Request case, the support agent typically acknowledges that the question or expectation is a feature request for the software. Agent may use words such as the functionality or feature does not exist or it is currently not supported. \n</description>\n</category>\n \n<category>\n<name>\n\"Software Defect\" \n</name>\n<description>\n“Software Defect” are cases where the application does not work as expected. The support agent confirms this through analysis and troubleshooting and mentions internal team is working on a fix or patch to address the bug or defect. \n</description>\n</category>\n \n<category>\n<name>\n\"Documentation Improvement\" \n</name>\n<description>\n“Documentation Improvement” are cases where there is a lack of documentation, incorrect documentation, or insufficient documentation and when the case is not attributed to a Software Defect or a Feature Request. In Documentation Improvement cases the agent acknowledges the application documentation is incomplete or not up to date, or that they will ask documentation team to improve the documentation. For Documentation Improvement cases, the agent may suggest a workaround that is not part of application documentation and does not reference the standard application documentation or link. References to workarounds or sources such as Github or Stack Overflow, when used as an example of a solution, are examples of a Documentation Improvement case because the details and examples are missing from the official documentation.\n</description>\n</category>\n \n<category>\n<name>\n\"Customer Education\" \n</name>\n<description>\n“Customer Education” cases are cases where the customer could have resolved the case information using the existing application documentation. In these cases, the agent is educating the customer they are not using the feature correctly or have an incorrect configuration, while guiding them to the documentation. Customer Education cases include scenario where an agent provides troubleshooting steps for a problem or answers a question and provides links to the official application documentation. User Education cases include cases when the customer asks for best practices and agent provides knowledge article links to the support center documentation. Customer Education also includes cases created by the agent or application developers to suggest and educate the customer on a change to reduce cost, improve security, or improve application performance. Customer Education cases include cases where the customer asks a question or requests help with an error or configuration and the agent guides them appropriately with steps or documentation links. Customer Education cases also include the cases where the customer is using an unsupported configuration or version that may be End Of Life (EOL). Customer Education cases also include inconclusive cases where the customer reported an issue with the application but the case is closed without resolution details.\n</description>\n</category>\n \n</categories>\n \nHere are some good examples with reasoning:\n \n<good examples>\n \n<example>\n<example data>\nCustomer: \"I noticed unexpected charges of $12,500 on our latest invoice, which is significantly higher than our usual $7,000 monthly spend. We haven't added new users, so I'm concerned about this increase.\"\nSupport: \"I understand your concern about the increased charges. Upon review, I see that 50 Premium Sales Cloud licenses were automatically activated on January 15th when your sandbox environments were refreshed. I can help adjust your sandbox configuration and discuss Enterprise License Agreement options to optimize costs.\"\nCustomer: \"Thank you for clarifying. Please tell me more about the Enterprise License options.\"\n</example data\n<example output>\n<classification>\n\"Billing Inquiry\"\n</classification>\n<explanation>\nCustomer is asking a question to clarify the unexpected increase in their billing statement charge and the agent explains why this occurred. The customer wants to learn more about ways to optimize costs.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"URGENT: We've detected unauthorized API calls from an unknown IP address accessing sensitive customer data in our production environment. Our monitoring shows 1000+ suspicious requests in the last hour.\"\nSupport: \"I understand the severity of this security incident. I've immediately revoked the compromised API credentials and initiated our security protocol. The suspicious traffic has been blocked. I'm escalating this to our Security team for forensic analysis. I'll stay engaged until this is resolved.\"\n</example data\n<example output>\n<classification>\n\"Security Awareness\"\n</classification>\n<explanation>\nCustomer reported unauthorized API calls and suspicious requests. The agent confirms revoking compromised API credentials and initiating the protocol.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"Is there a way to create custom notification templates for different user groups? We need department-specific alert formats, but I can only find a single global template option.\"\nSupport: \"I understand you're looking to customize notification templates per user group. Currently, this functionality isn't supported in our platform - we only offer the global template system. I'll submit this as a feature request to our product team. In the meantime, I can suggest using notification tags as a workaround.\"\nCustomer: \"Thanks, please add my vote for this feature.\"\n</example data\n<example output>\n<classification>\n\"Feature Request\"\n</classification>\n<explanation>\nCustomer is asking for a new feature to have custom notification templates for different user groups since they have a use case that is currently not supported by the application. The agent confirms the functionality does not exist and mentions submitting a feature request to the product team.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"Our data pipeline jobs are failing with a 'memory allocation error' during the aggregation phase. This started occurring after upgrading to version 4.2.1. The same ETL workflows were running fine before the upgrade. We've verified our infrastructure meets all requirements.\"\nSupport: \"After analyzing the logs, we've confirmed a memory leak in the aggregation module - a regression introduced in 4.2.1. Engineering has identified the root cause and is developing an emergency patch. We expect to release version 4.2.2 within 48 hours to resolve this issue.\"\n</example data\n<example output>\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nCustomer is reporting a data processing exception with a specific version and the agent confirms this is a regression and defect. The agent confirms that engineering is working to provide an emergency patch for the issue. \n</explanation>\n \n<example>\n<example data>\nCustomer: \"The data export function is failing consistently when we include custom fields. The export starts but crashes at 45% with error code DB-7721. This worked fine last week before the latest release.\"\nSupport: \"I've reproduced the issue in our test environment and confirmed this is a bug introduced in version 4.2.1. Our engineering team has identified the root cause - a query optimization error affecting custom field exports. They're working on a hotfix (patch 4.2.1.3).\"\nCustomer: \"Please notify when fixed.\"\n</example data>\n<example output>\n<classification>\n\"Software Defect\"\n</classification>\n<explanation>\nThis is a Software Defect as the data export function is not working as expected to export the custom fields. The agent acknowledged the issue and confirmed engineering is working on a hotfix.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"I'm trying to implement the batch processing API but the documentation doesn't explain how to handle partial failures or provide retry examples. The current docs only show basic success scenarios.\"\nSupport: The documentation is lacking detailed error handling examples for batch processing. I'll submit this to our documentation team to add comprehensive retry logic examples and partial failure scenarios. For now, I can share a working code snippet that demonstrates proper error handling and retry mechanisms.\"\nCustomer: \"Thanks, the code example would help.\"\n</example data\n<example output>\n<classification>\nDocumentation Improvement\n</classification>\n<explanation>\nThe agent acknowledges the gap in the documentation and mentions they will pass on this to the documentation team for further improvements. Agent mentions providing a working code snippet with retry examples.\n</explanation>\n \n<example>\n<example data>\nCustomer: \"We can't get our SSO integration working. The login keeps failing and we're not sure what's wrong with our configuration.\"\nSupport: \"I can help guide you through the SSO setup. Looking at your configuration, I notice the SAML assertion isn't properly formatted. Please follow our step-by-step SSO configuration guide here [link to docs]. Pay special attention to section 3.2 about SAML attributes. The guide includes validation steps to ensure proper integration.\"\nCustomer: \"Found the issue in section 3.2. Working now, thanks!\"\n</example data\n<example output>\n<classification>\nCustomer Education\n</classification>\n<explanation>\nCustomer is asking for help and guidance to get their SSO integration working. The agent went over the details and presented the steps along necessary along with the documentation links.\n</explanation>\n \n</good examples>\n \nHere are some examples for wrong classification with reasoning:\n \n<bad examples>\n \n<example>\n<example data>\nCustomer: \"We want to enhance our application security. Currently, each team member has individual login credentials. What's the recommended approach?\"\nSupport: \"recommend implementing SAML-based SSO with your existing identity provider. This will:\nCentralize authentication\nEnable MFA enforcement\nStreamline user provisioning\nEnhance security auditing\n</example data>\n<example output>\n<classification>\n\"Security Awareness\"\n</classification>\n<explanation>\nClassification should be Customer Education and not Security Awareness. General security questions where the agent is helping to educate the user such as Security guidelines and best practices, should be labeled as Customer Education.\n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"Our SAP invoices aren't syncing instantly with Salesforce opportunities. We've configured MuleSoft Composer as per documentation, but updates only happen intermittently.\"\nSupport: \"I understand you're looking for real-time synchronization. Currently, MuleSoft Composer's fastest sync interval is 15 minutes by design. While I can help optimize your current setup, I'll submit a feature request for real-time sync capability. Here's how to optimize the current polling interval: doc link\"\n</example data>\n<example output>\n<classification>\nCustomer Education\n</classification>\n<explanation>\nClassification should be Feature Request and not Customer Education. The agent tells the customer that fastest sync interval is 15 minutes by design. The agent also points out they will submit a Feature Request. Cases where the customer ask for features should be classified as Feature Request. \n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"Our sales ETL pipeline keeps timing out with error 'V_001' at the transform step. This was working perfectly before.\"\nSupport: \"I've analyzed your configuration. The timeout occurs because the transformation spans 5 years of data containing 23 cross-object formula fields and is running without filters. Please implement these optimization steps from our documentation: Document link on ETL performance\"\n</example data>\n<example output>\n<classification>\nSoftware Defect\n</classification>\n<explanation>\nClassification should be Customer Education and not Software Defect. The agent tells the user that timeout is caused by misconfiguration and needs to be restricted using filters. The agent provides documentation explaining how to troubleshoot the issue. Cases where the agent tells the user how to solve the problem and provides documentation should be labeled Customer Education.\n</explanation>\n</example>\n \n<example>\n<example data>\nCustomer: \"We are trying to deploy a custom workflow template but receiving this error: Resource handler returned message: 'Error: Multiple or missing values for mandatory single-value field, Field: ACTION_TYPE, Parameter: Workflow Action (Status Code: 400, Request ID: TKT-2481-49bc)' when deploying through Flow Designer.\"\nSupport: \"I've reviewed your Flow Designer deployment (instance: dev85xxx.xxx.com/flow/TKT-2481-49bc) which failed to create a Workflow Action resource. This error occurs when the action configuration is ambiguous. After checking the Flow Designer documentation [1], each Action Step in your template must define exactly one 'Action Type' attribute. The Flow Designer documentation [2] specifies that each workflow action requires a single, explicit action type definition. You cannot have multiple or undefined action types in a single step. This is similar to an issue reported in the Product Community [3]. Please review your workflow template and ensure each action step has exactly one defined Action Type. The documentation provides detailed configuration examples at [4]. Let me know if you need any clarification on implementing these changes.\n</example data>\n<example output>\n<classification>\nDocumentation Improvement\n</classification>\n<explanation>\nClassification should be Customer Education and not Documentation Improvement. The agent tells the user they have to change the action configuration and define an Action type attribute. Cases where the agent tells the user how to solve problem and provides documentation should be classified Customer Education.\n</explanation>\n</example>\n \n</bad examples>\n \nGiven the above categories defined in XML, logically think through which category fits best and then complete the classification. Provide a response in XML with the following elements: classification, explanation (limited to 2 sentences). Return your results as this sample output XML below and do not append your thought process to the response.\n \n<response> \n<classification> Software Defect </classification>\n<explanation> The support case is for ETL Pipeline Performance Degradation where the customer reports their nightly data transformation job takes 6 hours to complete instead of 2 hours before but no changes to configuration occurred. The agent mentions Engineering confirmed memory leak in version 5.1.2 and are deploying a Hotfix indicating this is a Software Defect.\n</explanation> \n</response> \n \nHere is the conversation you need to categorize:\n\n\n\nAbout the Authors\nSumeet Kumar is a Sr. Enterprise Support Manager at AWS leading the technical and strategic advisory team of TAM builders for automotive and manufacturing customers. He has diverse support operations experience and is passionate about creating innovative solutions using AI/ML.\nAndy Brand is a Principal Technical Account Manager at AWS, where he helps education customers develop secure, performant, and cost-effective cloud solutions. With over 40 years of experience building, operating, and supporting enterprise software, he has a proven track record of addressing complex challenges.\nTom Coombs is a Principal Technical Account Manager at AWS, based in Switzerland. In Tom’s role, he helps enterprise AWS customers operate effectively in the cloud. From a development background, he specializes in machine learning and sustainability.\nRamu Ponugumati is a Sr. Technical Account Manager and a specialist in analytics and AI/ML at AWS. He works with enterprise customers to modernize and cost optimize workloads, and helps them build reliable and secure applications on the AWS platform. Outside of work, he loves spending time with his family, playing badminton, and hiking.",
      "date": "2025-03-27",
      "authors": "Sumeet Kumar",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses how to effectively use Amazon Bedrock and generative AI for generating high-quality categorical ground truth data necessary for training machine learning models, particularly for multiclass classification in customer support cases. It emphasizes the importance of prompt engineering and provides a detailed methodology to achieve over 90% accuracy in labeling support cases.",
      "takeaways": [
        "- Generative AI, particularly Amazon Bedrock, can significantly reduce the time and cost involved in creating balanced labeled datasets for training machine learning models.",
        "- Effective prompt engineering is crucial for guiding large language models (LLMs) to produce accurate and relevant outputs, which can be achieved through iterative refinement and the use of good/bad examples.",
        "- The integration of generated labeled data into machine learning pipelines can enhance the accuracy and scalability of predictions in real-world applications, particularly in customer support environments."
      ]
    },
    {
      "id": 9,
      "title": "Enable Amazon Bedrock cross-Region inference in multi-account environments",
      "link": "https://aws.amazon.com/blogs/machine-learning/enable-amazon-bedrock-cross-region-inference-in-multi-account-environments/",
      "description": "Amazon Bedrock cross-Region inference capability that provides organizations with flexibility to access foundation models (FMs) across AWS Regions while maintaining optimal performance and availability. However, some enterprises implement strict Regional access controls through service control policies (SCPs) or AWS Control Tower to adhere to compliance requirements, inadvertently blocking cross-Region inference functionality in Amazon Bedrock. This creates a challenging situation where organizations must balance security controls with using AI capabilities.\nIn this post, we explore how to modify your Regional access controls to specifically allow Amazon Bedrock cross-Region inference while maintaining broader Regional restrictions for other AWS services. We provide practical examples for both SCP modifications and AWS Control Tower implementations.\nUnderstanding cross-Region inference\nWhen running model inference in on-demand mode, your requests might be restricted by service quotas or during peak usage times. Cross-Region inference enables you to seamlessly manage unplanned traffic bursts by utilizing compute across different Regions. With cross-Region inference, you can distribute traffic across multiple Regions, enabling higher throughput.\nMany organizations implement Regional access controls through:\n\nSCPs in AWS Organizations\nAWS Control Tower controls\nCustom AWS Identity and Access Management (IAM) policies\n\nThese controls typically deny access to all services in specific Regions for security, compliance, or cost management reasons. However, these broad denials also prevent Amazon Bedrock from functioning properly when it needs to access models in those Regions through cross-Region inference.\nHow Cross-Region inference works and interacts with SCPs\nCross-Region inference in Amazon Bedrock is a powerful feature that enables automatic cross-Region routing for inference requests. This capability is particularly beneficial for developers using on-demand inference mode, because it provides a seamless solution for achieving higher throughput and performance while effectively managing incoming traffic spikes in applications powered by Amazon Bedrock.\nWith cross-Region inference, developers can alleviate the need to predict demand fluctuations manually. Instead, the system dynamically routes traffic across multiple Regions, maintaining optimal resource utilization and performance. Importantly, cross-Region inference prioritizes the connected Amazon Bedrock API source Region when possible, helping minimize latency and improve overall responsiveness. This intelligent routing enhances applications’ reliability, performance, and efficiency without requiring constant oversight from development teams.\nAt its core, cross-Region inference operates on two key concepts: the source Region and the fulfillment Region. The source Region, also known as the origination Region, is where the inference request is initially invoked by the client. In contrast, the fulfillment Region is the Region that actually services the large language model (LLM) invocation request.\nCross-Region inference employs a proprietary custom routing logic that Amazon continuously evolves to provide the best inference experience for customers. This routing mechanism is intentionally heuristics-based, with a primary focus on providing high availability. By default, the service attempts to fulfill requests from the source Region, when possible, but it can seamlessly route requests to other Regions as needed. This intelligent routing considers factors such as Regional capacity, latency, and availability to make optimal decisions.\nAlthough cross-Region inference offers powerful flexibility, it requires access to models in all potential fulfillment Regions to function properly. This requirement is where SCPs can significantly impact cross-Region inference functionality.\nLet’s examine a scenario that highlights the critical interaction between cross-Region inference and SCPs. As illustrated in the following figure, we use two Regions, us-east-1 and us-west-2, and have denied all other Regions using an SCP that could have been implemented using AWS Organizations or an AWS Control Tower control.\n\nThe workflow consists of the following steps:\n\nA user makes an inference request to the us-east-1 Amazon Bedrock endpoint (source Region) using a cross-Region inference profile.\nThe Amazon Bedrock heuristics-based routing system evaluates available Regions for request fulfillment.\nus-west-2 and us-east-1 are allowed for Amazon Bedrock service access through SCPs, but us-east-2 is denied using the SCP.\nThis single Regional restriction (us-east-2) causes the cross-Region inference call to fail.\nEven though other Regions are available and allowed, the presence of one blocked Region (us-east-2) results in a failed request.\nThe client receives an error indicating they are not authorized to perform the action.\n\nThis behavior is by design; cross-Region inference service requires access to run inference in all potential fulfillment Regions to maintain its ability to optimally route requests. Attempts to use cross-Region inference will fail if any potential target Region is blocked by SCPs, regardless of other available Regions. To successfully implement cross-Region inference, organizations must make sure that their SCPs allow Amazon Bedrock api actions in all Regions where their target model is available. This means identifying all Regions where required models are hosted, modifying SCPs to allow minimal required Amazon Bedrock permissions in these Regions, and maintaining these permissions across all relevant Regions, even if some Regions are not primary operation zones. We will provide specific guidance on SCP modifications and AWS Control Tower implementations that enable cross-Region inference functionality in the following sections.\nUse case\nFor our sample use case, we use Regions us-east-1 and us-west-2. All other Regions are denied using the landing zone deny (GRREGIONDENY). The customer’s AWS accounts that are allowed to use Amazon Bedrock are under an Organizational Unit (OU) called Sandbox. We want to enable the accounts under the Sandbox OU to use Anthropic’s Claude 3.5 Sonnet v2 model using cross-Region inference. This model is available in us-east-1, us-east-2, and us-west-2, as shown in the following screenshot.\n\nIn the current state, when the user tries to use Anthropic’s Claude 3.5 Sonnet v2 model using cross-Region inference, they get an error stating the SCP is denying the action.\nModify existing SCPs to allow Amazon Bedrock cross-Region inference\nIf you aren’t using AWS Control Tower to govern the multi-account AWS environment, you can create a new SCP or modify an existing SCP to allow Amazon Bedrock cross-Region inference.\nThe following code is an example of how to modify an existing SCP that denies access to all services in specific Regions while allowing Amazon Bedrock inference through cross-Region inference for Anthropic’s Claude 3.5 Sonnet V2 model:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenySpecificRegionAllowCRI\",\n      \"Effect\": \"Deny\",\n      \"Action\": \"*\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestedRegion\": \"us-east-2\"\n        },\n        \"ArnNotLike\": {\n          \"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n        }\n      }\n    }\n  ]\n}\n\nThis policy effectively blocks all actions in the us-east-2 Region except for the specified resources. This is a deny-based policy, which means it should be used in conjunction with allow policies to define a full set of permissions.\nYou should review and adapt this example to your organization’s specific needs and security requirements before implementing it in a production environment.\nWhen implementing these policies, consider the following:\n\nCustomize the Region and allowed resources to fit your specific requirements\nTest thoroughly in your environment to make sure that it doesn’t unintentionally block necessary services or actions\nRemember that SCPs affect the users and roles in the accounts they’re attached to, including the root user\nService-linked roles are not affected by SCPs, allowing other AWS services to integrate with AWS Organizations\n\nImplementation using AWS Control Tower\nAWS Control Tower creates SCPs to manage permissions across your organization. Manually editing these SCPs is not recommended because it can cause drift in your AWS Control Tower environment. However, there are some approaches you can take to allow specific AWS services, which we discuss in the following sections.\nPrerequisites\nMake sure that you’re running the latest version of AWS Control Tower. If you’re using a version less than 3.x and have Regions denied through AWS Control Tower settings, you need to enable your AWS Control Tower version to update the Region deny settings. Refer to the following considerations related to AWS Control Tower upgrades from 2.x to 3.x.\nAdditionally, make sure that the Organization dashboard on AWS Control Tower doesn’t show policy drifts and that the OUs and accounts are in compliance.\nOption 1: Extend existing Region deny SCPs for cross-Region inference\nAWS Control Tower offers two primary Region deny controls to restrict access to AWS services based on Regions:\n\nGRREGIONDENY (landing zone Region deny control) – This control applies to the entire landing zone rather than specific OUs. When enabled, it disallows access to operations in global and Regional services outside of specified Regions, including all Regions where AWS Control Tower is not available and all Regions not selected for governance.\nMULTISERVICE.PV.1 (OU Region deny control) – This configurable control can be applied to specific OUs rather than the entire landing zone. It disallows access to unlisted operations in global and Regional AWS services outside of specified Regions for an organizational unit. This control is configurable. This control accepts one or more parameters, such as AllowedRegions, ExemptedPrincipalARNs, and ExemptedActions, which describe operations that are allowed for accounts that are part of this OU: \n  \nAllowedRegions – Specifies the Regions selected, in which the OU is allowed to operate. This parameter is mandatory.\nExemptedPrincipalARNs – Specifies the IAM principals that are exempt from this control, so that they are allowed to operate certain AWS services globally.\nExemptedActions – Specifies actions that are exempt from this control, so that the actions are allowed.\n \n\nWe will use the CT.MULTISERVICE.PV.1 control and configure it for our scenario.\n\nCreate an IAM role with an IAM policy that will allow Amazon Bedrock inference using cross-Region inference. Let’s name this IAM role Bedrock-Access-CRI. We will use this at a later step. This IAM role will be created in AWS accounts that are part of the Sandbox OU. \n  \n {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowBedrockInference\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:InvokeModelWithResponseStream\"\n            ],\n            \"Resource\": [\n                \"arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n                \"arn:aws:bedrock:*::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0\"\n            ]\n        }\n    ]\n}\n \n\n\nNavigate to the Landing zone settings page and choose Modify settings.\nEnable the Region, us-east-2 in our case, and leave the rest of the settings unchanged.\nChoose Update landing zone to complete the changes.\n\nThe updates can take up to 60 minutes or more depending on the size of the Organization. This will update the landing zone Region deny settings (GRREGIONDENY) to include the Region us-east-2 to govern the Region.\n\nWhen the landing zone setup is complete, review the Organization settings to make sure that there are no pending updates for AWS accounts across the OUs. If you see pending updates, complete updating them and make sure the status for the account status shows Enrolled.\nOn the AWS Control Tower console, choose All controls under Controls library in the navigation pane to see a list of controls.\nLocate MULTISERVICE.PV.1 and choose the policy to open the control. \nChoose Control actions followed by Enable to start the configuration.\nOn the Select an OU page, select the OU you want to apply this control to. For our use case, we use the Sandbox OU.\nChoose Next. \nOn the Specify Region access page, select the Regions to allow access for the OU. For our use case, we select us-west-2 and us-east-1.\n\nWe don’t select us-east-2 because we want to deny all services on us-east-2 and only allow Amazon Bedrock inference through cross-Region inference.\n\nChoose Next.\nOn the Add service actions – optional page, add the Amazon Bedrock actions to the NotActions We add bedrock:Invoke* to allow Amazon Bedrock InvokeModel actions.\nChoose Next. \nOn the Specify configurations and tags – optional page, add the IAM role we created earlier under Exempted principals and choose Next. \nReview the configuration and choose Enable control.\n\nAfter the control is enabled, you can review the configuration by choosing OUs enabled, Accounts, Artifacts, and the Regions tab.\nThis completes the configuration. You can test the Amazon Bedrock inference with Anthropic’s Sonnet 3.5 v2 using the Amazon Bedrock console or the API by assuming the custom IAM role mentioned in the previous step (Bedrock-Access-CRI).\nYou will see that you can make Amazon Bedrock inference calls to only Anthropic’s Sonnet 3.5 v2 model using cross-Region inference from all of the three Regions (us-east-1, us-east-2, and us-west-2). Attempts to access other services on us-east-2 are blocked due to the CT.MULTISERVICE.PV.1 control you configured earlier.\nBy following these approaches, you can safely extend the permissions managed by AWS Control Tower without causing drift or compromising your governance controls.\nOption 2: Enable the denied Region using AWS Control Tower and conditionally block using an SCP\nIn this option, we enable the denied Region (us-east-2) and create a new SCP to conditionally block us-east-2 while allowing Amazon Bedrock inference through cross-Region inference.\n\nNavigate to the Landing zone settings page and choose Modify settings.\nEnable the Region, us-east-2 in our case, and leave the rest of the settings unchanged.\nChoose Update landing zone to complete the changes.\n\nThe updates can take up to 60 minutes or more depending on the size of the Organization. You can monitor the status of this update on the console.\n\nWhen the landing zone setup is complete, review the Organization settings to make sure that there are no pending updates for AWS accounts across the OUs. If you see pending updates, complete updating them and make sure the status for the account status shows Enrolled.\nOn the AWS Control Tower console, choose Service Control Policies under Policies in the navigation pane.\nCreate a new SCP with the sample policy shown earlier. This SCP denies all actions for us-east-2 while allowing Amazon Bedrock inference using a CRI profile ARN for Anthropic’s Claude Sonnet 3.5 v2.\nApply the SCP to the specific OU. In this scenario, we use the Sandbox OU.\n\nBecause you’re creating a new SCP and not modifying the existing SCPs created by AWS Control Tower, you will not see a drift in the AWS Control Tower state.\nYou can now test the update by running a few inference calls using the Amazon Bedrock console or the AWS Command Line Interface (AWS CLI). You will see that you can make Amazon Bedrock inference calls to only Anthropic’s Sonnet 3.5 v2 model using cross-Region inference from all three of the Regions (us-east-1, us-east-2, and us-west-2). Access to other AWS services on us-east-2 will be denied.\nUsing Customizations for AWS Control Tower to deploy SCPs\nThe recommended way to add custom SCPs is through the Customizations for AWS Control Tower (CfCT) solution:\n\nDeploy the CfCT solution in your management account.\nCreate a configuration package with your custom SCPs.\n\nThe following screenshot shows an example SCP that denies a specific Region while allowing calls to Amazon Bedrock using cross-Region inference for Anthropic’s Sonnet 3.5 v2 model.\n\n\nPrepare a manifest.yaml file that defines your policies.\n\nThe following screenshot shows an example manifest.yaml that defines the resources targeting the Sandbox OU.\n\n\nDeploy your custom SCPs to specific OUs.\n\nSummary\nAmazon Bedrock cross-Region inference provides valuable flexibility for organizations looking to use FMs across Regions. By carefully modifying your service control policies or AWS Control Tower controls, you can enable this functionality while maintaining your broader Regional access restrictions.\nThis approach allows you to:\n\nMaintain compliance with Regional access requirements\nTake advantage of the full capabilities of Amazon Bedrock\nSimplify your application architecture by accessing models from your primary Region\n\nThere is no additional cost associated with cross-Region inference, including the failover capabilities provided by this feature. This includes management, data transfer, encryption, network usage, and potential differences in price per million token per model. You pay the same price per token of the individual models in your source Region.\nAs AI and machine learning capabilities continue to evolve, finding the right balance between security controls and innovation enablement will remain a key challenge for organizations. The approach outlined in this post provides a practical solution to this specific challenge.\nFor more information, refer to Increase throughput with cross-region inference.\n\nAbout the Authors\nSatveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer’s deep understanding of generative AI technologies allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value.\nRamesh Venkataraman is a Solutions Architect who enjoys working with customers to solve their technical challenges using AWS services. Outside of work, Ramesh enjoys following stack overflow questions and answers them in any way he can.\nDhawal Patel is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and artificial intelligence. He focuses on deep learning, including NLP and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker.\nSumit Kumar is a Principal Product Manager, Technical at AWS Bedrock team, based in Seattle. He has over 12 years of product management experience across a variety of domains and is passionate about AI/ML. Outside of work, Sumit loves to travel and enjoys playing cricket and lawn tennis.",
      "date": "2025-03-27",
      "authors": "Satveer Khurpa",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the Amazon Bedrock cross-Region inference capability, which allows organizations to access foundation models across AWS Regions while balancing compliance and security measures. It provides guidance on modifying Regional access controls to enable this functionality, ensuring efficient model inference while upholding security policies.",
      "takeaways": [
        "- Cross-Region inference in Amazon Bedrock enhances throughput by allowing automatic traffic distribution across multiple AWS Regions.",
        "- Organizations must carefully adjust their Service Control Policies (SCPs) to permit cross-Region inference while maintaining broader Regional restrictions for compliance.",
        "- The integration and management of cross-Region inference can optimize resource utilization without incurring additional costs, addressing a key challenge between security and innovation in AI applications."
      ]
    },
    {
      "id": 10,
      "title": "Amazon SageMaker JumpStart adds fine-tuning support for models in a private model hub",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-jumpstart-adds-fine-tuning-support-for-models-in-a-private-model-hub/",
      "description": "Amazon SageMaker JumpStart is a machine learning (ML) hub that provides pre-trained models, solution templates, and algorithms to help developers quickly get started with machine learning. Within SageMaker JumpStart, the private model hub feature allows organizations to create their own internal repository of ML models, enabling teams to share and manage models securely within their organization.\nToday, we are announcing an enhanced private hub feature with several new capabilities that give organizations greater control over their ML assets. These enhancements include the ability to fine-tune SageMaker JumpStart models directly within the private hub, support for adding and managing custom-trained models, deep linking capabilities for associated notebooks, and improved model version management. These new features streamline the ML workflow by combining the convenience of pre-built solutions with the flexibility of custom development, while maintaining enterprise-grade security and governance.\nFor enterprise customers, the ability to curate and fine-tune both pre-built and custom models is crucial for successful AI implementation. Model curation provides quality control, compliance, and security while preventing duplicate efforts across teams. When enterprises fine-tune curated models, they can specialize general-purpose solutions for their specific industry needs and gain competitive advantages through improved performance on their proprietary data. Similarly, the ability to fine-tune custom models enables organizations to continuously improve their AI solutions, adapt to changing business conditions, and preserve institutional knowledge, while maintaining cost-efficiency.\nA common enterprise scenario involves centralized data science teams developing foundation models (FMs), evaluating the performance against open source FMs, and iterating on performance. After they develop their custom FM, it can serve as a baseline for the entire organization, and individual departments—such as legal, finance, or customer service—can fine-tune these models using their department-specific data that might be subject to different privacy requirements or access controls. This hub-and-spoke approach to model development maximizes resource efficiency while allowing for specialized optimization at the department level. This comprehensive approach to model management, now supported by the enhanced private hub features in SageMaker JumpStart, enables enterprises to balance standardization with customization while maintaining proper governance and control over their ML assets.\nSolution overview\nSageMaker JumpStart has introduced several new enhancements to its private model hub feature, allowing administrators greater control and flexibility in managing their organization’s ML models. These enhancements include:\n\nFine-tuning of models referenced in the private hub – Administrators can now add models from the SageMaker JumpStart catalog to their private hub and fine-tune them using Amazon SageMaker training jobs, without having to create the models from scratch.\nSupport for custom models – In addition to the pre-trained SageMaker JumpStart models, administrators can now add their own custom-trained models to the private hub and fine-tune them as needed.\nDeep linking of notebooks – Administrators can now deep link to specific notebooks associated with the models in the private hub, making it straightforward for users to access and work with the models.\nUpdating models in the private hub – The private hub now supports updating models over time as new versions or iterations become available, allowing organizations to stay current with the latest model improvements.\n\nThese new capabilities give AWS customers more control over their ML infrastructure and enable faster model deployment and experimentation, while still maintaining the appropriate access controls and permissions within their organization.\nIn the following sections, we provide guidance on how to use these new private model hub features using the Amazon SageMaker SDK and Amazon SageMaker Studio console.\nTo learn more about how to manage models using private hubs, see Manage Amazon SageMaker JumpStart foundation model access with private hubs.\nPrerequisites\nTo use the SageMaker Python SDK and run the code associated with this post, you need the following prerequisites:\n\nAn AWS account that contains your AWS resources\nAn AWS Identity and Access Management (IAM) role with access to SageMaker Studio notebooks\nSageMaker JumpStart enabled in a SageMaker Studio domain\n\nCreate a private hub, curate models, and configure access control\nThis section provides a step-by-step guide for administrators to create a private hub, curate models, and configure access control for your organization’s users.\n\nBecause the feature has been integrated in the latest SageMaker Python SDK, to use the model granular access control feature with a private hub, let’s first update the SageMaker Python SDK: \n  \n!pip3 install sagemaker —force-reinstall —quiet\n \nNext, import the SageMaker and Boto3 libraries: \n  \nimport boto3 from sagemaker\nimport Session from sagemaker.session\nimport Hub\n \nConfigure your private hub: \n  \nHUB_NAME=\"CompanyHub\"\nHUB_DISPLAY_NAME=\"Allowlisted Models\"\nHUB_DESCRIPTION=\"These are allowlisted models taken from the SageMaker Public Hub\"\nREGION=\"<your_region_name>\" # for example, \"us-west-2\"\n \n\nIn the preceding code, HUB_NAME specifies the name of your hub. HUB_DISPLAY_NAME is the display name for your hub that will be shown to users in UI experiences. HUB_DESCRIPTION is the description for your hub that will be shown to users.\nUse an AWS Region where SageMaker JumpStart is available, as of March 2025: us-west-2, us-east-1, us-east-2, eu-west-1, eu-central-1, eu-central-2, eu-north-1, eu-south-2, me-south-1, me-central-1, ap-south-1, ap-south-2, eu-west-3, af-south-1, sa-east-1, ap-east-1, ap-northeast-2, ap-northeast-3, ap-southeast-3, ap-southeast-4, ap-southeast-5, ap-southeast-7, eu-west-2, eu-south-1, ap-northeast-1, us-west-1, ap-southeast-1, ap-southeast-2, ca-central-1, ca-west-1, cn-north-1, cn-northwest-1, il-central-1, mx-central-1, us-gov-east-1, us-gov-west-1.\n\nSet up a Boto3 client for SageMaker: \n  \nsm_client = boto3.client('sagemaker')\nsession = Session(sagemaker_client=sm_client)\nsession.get_caller_identity_arn()\n \nCheck if the following policies have been already added to your admin IAM role; if not, you can add them as inline policies (use the Region configured in Step 3): \n  \n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:GetObjectTagging\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>\",\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>/*\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n \n\nIn addition to setting up IAM permissions to the admin role, you need to scope down permissions for your users so they can’t access public contents.\n\nUse the following policy to deny access to the public hub for your users. These can be added as inline policies in the user’s IAM role (use the Region configured in Step 3): \n  \n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\": \"Deny\",\n            \"Resource\": [\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>\",\n                \"arn:aws:s3:::jumpstart-cache-prod-<REGION>/*\"\n            ],\n            \"Condition\": {\n                \"StringNotLike\": {\"s3:prefix\": [\"*.ipynb\", \"*/eula.txt\"]}\n            }\n        },\n        {\n            \"Action\": \"sagemaker:*\",\n            \"Effect\": \"Deny\",\n            \"Resource\": [\n                \"arn:aws:sagemaker:<REGION>:aws:hub/SageMakerPublicHub\",\n                \"arn:aws:sagemaker:<REGION>:aws:hub-content/SageMakerPublicHub/*/*\"\n            ]\n        }\n    ]\n}\n \n\nAfter you have set up the private hub configuration and permissions, you’re ready to create the private hub.\n\nUse the following code to create the private hub within your AWS account in the Region you specified earlier: \n  \nhub = Hub(hub_name=HUB_NAME, sagemaker_session=session)\n\ntry:\n  hub.create(\n      description=HUB_DESCRIPTION,\n      display_name=HUB_DISPLAY_NAME\n  )\n  print(f\"Successfully created Hub with name {HUB_NAME} in {REGION}\")\nexcept Exception as e:\n  if \"ResourceInUse\" in str(e):\n    print(f\"A hub with the name {HUB_NAME} already exists in your account.\")\n  else:\n    raise e\n \nUse describe() to verify the configuration of your hub. After your private hub is set up, you can add a reference to models from the SageMaker JumpStart public hub to your private hub. No model artifacts need to be managed by the customer. The SageMaker team will manage version or security updates. For a list of available models, refer to Built-in Algorithms with pre-trained Model Table.\nTo search programmatically, run the following command: \n  \nfrom sagemaker.jumpstart.filters import Or\n\nfilter_value = Or(\n\"framework == meta\",\n\"framework == deepseek\"\n)\nmodels = []\nnext_token = None\n\nwhile True:\n    response = hub.list_sagemaker_public_hub_models(\n        filter=filter_value,\n        next_token=next_token\n    )\n    models.extend(response[\"hub_content_summaries\"])\n    next_token = response.get(\"next_token\")\n    \n    if not next_token:\n        break\nprint(models)\n \n\nThe filter argument is optional. For a list of filters you can apply, refer to the following GitHub repo.\n\nUse the retrieved models from the preceding command to create model references for your private hub: \n  \nfor model in models:\n    print(f\"Adding {model.get('hub_content_name')} to Hub\")\n    hub.create_model_reference(model_arn=model.get(\"hub_content_arn\"), \n                               model_name=model.get(\"hub_content_name\"))\n \n\nThe SageMaker JumpStart private hub offers other useful features for managing and interacting with the curated models. Administrators can check the metadata of a specific model using the hub.describe_model(model_name=<model_name>) command. To list the available models in the private hub, you can use a simple loop:\n\nresponse = hub.list_models()\nmodels = response[\"hub_content_summaries\"]\nwhile response[\"next_token\"]:\n    response = hub.list_models(next_token=response[\"next_token\"])\n    models.extend(response[\"hub_content_summaries\"])\n\nfor model in models:\n    print(model.get('HubContentArn'))\n\nIf you need to remove a specific model reference from the private hub, use the following command:\n\nhub.delete_model_reference(\"<model_name>\")\n\nIf you want to delete the private hub from your account and Region, you will need to delete all the HubContents first, then delete the private hub. Use the following code:\n\nfor model in models:\n    hub.delete_model_reference(model_name=model.get('HubContentName'))\n    \nhub.delete()\n\nFine-tune models referenced in the private hub\nThis section walks through how to interact with allowlisted models in SageMaker JumpStart. We demonstrate how to list available models, identify a model from the public hub, and fine-tune the model using the SageMaker Python SDK as well as the SageMaker Studio UI.\nUser experience using the SageMaker Python SDK\nTo interact with your models using the SageMaker Python SDK, complete the following steps:\n\nJust like the admin process, the first step is to force reinstall the SageMaker Python SDK: \n  \n!pip3 install sagemaker —force-reinstall —quiet\n \nWhen interacting with the SageMaker SDK functions, add references to the hub_arn: \n  \nmodel_id=\"meta-vlm-llama-3-2-11b-vision\"\nmodel_version=\"2.1.8\"\nhub_arn=\"<YourHubARN>\"\n\nfrom sagemaker import hyperparameters\n\nmy_hyperparameters = hyperparameters.retrieve_default(\n    model_id=model_id, model_version=model_version, hub_arn=hub_arn\n)\nprint(my_hyperparameters)\nhyperparameters.validate(\n    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters, hub_arn=hub_arn\n)\n \nYou can then start a training job by specifying the model ID, version, and hub name: \n  \nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nestimator = JumpStartEstimator(\n    model_id=model_id,\n    hub_name=hub_arn,\n    model_version=model_version,\n    environment={\"accept_eula\": \"false\"},  # Please change {\"accept_eula\": \"true\"}\n    disable_output_compression=True,\n    instance_type=\"ml.p4d.24xlarge\",\n    hyperparameters=my_hyperparameters,\n)\nestimator.fit({\"training\": train_data_location})\n \n\nFor a custom model, see the example notebooks in GitHub.\nUser experience in SageMaker Studio\nComplete the following steps to interact with allowlisted models using SageMaker Studio:\n\nOn the SageMaker Studio console, choose JumpStart in the navigation pane or in the Prebuilt and automated solutions section. \nChoose one of model hubs you have access to.\n\nIf the user has access to multiple hubs, you will see a list of hubs, as shown in the following screenshot.\n\nIf the user has access to only one hub, you will be redirected to the model list.\n\n\nTo fine-tune a model, choose Train (this option will be enabled if it’s supported). \nModify your training job configurations like training data, instance type, and hyperparameters, and choose Submit. \n\nDeep link notebooks in the private hub\nYou can now also access the notebook associated with the model in your curated hub.\n\nChoose your model, then choose Preview notebooks. \nChoose Open in JupyterLab to start the deep link workflow. \nSelect a running JupyterLab space and choose Open notebook.\n\nYou will need to upgrade your space to use a SageMaker distribution of at least 2.4.1. For more information on how to upgrade your SageMaker distribution, see Update the SageMaker Distribution Image.\n\nThis will automatically open the selected notebook in your JupyterLab instance, with your private HubName inputted into the necessary classes.\n\nUpdate models in the private hub\nModify your existing private HubContent by calling the new sagemaker:UpdateHubContent API. You can now update an existing HubContent version in-place without needing to delete and re-add it. We don’t support updating the HubContentDocument at this time because there can be backward-incompatible changes that are introduced that fundamentally alter the performance and usage of the model itself. Refer to the public API documentation for more details.\n\nclient.update_hub_content(\n    hub_content_name=\"my-model\",\n    hub_content_version=\"1.0.0\",\n    hub_content_type=\"Model\",\n    hub_name=\"my-hub\",\n    support_status=\"DEPRECATED\"\n)\n\nAdditionally, you can modify your ModelReferences by calling the new sagemaker:UpdateHubContentReference API. Refer to the public API documentation for more usage details.\n\nclient.update_hub_content_reference(\n    hub_content_name=\"your-model\",\n    hub_content_type=\"ModelReference\",\n    hub_name=\"my-hub\",\n    min_version=\"1.2.0\"\n)\n\nConclusion\nThis post demonstrated the new enhancements to the SageMaker JumpStart private model hub feature, which gives enterprise customers greater control and flexibility in managing their ML assets. The key capabilities introduced include the ability to fine-tune pre-built SageMaker JumpStart models directly within the private hub, support for importing and fine-tuning custom-trained models, deep linking to associated notebooks for streamlined access and collaboration, and improved model version management through APIs. These features enable enterprises to curate a centralized repository of trusted, specialized ML models, while still providing the flexibility for individual teams and departments to fine-tune and adapt these models to their specific needs. The seamless integration with SageMaker Studio further streamlines the model development and deployment workflow, empowering enterprises to accelerate their ML initiatives while maintaining the appropriate security and control over their ML assets.\nNow that you’ve seen how the enhanced private model hub features in Amazon SageMaker JumpStart can give your organization greater control and flexibility over managing your machine learning assets, start leveraging these capabilities to curate a centralized repository of trusted models and accelerate your AI initiatives.\n\nAbout the Authors\nMarc Karp is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.\nNiris Okram is a senior academic research specialist solutions architect at AWS. He has extensive experience working with public, private and research customers on various fields related to cloud. He is passionate about designing and building systems to accelerate the customer’s mission on AWS cloud.\nBenjamin Crabtree is a software engineer with the Amazon SageMaker and Bedrock teams. He is passionate about democratizing the new and frequent breakthroughs in AI. Ben received his undergraduate degree from the University of Michigan and now lives in Brooklyn, NY.\nBanu Nagasundaram leads product, engineering, and strategic partnerships for SageMaker JumpStart, SageMaker’s machine learning and GenAI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.",
      "date": "2025-03-26",
      "authors": "Marc Karp",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the enhancements made to Amazon SageMaker JumpStart's private model hub, which now supports fine-tuning of both pre-trained and custom ML models. These features elevate model management by enabling organizations to curate, update, and optimize models while ensuring security and compliance.",
      "takeaways": [
        "- The new fine-tuning capability allows organizations to adapt pre-built SageMaker models to specific industry needs, enhancing AI performance.",
        "- Administrators can now manage custom-trained models within the private hub, streamlining model experimentations and ensuring better resource efficiency.",
        "- Enhanced deep linking capabilities facilitate easier access to associated notebooks for collaboration and development, supporting a more efficient ML workflow."
      ]
    },
    {
      "id": 11,
      "title": "Generative AI-powered game design: Accelerating early development with Stability AI models on Amazon Bedrock",
      "link": "https://aws.amazon.com/blogs/machine-learning/generative-ai-powered-game-design-accelerating-early-development-with-stability-ai-models-on-amazon-bedrock/",
      "description": "In the competitive world of game development, staying ahead of technological advancements is crucial. Generative AI has emerged as a game changer, offering unprecedented opportunities for game designers to push boundaries and create immersive virtual worlds. At the forefront of this revolution is Stability AI’s cutting-edge text-to-image AI model, Stable Diffusion 3.5 Large (SD3.5 Large), which is transforming the way we approach game environment creation.\nSD3.5 Large, available in Amazon Bedrock, is Stability AI’s most advanced text-to-image model to date. With 8.1 billion parameters, this model excels at generating high-quality, 1-megapixel images from text descriptions with exceptional prompt adherence, making it ideal for creating detailed game environments at speed. Its improved architecture, based on the Multimodal Diffusion Transformer (MMDiT), combines multiple pre-trained text encoders for enhanced text understanding and uses QK-normalization to improve training stability.\nThe model demonstrates improved performance in image quality, typography, and complex prompt understanding. It excels at creating diverse, high-quality images across multiple styles, making it valuable for industries such as media, gaming, advertising, and education.\nIn this post, we explore how you can use SD3.5 Large to address practical gaming needs such as early concept art and character design.\nKey improvements in SD3.5 Large compared to SD3 Large\nSD3.5 Large offers the following improvements:\n\nEnhanced photorealism – Delivers detailed 3D imagery with unprecedented realism\nSuperior scene complexity – Handles multiple subjects in intricate scenes with remarkable accuracy\nImproved anatomical rendering – Generates more precise and natural human representations\nDiverse representation – Creates images with inclusive representation of skin tones and features without extensive prompting\n\nReal-world use cases for game environment creation\nImage generation is poised to revolutionize a few key areas within the gaming industry. Firstly, it will significantly enhance the ideation and design process, allowing teams to rapidly create new scenes and objects, thereby accelerating the design cycle. Secondly, it will enable in-game content generation, empowering users to create new objects, modify avatar skins, or generate new textures. Although current adoption is more prevalent in the design phase, the continued advancement of generative AI is expected to lead to increased user-generated AI content (such as player avatars), which will substantially boost user creativity and overall gaming experience. This shift towards AI-assisted content creation in gaming promises to open up new realms of possibilities for both developers and players alike.\nThe following are sample prompts for creating early game worlds and their output:\n\nA vibrant fantasy landscape featuring rolling hills, a sparkling river, and a majestic castle in the distance under a bright blue sky.\n\n\n\nA dense tropical rainforest teeming with exotic plants and wildlife, sunlight filtering through the thick canopy, with a hidden waterfall cascading into a crystal-clear pool.\n\n\n\nA futuristic city skyline at dusk, featuring sleek skyscrapers with neon lights and flying vehicles soaring between them, reflecting on the glassy surface of a river.\n\n\nThe following are sample prompts for creating early game assets and props from different angles:\n\nAn intricately designed realistic game weapon prop of a fiery blue and green blade set against a blurred background of a gargantuan temple. The blade merges geometrical design of the blade with an alien cultural aesthetic.\nClose-up, side-angle view of an intricately designed realistic, game weapon prop of a fiery blue and green blade set against a blurred background of a gargantuan temple. The blade merges geometrical design of the blade with an alien cultural aesthetic.\nTop-down view of an intricately designed realistic, game weapon prop of a fiery blue and green blade set against a blurred background of a gargantuan temple. The blade merges geometrical design of the blade with an alien cultural aesthetic.\n\n\nSolution overview\nTo demonstrate the power of SD3.5 Large in game environment creation, let’s walk through a hypothetical workflow. We have provided a Jupyter notebook to deploy a sample gaming use case in the following GitHub repo. Use the us-west-2 AWS Region to run this demo.\nPrerequisites\nThis notebook is designed to run on AWS, using Amazon Bedrock for both Anthropic’s Claude 3 Sonnet and Stability AI model access. Make sure you have the following set up before moving forward:\n\nAn AWS account.\nAn Amazon SageMaker domain.\nAccess to Stability AI’s SD3.5 Large text-to-image model through the Amazon Bedrock console. For instructions, see Manage access to Amazon Bedrock foundation models.\n\nDefine the game world\nStart by outlining the core concepts of your game world, including its theme, atmosphere, and key locations. For example, “Mystic Realms is set in a vibrant fantasy world where players embark on quests to uncover ancient secrets and battle mystical creatures. The game features diverse environments, including enchanted forests, mystical mountains, and forgotten ruins. The atmosphere is whimsical and magical, with bright colors and fantastical elements that evoke a sense of wonder.”\nCraft detailed prompts for worlds and objects\nUse natural language to describe specific environments and objects you want to create. The following screenshot shows some generated prompts.\n\nYou can also generate initial concept images with Amazon Bedrock following these steps:\n\nOn the Amazon Bedrock console, under Foundation models in the navigation pane, choose Model catalog.\nFor Providers, select Stability AI, then choose Stable Diffusion 3.5 Large.\nChoose Open in playground.\nEnter your prompt and choose Run. A high-fidelity image will be generated in seconds.\n\nIterate and refine\nAfter you have a base concept you’re happy with, you can generate variations to explore different possibilities for the same environment. Analyze the generated images and refine your prompts to achieve the desired results. You might want to adjust elements like lighting, color palette, or specific environmental features. Finally, use the generated images as reference material for 3D artists to create fully realized game environments.\nClean up\nTo avoid charges, you must stop the active SageMaker notebook instances if you used the notebook demo. For instructions, refer to Clean up Amazon SageMaker notebook instance resources.\nConclusion\nStability AI’s latest series of models represents a significant advancement in generative AI, providing game developers, designers, and content creators with a powerful tool to enhance creative workflows and explore new dimensions of visual storytelling. By using Stability AI’s capabilities, organizations can address practical gaming needs, from concept art and character design to level creation and marketing campaigns. However, it’s essential to approach this technology with a responsible and ethical mindset, considering potential biases, respecting intellectual property rights, and mitigating the risks of misuse. By embracing these models while being aware of their limitations and ethical considerations, gaming professionals can push the boundaries of what’s possible in game design and visual content creation.\nTo get started, check out Stability AI models available in Amazon Bedrock.\n\nAbout the Authors\nIsha Dua is a Senior Solutions Architect based in the San Francisco Bay Area. She helps AWS Enterprise customers grow by understanding their goals and challenges, and guiding them on how they can architect their applications in a cloud-native manner while making sure they are resilient and scalable. She’s passionate about machine learning technologies and environmental sustainability.\nParth Patel is a Senior Solutions Architect at AWS in the San Francisco Bay Area. Parth guides customers to accelerate their journey to the cloud and help them adopt and grow on the AWS Cloud successfully. He focuses on machine learning, environmental sustainability, and application modernization.",
      "date": "2025-03-26",
      "authors": "Isha Dua",
      "journal": "aws.amazon.com",
      "therapyArea": "Technical AI Updates",
      "term": "AWS",
      "summary": "The article discusses the advancements of Stability AI's text-to-image model, SD3.5 Large, which enhances game design by generating high-quality environments and assets through generative AI techniques. The model's capabilities allow for improved realism, complex scene handling, and transformative workflows in game development.",
      "takeaways": [
        "- Stability AI's SD3.5 Large model offers enhanced photo-realism and superior scene complexity, revolutionizing the design process for game developers.",
        "- The model enables rapid creation of game environments and assets, significantly accelerating early concept art and character design stages.",
        "- Generative AI is expected to increase user-generated content in gaming, allowing players to customize and create, thereby enriching the overall gaming experience."
      ]
    },
    {
      "id": 12,
      "title": "The newest recipients of Google.org’s AI Opportunity Fund",
      "link": "https://blog.google/outreach-initiatives/google-org/ai-opportunity-fund-march-2025-update/",
      "description": "Google.org has announced which organizations will receive the final $10 million in funding from its $75 million AI Opportunity Fund.",
      "date": "2025-03-31",
      "authors": "Maggie Johnson",
      "journal": "blog.google",
      "therapyArea": "Technical AI Updates",
      "term": "Google AI Blog",
      "summary": "Google.org has announced the final recipients of its AI Opportunity Fund, which has distributed a total of $75 million to support organizations working in the field of artificial intelligence. This funding aims to promote equity and accessibility in AI technology.",
      "takeaways": [
        "- The AI Opportunity Fund has now concluded its funding cycle with the final $10 million allocation.",
        "- The fund aims to support organizations that leverage AI to address social challenges and improve community outcomes.",
        "- This initiative reflects a growing trend in philanthropy to invest in AI for social good, promoting diversity and inclusion in AI development."
      ]
    },
    {
      "id": 13,
      "title": "[The AI Show Episode 141]: Road to AGI (and Beyond) #1 — The AI Timeline is Accelerating",
      "link": "https://www.marketingaiinstitute.com/blog/the-ai-show-episode-141",
      "description": "\n  \n\nThe future of AI is arriving faster than most are ready for. \nIn this kickoff episode of The Road to AGI series, Paul Roetzer shares why Artificial General Intelligence (AGI) may be only a few years away, why the definition of AGI itself is a moving target, and how leaders can prepare for profound disruption—sooner than they think. \n",
      "date": "2025-03-27",
      "authors": "Claire Prudhomme",
      "journal": "marketingaiinstitute.com",
      "therapyArea": "AI Marketing and Advertising",
      "term": "Marketing AI Institute",
      "summary": "In Episode 141 of The AI Show, Paul Roetzer discusses the rapid advancements toward Artificial General Intelligence (AGI) and the implications of its potential arrival within a few years. He emphasizes the evolving definition of AGI and offers guidance for leaders to navigate the upcoming disruptions.",
      "takeaways": [
        "- The timeline for achieving AGI is accelerating, with significant advancements expected in the near future.",
        "- The definition of AGI is not fixed and continues to evolve, reflecting changes in technology and understanding.",
        "- Leaders need to prepare for the disruptions that AGI may bring to various industries, including the pharmaceutical sector."
      ]
    }
  ]
}
